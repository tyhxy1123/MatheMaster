\chapter{Basic iterative methods}%
\label{cha:Basic iterative methods}

We want to solve the linear system
\[
	Au = f \qquad (Ax = h)
.\] 
\begin{itemize}
	\item traditional directl methodes (Gauss elemination) are insufficient:
		\begin{itemize}
			\item $\O(n^{4})$ steps
			\item $\O(n^{3})$ storage units
		\end{itemize}

	\item Iterative methods try to improve an implicit guess until some tolerance criterion is reached
		\begin{itemize}
			\item $\O(n^{2})$ storage units
		\end{itemize}
\end{itemize}

\section{Relaxation methods}%
\label{sec:Relaxation methods}

Basic relaxation methods are based on a coordinate-wise relaxation/reduction of an initial residuum
\[
\gamma = b- Ax
.\] 
In each relaxation step there methodes try to annihilate a component of the residual.

The general idea is to transform the linear system into a fixed point problem
\[
	x = x + P^{-1}(b-Ax)
,\] 
where $P$ is called a proconditioner (matrix) and should be easy to invert in the sense the $Pe=r$ can be solved easily and $P$ should be an approximation of matrix $A$.

Basic idea to solve a fixed-point problem is a fixed-point iteration
\[
	x^{k+1} = x^{k} + P^{-1}(b-Ax^{k})
.\] 
Rewriting this to make it a bit prettier results in
\[
	x^{k+1} = Gx^{k} + g =:T(x^{k})
.\] 
where $G = I-P^{-1}A$ is the iteration matrix and $g = P^{-1}b$.

\subsection{Some examples of relaxation schemes}
\label{sec:Some examples of relaxation schemes}

\begin{enumerate}[label=\Alph{enumi})]
	\item Jacobi method 

		Let $P_{J} = \diag(A)$, then obviously
		\[
			x^{k+1} = x^{k} + \diag(A)^{-1}(b-Ax^{k})
		.\] 
		written in terms of the components
		\[
			x^{k}= (\xi _{i}^{k})_{i}, \quad b=(\beta_{i})_{i}
		\] 
		gives
		\[
			a_{ii}\xi _{i}^{(k+1)} = -\sum_{\substack{j=1 \\ j \neq i}}^{n}{a_{ij}\xi _{j}^{(k)}+ \beta_{i}}
		.\] 
		Let $A=D-L-U$ be a splitting of the matrix $A$ into diagonal $D$, lower triangular part $L$ and upper triangular part $U$
		\[
		A = \begin{pmatrix}
			\diagdown & & -U \\
			  &D&    \\
			-L & & \diagdown
		\end{pmatrix}
		.\] 
		Then $P_{J} = D$
		\[
			Dx^{k+1} = (L+U)x^{k} + b
		.\] 
		For the 5-point stencil $\laplace_{h}^{(5)}$ we have 
		\[
			D = \diag(\frac{4}{h^{2}}) 		
		\] 
		and the matrix $L+U$ looking as follows
		\begin{figure}[H]
			\begin{center}
				\includegraphics[width=0.5\textwidth]{pics/chapter1/huebschematrix.png}
			\end{center}
			\caption{$L+U$}
			\label{fig:huebschematrix}
		\end{figure}
		
		So written directly in terms of the grid vector $u_{i,j}$ we get the Jacobi scheme for inner nodes
		\[
			u_{i,j}^{k+1} = \frac{1}{4}(u_{i,j-1}^{k}+ u_{i-1,j}^{k} + u_{i,j+1}^{k} + u_{i+1,j}^{k} + h^{2}f_{i,j})
		.\] 
		\underline{Note}: Instead of $P=D$ one could implement a \underline{damped Jacobi scheme} with
		\[
			P_{J}(\omega )=\omega \cdot D \qquad \text{ with } 0 < \omega \leq 1
		.\] 
	\item Gauss-Seidel Relaxation

		Instead of diagonal $P$, we use triangular matrix
		\[
			\underset{\text{ (forward) }}{P_{GS} = D-L} \quad\text{ or }\quad \underset{\text{ (backward) }}{P_{GS'}=D-U}
		.\] 
		\begin{figure}[H]
			\begin{center}
				\input{tikz/chapter1/forward_backward_iteration.tex}
			\end{center}
			\caption{forward and backward iteration matrix form}
			\label{fig:forwardbackward}
		\end{figure}
		
		We get the component-wise scheme
		\[
			\xi _{i}^{(k+1)}=\frac{1}{a_{ii}} \Big( -\sum_{j=1}^{i-1}{a_{ij}} \xi _{j}^{(k+1)}-\sum_{j=i+1}^{n}{a_{ij}\xi _{j}^{k}} + \beta _{i} \Big) \qquad i=1, \ldots, n
		\] 
		or (backward)
		\[
			\xi _{i}^{(k+1)} = \frac{1}{a_{ii}} \Big( -\sum_{j=1}^{i-1}{a_{ij}\xi _{i}^{(k)}}-\sum_{j=i+1}^{n}{a_{ij}\xi _{j}^{(k+1)}} + \beta _{i} \Big) \qquad i=n, \ldots ,1
		.\] 
		\underline{Note} : A relaxation method uses:
		\[
			P_{GS}(\omega ) := \frac{1}{\omega }(D-L) \quad \text{ or } \quad P_{GS'}(\omega ) := \frac{1}{\omega }(D-U)
		\] 
		i.e. (forward)
		\[
			\xi _{i}^{(k+1)} = (1-\omega )\xi _{i}^{(k)} + \frac{\omega }{a_{ii}} \Big( -\sum_{j=1}^{i-1}{a_{ij}\xi _{j}^{(k+1)}} - \sum_{j=i+1}^{n}{a_{ij}\xi _{j}^{(k)}} + \beta _{i} \Big) 
		.\] 
	\item Richardson relaxation
		\begin{align*}
			P_{R}(\omega ) :&= \frac{1}{\omega } I \\
			\xi _{i}^{(k+1)} &= \xi _{i}^{(k)} + \omega \Big( -\sum_{j=1}^{n}{a_{ij}\xi _{j}^{(k)}} + \beta _{i} \Big) 
		\end{align*}
\end{enumerate}
		Questions:
		\begin{itemize}
			\item How to characterize convergence?
			\item How to choose the relaxation parameter $\omega $?
		\end{itemize}

		Denote by $e^{k} := A^{-1}b - x^{k}$ the error in the k-th iteration and by $G:=I-P^{-1}A$ the iteration matrix. So
		\[
			e^{k+1}= (I-P^{-1}A)e^{k} = Ge^{k} = G^{2}e^{k-1} = \cdots = G^{k+1}e^{0}
		.\] 
		Taking the norm on both sides results in
		\[
			\norm{e^{k+1}}  \leq \normiii{(I-P^{-1}A)^{k+1}} \cdot \norm{e^{0}} 
		\] 
		with vector norm $\norm{\cdot}$ and induced matrix norm $\normiii{\cdot}$.
		\begin{lemma}
		\label{thm:richardconvergencelemma}
		The error norm $\norm{e^{k}} \rightarrow 0$ ( for $k \rightarrow \infty$) and $x^{k} \rightarrow A^{-1}b$ for all initial errors $e^{0}$ $\iff$
		\[
			\lim_{k \to \infty} \normiii{(I-P^{-1}A)^{k}} = 0
		.\] 
		The proof is omitted here and can be read in 
		[Greenbaum 97. Lemma 2.11].
		\end{lemma}

		\begin{theorem}
		\label{thm:richardconvergencetheorem}
			$x^{k} \rightarrow A^{-1}b$ for $k \rightarrow \infty$ for all initial values $x^{0}$ $\iff$
			\[
				\rho(I-P^{-1}A) < 1
			.\] 
			(with $\rho(\cdot)$ the spectral radius of the matrix)
		\end{theorem}
		\begin{proof}\
		\label{thm:richardconvergencetheoremproof}
		\begin{itemize}
			\item "$\Longrightarrow$" 

				Assume that $x^{k}\rightarrow A^{-1}b$ converges. 
				Let $\lambda $ be the eigenvalue of $G$ with $\abs{\lambda} = \rho(G)$ and $x$ be the associated eigenvector.
				
				Choose $b=0$ and $x^{0}=cx$ for any $c \in \R \setminus \left\{ 0 \right\} $. We get
				\[
				x^{k+1} = Gx^{k} = G^{k+1}x^{0} = \lambda ^{k+1} x^{0}
				.\] 
				\[
					\begin{array}{rl}
						\abs{\lambda } >1 &\implies \text{ divergence } \\
						\abs{\lambda }  = 1 &\implies \text{ no convergence }
				\end{array}
				\lightning
				.\] 
			\item "$\Longleftarrow$"

				Assume $\rho(G)<1$.

				There exists an $\epsilon >0$ and norm in $\R^{n \times n}$ such that.
				\[
					\normiii{G} \leq \rho(G) + \epsilon  < 1
				\] 
				with the previous lemma we get convergence.
		\end{itemize}
		[also see Y. Saad 03. Corollary 4.2]
		\end{proof}

\begin{definition}
\label{thm:additivesplitting}
Let $A, M, N$ be three given matrices with $ A = M-N$. The pair $(M,N)$ is called a \underline{regular splitting} of $A$ if
\begin{itemize}
	\item $M$ is non-singular
	\item $M^{-1}, N$ are non-negative (i.e. all entries are non-negative) ($M^{-1} \geq 0, N \geq 0$)
\end{itemize}
\end{definition}
With this splitting, the relaxation scheme can be written as 
\[
	x^{k+1} = \underbrace{M^{-1}N}_{G} x^{k} + M^{-1}b
.\] 
Thus
\begin{align*}
M \overset{\wedge}&{=} P \\
N \overset{\wedge}&{=} P-A
\end{align*}

\begin{theorem}
\label{thm:regularsplittingtheorem}
Let $M,N$ be a regular splitting of $A$. Then $\rho(\underbrace{M^{-1}N}_{G} ) < 1$ $\iff$

A is non-singular and $A^{-1}$ is non-negative.
\end{theorem}

\begin{proof}\
\label{thm:regularsplittingtheoremproof}
\begin{itemize}
	\item "$\Longrightarrow$"

		Assume $\rho(G) < 1$. 
		\[
			A = \underbrace{M}_{\text{ non-sing. }}\cdot \underbrace{(I-G)}_{\text{ non-sing. }} = M -MG = M-N
		.\] 
		So $A$ is non-singular as a product of non-singular matrices.
		\[
			(I-G)^{-1} = \sum_{i=0}^{\infty}{G^{i}}
		\] 
		with $G$ non-negative. So the powers $G^{i}$ are non-negative. Hence $(I-G)^{-1}$ is non-negative.
		\[
		\implies A^{-1} \text{ is non-negative. }
		.\] 
	\item "$\Longleftarrow$"

		Assume $A$ is non-singular and $A^{-1}$ is non-negative.

		Since $A$ and $M$ are non-singular, the matrix $(I-G)$ is non-singular.
		\begin{align*}
			A^{-1}N &= (M(I-M^{-1}N))^{-1}N \\
					&= (I-M^{-1}N)^{-1}M^{-1}N \\
					\tag{$\ast\ast$}
					&= (I-G)^{-1}G
		\end{align*}

		\[
		M^{-1}, N \geq 0 \qquad G = M^{-1}N \geq 0
		.\] 

		Let $\lambda $ be an eigenvalue of $G$ with $\abs{\lambda }=\rho(G)$ and associated eigenvector $x$ with $x \geq 0$ (Perron-Frobenius)
		\[
			Gx = \rho(G)x
		\] 
		and with $(\ast\ast)$ we get $A^{-1}Nx = (I-G)^{-1}x = \frac{\rho(G)}{1 - \rho(G)}x$.

		This implies
		\[
			\frac{\rho(G)}{1-\rho(G)} \geq 0
		\] 
		and so $0 \leq \rho(G) \leq 1$.

		Since $(I-G)$ is non-singular, $\rho(G) \neq 1$.

		[also see Y.Saad 03 Th.4.4]
\end{itemize}
\end{proof}

\begin{lemma}
\label{thm:matrixequivalences}
	Let $A \in \R^{n \times n}$ with $a_{ij} \leq 0, i \neq j$. Then the following statements are equivalent:
	\begin{enumerate}[label=\arabic{enumi})]
		\item $A$ is an M-matrix
		\item $A$ is non-singular and $A^{-1} \geq 0$ (and $a_{ii} > 0$)
		\item $a_{ii} > 0$ and $A\cdot D$ is strictly (row-) diagonally dominant for some diagonal matrix $D$
	\end{enumerate}
	
	For a proof see [Horn, Johnson : Topics of matrix analysis, 91']
\end{lemma}

\begin{theorem}
\label{thm:theorem2.3}
	If $A$ is strictly diagonally dominant or irreducibly diagonally dominant, then the associated Jacobi and GS iteration scheme converges for any initial vector $x^{0}$.
\end{theorem}

\begin{proof}
\label{thm:theorem2.3proof}
	[Y. Saad 03 Theorem 4.9]
\end{proof}

What is the "optimal" relaxation parameter?

In which sense "optimal"?

Denote by $G(\omega )=I-P^{-1}(\omega )A$ the iteration matrix induced by $P(\omega )$.

We say $\omega $ is optimal if
\[
	\omega  = \argmin_{\alpha \in \R^{+}} \rho(G(\alpha ))
.\] 
An iteration scheme is called \underline{underrelaxation method} if $\omega  < 1$. Otherwise it is called \underline{overrelaxation method}.

\begin{theorem}
\label{thm:theorem2.4}
Let $G_{J}$ be the iteration matrix of the Jacobi relaxation and assume	$G_{J}$ has only real eigenvalues $\lambda_1 \leq \lambda_2 \leq \ldots \leq \lambda_{n}$ with linear independent eigenvectors $x_1, \ldots , x_{n}$ and $\rho(G_{J}) < 1$. Then the iteration matrix $G_{J}(\omega )$ has eigenvalues
\[
\mu _{i} =  1 - \omega - \omega \lambda _{i}
\] 
and 
\[
\omega _{\text{opt}} = \frac{2}{2-\lambda_1-\lambda _{n}}
\] 

Proof can be found in [A. Meister Numerik linearer Gleichungssysteme]
\end{theorem}

\begin{figure}[H]
	\begin{center}
		\input{tikz/chapter1/eigenwerte.tex}
	\end{center}
	\caption{eigenvalues lie on opposite sites on the unit sphere}
	\label{fig:eigenwerte}
\end{figure}


\underline{Note}: So in case of symmetric eigenvalue around the origin we get $\omega _{\text{opt}}=1$

\begin{theorem}
\label{thm:theorem2.5}
	Let $A \in \R^{n \times n}$ with $a_{ii} \neq 0$
	\[
		\rho(G_{GS}(\omega )) \geq \abs{\omega -1} 
	.\] 
\end{theorem}

\begin{proof}
\label{thm:theorem2.5proof}
Let $\lambda _{1}, \ldots , \lambda _{n}$ be eigenvectors of $G_{GS}(\omega )$.
\begin{align*}
	\prod_{i=1}^{n} \lambda _{i} &= \det G_{GS}(\omega ) \\
								 &= \det ((D-\omega L)^{-1}) \det ((1+\omega ) D+ \omega U) \\
								 &= \det (D^{-1}) \det((1-\omega )D) \\
								 &= \det D^{-1} (1-\omega )^{n} \det D \\
								 &= (1-\omega )^{n}
\end{align*}

\end{proof}
