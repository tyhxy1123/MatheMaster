\chapter{Basic iterative methods}%
\label{cha:Basic iterative methods}

We want to solve the linear system
\[
	Au = f \qquad (Ax = h)
.\] 
\begin{itemize}
	\item traditional directl methodes (Gauss elemination) are insufficient:
		\begin{itemize}
			\item $\O(n^{4})$ steps
			\item $\O(n^{3})$ storage units
		\end{itemize}

	\item Iterative methods try to improve an implicit guess until some tolerance criterion is reached
		\begin{itemize}
			\item $\O(n^{2})$ storage units
		\end{itemize}
\end{itemize}

\section{Relaxation methods}%
\label{sec:Relaxation methods}

Basic relaxation methods are based on a coordinate-wise relaxation/reduction of an initial residuum
\[
\gamma = b- Ax
.\] 
In each relaxation step these methodes try to annihilate a component of the residual.

The general idea is to transform the linear system into a fixed point problem
\[
	x = x + P^{-1}(b-Ax)
,\] 
where $P$ is called a proconditioner (matrix) and should be easy to invert in the sense that $Pe=r$ can be solved easily and $P$ should be an approximation of matrix $A$.

Basic idea to solve a fixed-point problem is a fixed-point iteration
\[
	x^{k+1} = x^{k} + P^{-1}(b-Ax^{k})
.\] 
Rewriting this to make it a bit prettier results in
\[
	x^{k+1} = Gx^{k} + g =:T(x^{k})
.\] 
where $G = I-P^{-1}A$ is the iteration matrix and $g = P^{-1}b$.

\subsection{Some examples of relaxation schemes}
\label{sec:Some examples of relaxation schemes}

\begin{enumerate}[label=\Alph{enumi})]
	\item Jacobi method 

		Let $P_{J} = \diag(A)$, then obviously
		\[
			x^{k+1} = x^{k} + \diag(A)^{-1}(b-Ax^{k})
		.\] 
		written in terms of the components
		\[
			x^{k}= (\xi _{i}^{k})_{i}, \quad b=(\beta_{i})_{i}
		\] 
		gives
		\[
			a_{ii}\xi _{i}^{(k+1)} = -\sum_{\substack{j=1 \\ j \neq i}}^{n}{a_{ij}\xi _{j}^{(k)}+ \beta_{i}}
		.\] 
		Let $A=D-L-U$ be a splitting of the matrix $A$ into diagonal $D$, lower triangular part $L$ and upper triangular part $U$
		\[
		A = \begin{pmatrix}
			\diagdown & & -U \\
			  &D&    \\
			-L & & \diagdown
		\end{pmatrix}
		.\] 
		Then $P_{J} = D$
		\[
			Dx^{k+1} = (L+U)x^{k} + b
		.\] 
		For the 5-point stencil $\laplace_{h}^{(5)}$ we have 
		\[
			D = \diag(\frac{4}{h^{2}}) 		
		\] 
		and the matrix $L+U$ looks like the following image.
		\begin{figure}[H]
			\begin{center}
				\includegraphics[width=0.5\textwidth]{pics/chapter1/huebschematrix.png}
			\end{center}
			\caption{$L+U$}
			\label{fig:huebschematrix}
		\end{figure}
		
		So written directly in terms of the grid vector $u_{i,j}$ we get the Jacobi scheme for inner nodes
		\[
			u_{i,j}^{k+1} = \frac{1}{4}(u_{i,j-1}^{k}+ u_{i-1,j}^{k} + u_{i,j+1}^{k} + u_{i+1,j}^{k} + h^{2}f_{i,j})
		.\] 
		\underline{Note}: Instead of $P=D$ one could implement a \underline{damped Jacobi scheme} with
		\[
			P_{J}(\omega )=\omega \cdot D \qquad \text{ with } 0 < \omega \leq 1
		.\] 
	\item Gauss-Seidel Relaxation

		Instead of diagonal $P$, we use triangular matrix
		\[
			\underset{\text{ (forward) }}{P_{GS} = D-L} \quad\text{ or }\quad \underset{\text{ (backward) }}{P_{GS'}=D-U}
		.\] 
		\begin{figure}[H]
			\begin{center}
				\input{tikz/chapter1/forward_backward_iteration.tex}
			\end{center}
			\caption{forward and backward iteration matrix form}
			\label{fig:forwardbackward}
		\end{figure}
		
		We get the component-wise scheme
		\[
			\xi _{i}^{(k+1)}=\frac{1}{a_{ii}} \Big( -\sum_{j=1}^{i-1}{a_{ij}} \xi _{j}^{(k+1)}-\sum_{j=i+1}^{n}{a_{ij}\xi _{j}^{k}} + \beta _{i} \Big) \qquad i=1, \ldots, n
		\] 
		or (backward)
		\[
			\xi _{i}^{(k+1)} = \frac{1}{a_{ii}} \Big( -\sum_{j=1}^{i-1}{a_{ij}\xi _{i}^{(k)}}-\sum_{j=i+1}^{n}{a_{ij}\xi _{j}^{(k+1)}} + \beta _{i} \Big) \qquad i=n, \ldots ,1
		.\] 
		\underline{Note} : A damped relaxation method uses:
		\[
			P_{GS}(\omega ) := \frac{1}{\omega }(D-L) \quad \text{ or } \quad P_{GS'}(\omega ) := \frac{1}{\omega }(D-U)
		\] 
		i.e. (forward)
		\[
			\xi _{i}^{(k+1)} = (1-\omega )\xi _{i}^{(k)} + \frac{\omega }{a_{ii}} \Big( -\sum_{j=1}^{i-1}{a_{ij}\xi _{j}^{(k+1)}} - \sum_{j=i+1}^{n}{a_{ij}\xi _{j}^{(k)}} + \beta _{i} \Big) 
		.\] 
	\item Richardson relaxation
		\begin{align*}
			P_{R}(\omega ) :&= \frac{1}{\omega } I \\
			\xi _{i}^{(k+1)} &= \xi _{i}^{(k)} + \omega \Big( -\sum_{j=1}^{n}{a_{ij}\xi _{j}^{(k)}} + \beta _{i} \Big) 
		\end{align*}
\end{enumerate}
		Questions:
		\begin{itemize}
			\item How to characterize convergence?
			\item How to choose the relaxation parameter $\omega $?
		\end{itemize}

		Denote by $e^{k} := A^{-1}b - x^{k}$ the error in the k-th iteration and by $G:=I-P^{-1}A$ the iteration matrix. So
		\[
			e^{k+1}= (I-P^{-1}A)e^{k} = Ge^{k} = G^{2}e^{k-1} = \cdots = G^{k+1}e^{0}
		.\] 
		Taking the norm on both sides results in
		\[
			\norm{e^{k+1}}  \leq \normiii{(I-P^{-1}A)^{k+1}} \cdot \norm{e^{0}} 
		\] 
		with vector norm $\norm{\cdot}$ and induced matrix norm $\normiii{\cdot}$.
		\begin{lemma}
		\label{thm:richardconvergencelemma}
		The error norm $\norm{e^{k}} \rightarrow 0$ ( for $k \rightarrow \infty$) and $x^{k} \rightarrow A^{-1}b$ for all initial errors $e^{0}$ $\iff$
		\[
			\lim_{k \to \infty} \normiii{(I-P^{-1}A)^{k}} = 0
		.\] 
		The proof is omitted here and can be read in 
		[Greenbaum 97. Lemma 2.11].
		\end{lemma}

		\begin{theorem}
		\label{thm:richardconvergencetheorem}
			$x^{k} \rightarrow A^{-1}b$ for $k \rightarrow \infty$ for all initial values $x^{0}$ $\iff$
			\[
				\rho(I-P^{-1}A) < 1
			.\] 
			(with $\rho(\cdot)$ the spectral radius of the matrix)
		\end{theorem}
		\begin{proof}\
		\label{thm:richardconvergencetheoremproof}
		\begin{itemize}
			\item "$\Longrightarrow$" 

				Assume that $x^{k}\rightarrow A^{-1}b$ converges. 
				Let $\lambda $ be the eigenvalue of $G$ with $\abs{\lambda} = \rho(G)$ and $x$ be the associated eigenvector.
				
				Choose $b=0$ and $x^{0}=cx$ for any $c \in \R \setminus \left\{ 0 \right\} $. We get
				\[
				x^{k+1} = Gx^{k} = G^{k+1}x^{0} = \lambda ^{k+1} x^{0}
				.\] 
				\[
					\begin{array}{rl}
						\abs{\lambda } >1 &\implies \text{ divergence } \\
						\abs{\lambda }  = 1 &\implies \text{ no convergence }
				\end{array}
				\lightning
				.\] 
			\item "$\Longleftarrow$"

				Assume $\rho(G)<1$.

				There exists an $\epsilon >0$ and norm in $\R^{n \times n}$ such that.
				\[
					\normiii{G} \leq \rho(G) + \epsilon  < 1
				\] 
				with the previous lemma we get convergence.
		\end{itemize}
		[also see Y. Saad 03. Corollary 4.2]
		\end{proof}

\begin{definition}
\label{thm:additivesplitting}
Let $A, M, N$ be three given matrices with $ A = M-N$. The pair $(M,N)$ is called a \underline{regular splitting} of $A$ if
\begin{itemize}
	\item $M$ is non-singular
	\item $M^{-1}, N$ are non-negative (i.e. all entries are non-negative) ($M^{-1} \geq 0, N \geq 0$)
\end{itemize}
\end{definition}
With this splitting, the relaxation scheme can be written as 
\[
	x^{k+1} = \underbrace{M^{-1}N}_{G} x^{k} + M^{-1}b
.\] 
Thus
\begin{align*}
M \overset{\wedge}&{=} P \\
N \overset{\wedge}&{=} P-A
\end{align*}

\begin{theorem}
\label{thm:regularsplittingtheorem}
Let $M,N$ be a regular splitting of $A$. Then $\rho(\underbrace{M^{-1}N}_{G} ) < 1$ $\iff$

A is non-singular and $A^{-1}$ is non-negative.
\end{theorem}

\begin{proof}\
\label{thm:regularsplittingtheoremproof}
\begin{itemize}
	\item "$\Longrightarrow$"

		Assume $\rho(G) < 1$. 
		\[
			A = \underbrace{M}_{\text{ non-sing. }}\cdot \underbrace{(I-G)}_{\text{ non-sing. }} = M -MG = M-N
		.\] 
		So $A$ is non-singular as a product of non-singular matrices.
		\[
			(I-G)^{-1} = \sum_{i=0}^{\infty}{G^{i}}
		\] 
		with $G$ non-negative. So the powers $G^{i}$ are non-negative. Hence $(I-G)^{-1}$ is non-negative.
		\[
		\implies A^{-1} \text{ is non-negative. }
		.\] 
	\item "$\Longleftarrow$"

		Assume $A$ is non-singular and $A^{-1}$ is non-negative.

		Since $A$ and $M$ are non-singular, the matrix $(I-G)$ is non-singular.
		\begin{align*}
			A^{-1}N &= (M(I-M^{-1}N))^{-1}N \\
					&= (I-M^{-1}N)^{-1}M^{-1}N \\
					\tag{$\ast\ast$}
					&= (I-G)^{-1}G
		\end{align*}

		\[
		M^{-1}, N \geq 0 \qquad G = M^{-1}N \geq 0
		.\] 

		Let $\lambda $ be an eigenvalue of $G$ with $\abs{\lambda }=\rho(G)$ and associated eigenvector $x$ with $x \geq 0$ (Perron-Frobenius)
		\[
			Gx = \rho(G)x
		\] 
		and with $(\ast\ast)$ we get $A^{-1}Nx = (I-G)^{-1}x = \frac{\rho(G)}{1 - \rho(G)}x$.

		This implies
		\[
			\frac{\rho(G)}{1-\rho(G)} \geq 0
		\] 
		and so $0 \leq \rho(G) \leq 1$.

		Since $(I-G)$ is non-singular, $\rho(G) \neq 1$.

		[also see Y.Saad 03 Th.4.4]
\end{itemize}
\end{proof}

\begin{lemma}
\label{thm:matrixequivalences}
	Let $A \in \R^{n \times n}$ with $a_{ij} \leq 0, i \neq j$. Then the following statements are equivalent:
	\begin{enumerate}[label=\arabic{enumi})]
		\item $A$ is an M-matrix
		\item $A$ is non-singular and $A^{-1} \geq 0$ (and $a_{ii} > 0$)
		\item $a_{ii} > 0$ and $A\cdot D$ is strictly (row-) diagonally dominant for some diagonal matrix $D$
	\end{enumerate}
	
	For a proof see [Horn, Johnson : Topics of matrix analysis, 91']
\end{lemma}

\begin{theorem}
\label{thm:theorem2.3}
	If $A$ is strictly diagonally dominant or irreducibly diagonally dominant, then the associated Jacobi and GS iteration scheme converges for any initial vector $x^{0}$.
\end{theorem}

\begin{proof}
\label{thm:theorem2.3proof}
	[Y. Saad 03 Theorem 4.9]
\end{proof}

What is the "optimal" relaxation parameter?

In which sense "optimal"?

Denote by $G(\omega )=I-P^{-1}(\omega )A$ the iteration matrix induced by $P(\omega )$.

We say $\omega $ is optimal if
\[
	\omega  = \argmin_{\alpha \in \R^{+}} \rho(G(\alpha ))
.\] 
An iteration scheme is called \underline{underrelaxation method} if $\omega  < 1$. Otherwise it is called \underline{overrelaxation method}.

\begin{theorem}
\label{thm:theorem2.4}
Let $G_{J}$ be the iteration matrix of the Jacobi relaxation and assume	$G_{J}$ has only real eigenvalues $\lambda_1 \leq \lambda_2 \leq \ldots \leq \lambda_{n}$ with linear independent eigenvectors $x_1, \ldots , x_{n}$ and $\rho(G_{J}) < 1$. Then the iteration matrix $G_{J}(\omega )$ has eigenvalues
\[
\mu _{i} =  1 - \omega - \omega \lambda _{i}
\] 
and 
\[
\omega _{\text{opt}} = \frac{2}{2-\lambda_1-\lambda _{n}}
\] 

Proof can be found in [A. Meister Numerik linearer Gleichungssysteme]
\end{theorem}

\begin{figure}[H]
	\begin{center}
		\input{tikz/chapter1/eigenwerte.tex}
	\end{center}
	\caption{eigenvalues lie on opposite sites on the unit sphere}
	\label{fig:eigenwerte}
\end{figure}


\underline{Note}: So in case of symmetric eigenvalue around the origin we get $\omega _{\text{opt}}=1$

\begin{theorem}
\label{thm:theorem2.5}
	Let $A \in \R^{n \times n}$ with $a_{ii} \neq 0$
	\[
		\rho(G_{GS}(\omega )) \geq \abs{\omega -1} 
	.\] 
\end{theorem}

\begin{proof}
\label{thm:theorem2.5proof}
Let $\lambda _{1}, \ldots , \lambda _{n}$ be eigenvectors of $G_{GS}(\omega )$.
\begin{align*}
	\prod_{i=1}^{n} \lambda _{i} &= \det G_{GS}(\omega ) \\
								 &= \det ((D-\omega L)^{-1}) \det ((1+\omega ) D+ \omega U) \\
								 &= \det (D^{-1}) \det((1-\omega )D) \\
								 &= \det D^{-1} (1-\omega )^{n} \det D \\
								 &= (1-\omega )^{n}
\end{align*}

\end{proof}

\begin{mdframed}
	\underline{Remark}  : The script might miss some parts here due to an interactive reading course lecture experiment.

	For damped iteration schemes (Gauss-Seidel \& Richardson) we considered the questions:
	\begin{enumerate}[label=(\arabic{enumi})]
		\item For which $\omega $ do we get convergence?
		\item What is the spectral radius $\rho(G(\omega ))$?
		\item What is the optimal parameter choice $\omega _{\text{opt}}$?
		\item Requirements on the matrix $A$?
		\item How to choose $\omega $ iteratively?
	\end{enumerate}
	For answers look here:
	\begin{enumerate}[label=(\alph{enumi})]
		\item Gauss-Seidel / SOR (successive overrelaxation)
			\begin{itemize}
				\item Y. Saad, Ch. 4.2.5, Thm. 4.1.6
				\item A. Greenbaum, Ch. 10.11, Thm 10.1.3
				\item W. Hackbusch, Ch. 4.4.3
				\item A. Meister, Ch. 4.1.3.2
			\end{itemize}
		\item Richardson
			\begin{itemize}
				\item Y. Saad, Example 4.1
				\item W. Hackbusch, Ch. 4.4.1
				\item A. Meister, Ch. 4.1.4
			\end{itemize}
	\end{enumerate}
\end{mdframed}

\begin{mdframed}
%\label{thm:}
	\underline{Results for GS/SOR method}:

	\begin{thm}
	%\label{thm:}
		Let $A = I - L - U$ with
		\begin{equation}\label{eq:gsstar}\tag{$\ast$}
			\forall c \in \R ~ \forall\gamma  \in \R \setminus \left\{ 0 \right\}: \qquad \det(cI-L-U) = \det(cI-\gamma L - \gamma ^{-1}U) 
		\end{equation}
		and $\lambda \neq 0$ is eigenvalue of $G_{GS}(\omega )$ for $\omega \in(0,2)$, then
		\[
		\mu  = \frac{\lambda +\omega +1}{\omega \lambda ^{\frac{1}{2}}} \text{ is eigenvalue of }G_{J}
		.\] 
	\end{thm}
	\begin{thm}
	%\label{thm:}
		If
		\begin{itemize}
			\item $A$ satisfies \href{eq:gsstar}{($\ast$)} 
			\item $G_{J}$ only real eigenvalues
			\item $\beta = \rho (G_{J}) < 1$
		\end{itemize}
		Then
		\begin{itemize}
			\item SOR converges
			\item \[
					\rho (G_{GS}(\omega )) = \begin{cases}
						\frac{1}{4} \left[ \omega \beta + \sqrt{(\omega \beta )^2 + 4(\omega +1)}  \right]^2 & \text{ for }0 < \omega \leq \omega _{opt} \\
						\omega -1 & \text{ for } \omega _{opt} \leq  \omega < 2
					\end{cases}
			.\] 
		\item $\omega _{opt} = \frac{2}{1 +\sqrt{1-\beta ^2} }$
		\end{itemize}
	\end{thm}

	\underline{Results for Richardson method} : N/A
\end{mdframed}

\section{Symmetric splitting schemes}
\label{sec:Symmetric splitting schemes}

Let $P$
\begin{itemize}
	\item easy to invert
	\item approx. $A$
\end{itemize}
If $P$ is symmetric and positive definite, we could rewrite our linear system as:
\[
	\left.
	\begin{array}{rl}
	\underbrace{P^{-\frac{1}{2}}A P^{-\frac{1}{2}}}_{\tilde{A}} y &= \underbrace{ P^{-\frac{1}{2}}b}_{\tilde{b}} \\
	P^{-\frac{1}{2}}y &= x
	\end{array}
	\right\}
	\tilde{A} y = \tilde{b}
.\] 
If A ist positive definite then $\tilde{A}$ is also positive definite. This procedure is called "(symmetric) preconditioning" of $Ax = b$.

We have
\begin{itemize}
	\item Jacobi preconditioner $P_{J}$ is symmetric and for $\omega , d_{ii} \in \R_{>0}$ also positive definite.
	\item Richardson preconditioner $P_{R}$ is symmetric and positive definite for $\omega \in \R_{>0}$.
	\item Gauss-Seidel preconditioner $P_{GS}(\omega )$ is not symmetric.
\end{itemize}

\underline{How to symmetrize GS?} 

We try to combine forward and backward GS
	\begin{align*}
		(\omega ^{-1}D-L) x^{k+\frac{1}{2}} &= b -\left( \frac{\omega -1}{\omega }D-U \right) x^{k} \\
		(w^{-1}D-U) x^{k+1} &= b- \left( \frac{\omega -1}{\omega }D-L \right) x^{k+\frac{1}{2}} \\
							&= b - \left( \frac{\omega -1}{\omega }D-L \right) (\omega ^{-1}D-L)^{-1}\Big(b-\left(\frac{\omega -1}{\omega }D-U\right)x^{k}\Big)
	\end{align*}
	It follows
	\[
	x^{k+1}=Gx^{k} + P^{-1}b
	\] 
	with
	\begin{align*}
		G &= (D-\omega U)^{-1} \Big( (1-\omega )D+\omega L)\Big)(D-\omega L)^{-1} \Big( (1-\omega ) D + \omega U \Big) \\
		P^{-1}&= \omega (2-\omega )(D-\omega U)^{-1}D (D-\omega L)^{-1}
	\end{align*}
	\[
\implies P \text{ symmetric positive definite }	
	.\] 
	\[
		\implies \text{ scheme is called symmetric SOR \underline{(SSOR)} } (\text{ if } \omega > 1)
	.\] 

\section{Block-relaxation methods}%
\label{sec:Block-relaxation methods}

Instead of eliminating individual components of the residual, we could work with blocks of components:

Let
\[
A = \begin{bmatrix}
\begin{bmatrix}
A_{11}
\end{bmatrix}
&\cdots & 
\begin{bmatrix}
A_{1p}
\end{bmatrix} \\
\vdots &\ddots& \vdots\\
\begin{bmatrix}
A_{p1}
\end{bmatrix}
&\cdots & 
\begin{bmatrix}
A_{pp}
\end{bmatrix}
\end{bmatrix}
\qquad
x = \begin{bmatrix}
x_1 \\ \vdots \\ x_{p}
\end{bmatrix}
\qquad
b = \begin{bmatrix}
b_1 \\ \vdots \\ b_{p}
\end{bmatrix}
.\] 
One block of a matrix-vector product $Ax$:
\[
	(Ax)_{i} =  \sum_{j=1}^{p}{A_{ij}x_{j}}
.\] 
Analogously to the point-relaxation schemes, we introduce a splitting $A = D-L-U$ with
\[
D=
\begin{bmatrix}
	\begin{bmatrix}
	A_{11}
	\end{bmatrix}
	\\
	&\begin{bmatrix}
	A_{22}
	\end{bmatrix}
	\\
	&&\ddots
	\\
	&&&\begin{bmatrix}
	A_{pp}
	\end{bmatrix}
\end{bmatrix}
\qquad
L = \begin{bmatrix}
0 \\
\begin{bmatrix}
	A_{21}
\end{bmatrix}
& 0 \\
&\begin{bmatrix}
A_{32}
\end{bmatrix}
\\
&&\ddots \\
&&&\begin{bmatrix}
A_{p p-1}
\end{bmatrix}
& 0
\end{bmatrix}
\] 
and $U=D-L-A$.

Thus for the Jacobi method, we get the scheme
\[
	A_{ii}x_{i}^{(k+1)} = b_{i}+\Big( (L+U)x^{k} \Big) _{i} = b_{i} - \sum_{j \neq i}^{}{A_{ij}x_{j}^{(k)}}
.\] 
Typically $A_{ii}$ are much smaller than $A$ and sometimes simple to invert.
\begin{exam}
%\label{thm:}
	Finite Differences $A=\laplace_{h}^{(5)}$ with lexicographical ordering:
	\[
	A_{ii} = A_{0} = \frac{1}{h^2}\begin{bmatrix}
		4 & -1 \\
		-1 & 4 \\
		&&\ddots \\
		&&&4& -1\\
		&&&-1 & 4
	\end{bmatrix}
	\in \R^{(N_{x}+2) \times (N_{x} +2)}
	.\] 
	This matrix is tridiagonal.
	\[
	L+U = \frac{1}{h^2}\begin{bmatrix}
	& I \\
	I && \ddots \\
  	&\ddots &&I \\
	&&I
	\end{bmatrix}
	.\] 
	with $n_1 = n_2 = \ldots  N_{x} +2$ and $p=N_{y}+2$

	\begin{itemize}
		\item efficient Algorithm: Thomas Algorithm
		\item all blocks are the same
		\item factorize only once
	\end{itemize}
\end{exam}

\subsection{General block concept}
\label{sec:General block concept}

\underline{Basic iteration}

Let $S=\left\{ 1, \ldots, n \right\} $ be the overall index set. Partition $S$ into subsets $S_{i}$ ($i=1, \ldots, p$) (not necessarily disjunct) with
\[
S_{i}\subseteq S \qquad \bigcup_{i=1, \ldots, p}S_{i} = S
.\] 
If $S_{i}\cap S_{j} = \emptyset$ ($i \neq j$) we call this a \underline{non-overlapping} partitioning of $S$, otherwise a \underline{overlapping} partitioning.\enter

In the following let $n_{i}= \abs{S_{i}} $ be the size of the subset $S_{i}$ and 
\[
S_{i}= \left\{ m_{i}(1), m_{i}(2), \ldots , m_{i}(n_{i}) \right\} 
.\] 

To restrict the rows/columns of a matrix to the subsets, we use a transformation matrix
\[
	V_{i}= \left[ e_{m_{i}(1)}, e_{m_{i}(2)}, \ldots , e_{m_{i}(n_{i})} \right] \in \R^{n \times n_{i}}
.\] 
Additionally we introduce the matrices $W_{i} \in \R^{n \times n}$ such that
\[
\sum_{i=1}^{p}{V_{i}W_{i}^{T}} = I \in \R^{n \times n}
.\] 
Therefor choose $\eta _{m_{i}(j)} \in \R$ such that
\[
	W_{i} = \left[ \eta _{m_{i}(1)}e_{m_{i}(1)}, \ldots , \eta _{m_{i}(n_{i})}e_{m_{i}(n_{i})} \right] 
,\] 
e.g.
\[
\eta _{i} = \frac{1}{s_{i}} \quad\text{ where }\quad s_{i} = \# \left\{  l \in \left\{ 1, \ldots, p \right\} | i \in S_{l} \right\} 
.\] 
Then
\[
	W_{i} = \diag(\eta _{m_{i}(1)}, \ldots , \eta _{m_{i}(n_{i}}) V_{i}
.\] 

\begin{exam}
%\label{thm:}
	Let the index set $S= \left\{ 1,2,3,4,5 \right\} $ with two subsets $S_1 = \left\{  1,2,3 \right\} $ and $S_2 = \left\{ 3,4,5 \right\} $. Then we have
	\[
	V_{1}= \begin{bmatrix}
	1 \\
	& 1 \\
	&&1 \\
	&&0 \\
	&&0 
	\end{bmatrix}
	\quad
	V_2 = \begin{bmatrix}
	0 \\
	0 \\
	1 \\
	& 1 \\
	&& 1
	\end{bmatrix}
	\quad \text{ and } \quad
	W_1 = \begin{bmatrix}
	1 \\
	& 1 \\
	&&0.5 \\
	&&0 \\
	&&0
	\end{bmatrix}
	\quad
	W_2 = \begin{bmatrix}
	0 \\
	0 \\
	0.5 \\
	& 1\\ 
	&& 1
	\end{bmatrix}
	.\] 
\end{exam}

We introduce the $n_{i} \times n_{j}$ block matrices $A_{ij}$ and vectors $x_{i}, b_{i}$ as 
\[
A_{ij} = W_{i}^{T}AV_{j}, \qquad x_{i} = W_{i}^{T}x, \qquad b_{i}= W_{i}^{T}b
.\] 
Thus the solution vector $x$ can be expressed as 
\[
x = \sum_{i=1}^{p}{V_{i}x_{i}}
.\] 
The matrix $V_{i}W_{i}^{T}$ is a projector from $\R^{n}$ to a subspace $\mathcal{K}_{i} \subset \R^{n}$ spanned by the columns of $V_{i}$. We call $W_{i}^{T}$ a \underline{restriction operator} and $V_{i}$ a \underline{prolongation operator}.

\subsection*{Projection procedure}
\begin{itemize}
	\item extract approximate solution to $Ax=b$ in subspace of $\R^{n}$
	\item subspace $\mathcal{K}$ is called "search space" / "trial space" and $\dim \mathcal{K} = m$
	\item $m$ constraints must be imposed to extract such an approximation, i.e.
		\begin{itemize}
			\item Residual $r = b -Ax$ is constrained to be orthogonal to $m$-dim subspace $\mathcal{L} \subset  \R^{n}$
			\item $\mathcal{L}$ is called "constraints space" / test space
			\item $\mathcal{K}$ and $\mathcal{L}$ can be the same $\rightarrow$ orthogonal method
			\item $\mathcal{K}$ and $\mathcal{L}$ can be different $\rightarrow$ oblique method
		\end{itemize}
\end{itemize}

\subsection*{General procedure}
\label{sec:General procedure}

Let $x_0$ be an initial guess. Find $x \in x_0 + \mathcal{K}$ such that
\begin{equation}\label{eq:generalproceedstar}\tag{$\ast$}
	b - Ax ~\bot~ \mathcal{L}
\end{equation}
So, if $x$ is written as $x=x_0 + \delta $ and $r_0 = b -Ax_0$ then \href{eq:generalproceedstar}{($\ast$)} means
\begin{align*}
	&b-a(x_0 + \delta ) ~\bot~\mathcal{L} \\
	\implies &r_0 - A \delta ~\bot~ \mathcal{L}
\end{align*}
That is:
\[
	\text{ Find } x = x_0 + \delta \quad \delta  \in \mathcal{K} \quad\text{ s.t. }\quad (r_0 - A \delta  , \omega ) = 0 \quad \forall \omega  \in \mathcal{L}
.\] 

In the following, we assume that a space $\mathcal{K}_{i}$ is spanned by the columns of $V_{i}$ and $\mathcal{L}_{i}$ is spanned by the columns of $W_{i}$ respectively.
Let
\[
x^{k+1} = x^{k} + \underbrace{V_{i}d_{i}}_{=\delta } 
\] 
and
\[
r^{k} = b - Ax^{k}
\] 
then we get for the new residual
\begin{align*}
	r^{k+1} &= b - Ax^{k+1} \\
			&= b- A(x^{k} + V_{i}d_{i}) \\
			&= r^{k} - AV_{i}d_{i}
\end{align*}
The orthogonality condition reads:
\begin{align*}
	0 = W_{i}^{T}r^{k+1} = W_{i}^{T}(r^{k}- AV_{i}d_{i}) \\
	\implies W_{i}^{T} r^{k} = \underbrace{W_{i}^{T} A V_{i}}_{= A_{ii}} d_{i} 
\end{align*}

If $A_{ii}$ is non-singular, we get
\[
d_{i} = A_{ii}^{-1} W_{i}^{T} r^{k}
\] 
and thus
\[
x^{k+1} = x^{k}+ V_{i}A_{ii}^{-1}W_{i}^{T} r^{k}
.\] 

% TODO : ordentlichen Pseudocode hier

\subsection*{Block-Jacobi-Iteration}
\label{sec:Block-Jacobi-Iteration}
"additive projection procedure"\\

FOR $i=1, \ldots, p$\\
\tab	solve $A_{ii}d_{i} = W_{i}^{T}(b-Ax^{k})$\\
END FOR\\
$x^{k+1}= x^{k} + \sum_{i=1}^{p}{\color{orange}\omega _{i}\color{black}V_{i}d_{i}}$

\subsection*{Block-Gaus-Seidel-Iteration}
\label{sec:Block-Jacobi-Iteration}
"multiplicative projection procedure"\\

Let $x^{k+1} = x^{k}$\\
FOR $i=1, \ldots, p$\\
\tab 	$A_{ii}d_{i} = W_{i}^{T}(b-Ax^{k+1})$\\
\tab 	$x^{k+1} = x^{k+1} +\color{orange}\omega _{i}\color{black} V_{i}d_{i}$\\
END FOR

\subsection*{Multi-Splitting-Parallelization}
\label{sec:Multi-Splitting-Parallelization}

\begin{definition}
%\label{thm:}
	Let $A, B_{k}, C_{k}, D_{k}\in \R^{n \times n}$.
	The matrices $B_{k}, C_{k}, D_{k}$ are called a \underline{multi-splitting} of $A$ if
	\begin{enumerate}[label=(\arabic{enumi})]
		\item $A = B_{k} - C_{k}$ for $i=1, \ldots, p$, where $B_{k}$ is invertible
		\item $\sum_{k}^{}{D_{k}} = I$, where $D_{k}$ is diagonal matrix with $D_{k} \geq  0$
	\end{enumerate}
\end{definition}

Algorithm:

Choose $x_0$ arbitrarily and $\omega $\\
FOR $k=1,\ldots $ \\
\tab	$\overline{x} = x_{i}$\\
\tab	FOR $i=1, \ldots, p$\\
\tab	Find $D_{k}y_{k}$ where $y_{k}$ satisfies $B_{k}y_{k} = C_{k}\overline{x}+b$\\
\tab	END FOR\\
\tab	$x_{k+1} = (1-\omega ) x_{k} + \omega \sum_{k}^{}{D_{k}y_{k}}$\\
END FOR
