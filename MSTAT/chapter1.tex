% This work is licensed under the Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
% of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
% send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\addchap{Mathematische Statistik}
Viele Schätzer in der Statistik sind definiert als Minimal- oder Maximalstelle von bestimmten \textit{Kriteriumsfunktionen}, z. B. der \textit{Maximum-Likelihood-Schätzer (MLS)} oder \textit{Minimum-Qudrat-Schätzer (MQS, KQS)} oder \textit{Bayes-Schätzer}. Allgemein nennt man solche Schätzer \textbf{M-Schätzer}.\\
Ziel: Untersuchung des asymptotischen Verhaltens ($n\to\infty$) von M-Schätzern über einen \textit{funktionalen Ansatz}. Als Beispiel:

\section{Der Median}
Sei $X:(\Omega,\A,\P)\to\R$ eine reelle Zufallsvariable mit Verteilungsfunktion\\ $F_X:\R\to[0,1],~F_X(x):=\P[X\leq x]$, also $X\sim F_X$. Definiere

\begin{align}
Y(t)&:=\E\left(|X-t|\right)\label{DefY}\tag{1.0}\\ \nonumber
&=\int\limits_\Omega |X(\omega)-t|\d\P(\omega)\\ \nonumber
\overset{\text{Trafo}}&=
\int\limits_\R|x-t|\Big(\P\circ X\d x\Big)\\ \nonumber
&=\int\limits_\R|x-t|(F\d x) \nonumber
\qquad\forall t\in\R\\
m&:=\arg\min\limits_{t\in\R}Y(t):=\text{ (irgendeine) Minimalstelle der Funktion}\nonumber
\end{align} 

\begin{notation}
$F(m-):=F(m-0):=\lim\limits_{t\uparrow m} F(t)$
\end{notation}

Charakterisierung der Menge aller Mediane in folgendem kleinen Lemma:

\begin{lemma}\label{lemmaMedian}
Sei $X\sim F_X$ integrierbar und $m\in\R$. Dann äquivalent:
\begin{enumerate}[label=(\alph*)]
\item $F(m-)\leq\frac{1}{2}\leq F(m)$
\item $\E[|X-t|]\geq\E[|X-m|]\qquad\forall t\in\R$
\item $m$ ist Median
\end{enumerate}
\end{lemma}

\begin{proof}
\underline{Zeige (a) $\Rightarrow$ (b):}\\
Setze $h(t):=\E[|X-t|-|X-m|]\stackeq{\text{Lin}}Y(t)-Y(m)$. Dann ist 2. äquivalent zu $h(t)\geq0~\forall t\in\R$. Dies ist noch zu zeigen.\nl
\underline{Fall 1: $t<m$}
\begin{align*}
&h(t)\\
\overset{\text{Trafo}}&=
\int\limits_{\R}|x-t|-|x-m| Q_F(\d x)\\
&=\int\limits_{(-\infty,t]}\underbrace{|x-t|-|x-m|}_{=t-x-(m-x)=-(m-t)} F(\d x)
+\int\limits_{(t,m)}\underbrace{\underbrace{|x-t|-|x-m|}_{\underbrace{x-t}_{\geq0}-\underbrace{(m-x)}_{\leq m-t}}}_{\geq-(m-t)} F(\d x)\\
&\qquad
+\int\limits_{[m,\infty)}\underbrace{|x-t|-|x-m|}_{x-t-(x-m)=m-t} F(\d x)\\
&\geq-(m-t)\cdot \underbrace{Q((-\infty,t])}_{F(t)}+
\Big(-(m-t)\cdot F(m-)-F(t)\Big)
+(m-t)\cdot\underbrace{Q([m,\infty))}_{1-\underbrace{Q((-m,m))}_{F(m-)}}\\
&=-\underbrace{(m-t)}_{\geq0}\cdot(\underbrace{1-2\cdot F(m-)}_{\stackrel{1.}{\geq}0})\\
&\geq0
\end{align*}

\underline{Fall 2: $t> m$}
\begin{align*}
h(t)&=\int\limits_{(-\infty,m]}\ldots F(\d x)+\int\limits_{(m,t]}\ldots+\int\limits_{(t,\infty)}\ldots F(\d x)\\
&\ldots\\
&\geq(t-m)\cdot(\underbrace{2\cdot F(m)-1}_{\stackrel{1.}{\geq}0})\\
&\geq0
\end{align*}

\underline{Fall 3: $t=m$} ist trivial. $\#$\nl
\underline{Zeige (b) $\Rightarrow$ (a):}\\
Nach Annahme ist $h(t)\geq0~\forall t\in\R$.\nl
\underline{Fall 1: $t<m$} Die obige Rechnung im Fall 1 bei 1. $\Rightarrow$ 2. zeigt:
\begin{align*}
0\leq h(t)&=-(m-t)\cdot F(t)+\int\limits_{(t,m)}\underbrace{\underbrace{x}_{<m}-t-(m-x) }_{=2x-t-m\leq m-t}F(\d x)+(m-t)\cdot(1-F(m-))\\
&\leq-(m-t)\cdot\Big(F(t)-1\underbrace{+F(m-)-F(m-)}_{=0}+F(t)\Big)\\
&=\underbrace{(m-t)}_{>0}\cdot(1-2\cdot F(t))\\
&\Longrightarrow\forall t<m:0\leq(m-t)\cdot(1-2\cdot F(t))\\
&\Longrightarrow\forall t<m:0\leq 1-2\cdot F(t)\\
&\Longrightarrow\forall t<m:F(t)\leq\frac{1}{2}\\
&\stackrel{t\uparrow m}{\Longrightarrow}F(m-)\leq\frac{1}{2}
\end{align*}
\underline{Fall 2: $t> m$} Siehe 2. Fall, analog.\nl
\underline{Zeige (a) $\gdw$ (c):}
(b) ist offensichtlich äquivalent zur Definition des Medians.
\end{proof}

\begin{bemerkungnr}\
\begin{enumerate}
\item Lemma \ref{lemmaMedian} (a) besagt, dass $\lbrace m\in\R: m\text{ erfüllt } 1.\rbrace$ die Menge aller Mediane von $F$ ist.
\item Im Allgemeinen gibt es mehrere Mediane. Üblicherweise \underline{wählt} man $m:=F^{-1}(\frac{1}{2})$, wobei
\begin{align*}
F^{-1}(u):=\inf\left\lbrace x\in\R:F(x)\geq u\right\rbrace\quad\forall u\in (0,1)
\end{align*}
die \textbf{Quantilfuntion / die verallgemeinerte Inverse} ist. Da
\begin{align*}
F\left(F^{-1}(u)-\right)\leq u\leq F\left(F^{-1}(u)\right)\qquad\forall u\in (0,1),
\end{align*}
erfüllt $m=F^{-1}\left(\frac{1}{2}\right)$ die Bedingung (a) in Lemma \ref{lemmaMedian} und ist somit ein Median, nämlich der kleinste.
\item Die obige Funktion \eqref{DefY}, also
\begin{align*}
Y:\R\to\R,\qquad Y(t)=\int\limits|x-t|~F(\d x)\qquad\forall t\in\R,
\end{align*}
ist stetig (nutze Folgenkriterium + dominierte Konvergenz bzw. Satz von Lebesgue), aber im Allgemeinen nicht differenzierbar, z. B. falls $F\sim X$ eine diskrete Zufallsvariable ist. In diesem Fall ist somit die Minimierung über Differentiation nicht möglich!
\end{enumerate}
\end{bemerkungnr}

Zur Schätzung von $m$ seien $X_1,\ldots, X_n$ i.i.d.$\sim F$ mit zugehöriger \textbf{empirischer Verteilungsfunktion}
\begin{align*}
F_n(x):=\frac{1}{n}\cdot\sum\limits_{i=1}^n\indi_{\lbrace X_i\leq x\rbrace}\qquad\forall x\in\R.
\end{align*}
Tatsächlich ist $F_n$ die Verteilungsfunktion zum \textbf{empirischen Maß}
\begin{align*}
Q_n:=\frac{1}{n}\cdot\sum\limits_{i=1}^n\delta_{X_i}\text{ wobei }\delta_x\text {das Dirac-Maß in}x\in\R
\end{align*}

Gemäß dem Satz von Gliwenko-Cantelli gilt:
\begin{align*}
\sup\limits_{x\in\R}|F_n(x)-F(x)|\stackrel{n\to\infty}{\longrightarrow}0\text{ konvergiert $\P$-fast sicher für alle Vereteilungsfunktionen }F
\end{align*}

\begin{erinnerung}
Für das Dirac-Maß $\delta_x:\A\to\R_+,\qquad\delta_x(A):=\indi_A(x)$ gilt:
\begin{align*}
\int\limits f(t)~\delta_x(\d t)=f(x)
\end{align*}
\end{erinnerung}

Ein vages Stetigkeitsargument motiviert folgenden Schätzer für $m$:
\begin{align*}
\hat{m}_n&:=\arg\min\limits_{t\in\R}Y_n(t):=\text{ (irgendeine) Minimalstelle der Funktion}\\
Y_n(t)&:=\int\limits_\Omega |x-t|F_n(\d x)\\ 
&=\int\limits_\Omega |x-t|Q_n(\d x)\\ 
&=\frac{1}{n}\cdot\sum\limits_{i=1}^n\int\limits|x-t|~\delta_{X_i}(\d x)\\
&=\frac{1}{n}\cdot\sum\limits_{i=1}^n|X_i-t|
\end{align*} 

$\hat{m}_n$ heißt \textbf{empirischer Median} von $X_1,\ldots,X_n$ mit üblicher Auswahl $\hat{m}_n=F_n^{-1}\left(\frac{1}{2}\right)$ gemäß Lemma \ref{lemmaMedian} (da empirische Verteilungsfunktion eine Verteilungsfunktion ist).

%Hier wäre Abbildung 1

\begin{bemerkung}\
\begin{itemize}
\item Wenn man eine ungerade Anzahl von Daten hat, ist der Median der mittlere Wert, nachdem man die Daten der Größe nach geordnet hat.
\item Hat man hingegen eine gerade Anzahl an Daten, dann ist der Median der kleinere der beiden mittleren Werte.
\end{itemize}
\end{bemerkung}

Mit dem starken Gesetz der großen Zahlen (SGGZ) gilt
\begin{align}\label{eq1.1}
\forall t\in\R: \Big(Y_n(t)\stackrel{n\to\infty}{\longrightarrow}
\E[|X_1-t|]=Y(t)\text{ fast sicher}\Big)
\end{align}
Problem: Folgt aus \eqref{eq1.1} bereits, dass
\begin{align*}
\arg\min\limits_{t\in\R} Y_n(t)
\stackrel{n\to\infty}{\longrightarrow}
\arg\min\limits_{t\in\R}
Y(t)\text{ fast sicher?}
\end{align*}
Dann folgte:
\begin{align*}
\hat{m}_n
\stackrel{n\to\infty}{\longrightarrow}
m\text{ fast sicher (\textbf{starke Konvergenz})}
\end{align*}

Wir formalisieren und verallgemeinern:
\begin{align*}
&X_i:(\Omega, \A,P)\to(\R,\B(\R))\text{ messbar},\qquad\omega\mapsto X_i(\omega)\\
&\Longrightarrow
Y_n(t):=Y_n(t,\omega)=
\frac{1}{n}\cdot\sum\limits_{i=1}^n\left|X_i(\omega)-t\right|\\
&\Longrightarrow
Y_n(t,\cdot):(\Omega,\A)\to(\R,\B(\R))\text{ messbar }\forall t\in\R
\end{align*}

\begin{defi}
Die \textbf{Kollektion}
\begin{align*}
Y_n:=\lbrace Y_n(t,\cdot):t\in\R\rbrace
=\lbrace Y_n(t):t\in\R\rbrace
\end{align*}
heißt \textbf{stochastischer Prozess (SP)}. Die Abbildung
\begin{align*}
X_n(\cdot,\omega):\R\to\R,\qquad t\mapsto Y_n(t,\omega)
\end{align*}
heißt \textbf{Trajektorie / Pfad} des SP $Y_n$ zu festem $\omega\in\Omega$.
\end{defi}

In unserem Beispiel sind für \underline{alle} $\omega\in\Omega$ die Pfade stetig auf $\R$. Die Abbildung
\begin{align*}
Y_n:\Omega\to X C(\R,\R),\qquad\omega\mapsto Y_n(\cdot,\omega)
\end{align*}
heißt \textbf{Pfadabbildung} des SP $Y_n$. Wir identifizieren also den SP $Y_n$ mit seiner Pfadabbildung. Damit ist $Y_n$ eine Abbildung von $\Omega$ in den Funktionenraum 
\begin{align*}
C(\R):=C(\R,\R):=\lbrace f:\R\to\R: f\text{ ist stetig }\rbrace.
\end{align*}

Sei $d:C(\R)\times C(\R\to\R$ die Metrik der gleichmäßigen Konvergenz der Kompakta auf $\C(\R)$ (formale Definition kommt später) und sei
\begin{align*}
\B(C(\R)):=\B_d\big(C(\R)\big)=\sigma\big(\lbrace G\subseteq C(\R):G\text{ ist offen bzgl. }d\rbrace\big)
\end{align*}
die von $d$ induzierte \textbf{Borel-$\sigma$-Algebra}.\\
Wir werden sehen, dass die Abbildung
\begin{align*}
Y_n:(\Omega,\A,\P)\to\Big(C(\R),\B\big(C(\R)\big)\Big)
\end{align*}
messbar ist. $Y_n$ ist also eine Zufallsvariable mit Werten im metrischen Raum $\big(C(\R),d\big)$.

Formulierung des Problems im allgemeinen Rahmen:
Seien $Y_n,~n\in\N$ mit $Y$ SP mit stetigen Pfaden (stetige SP).
Was lässt sich sagen über die Gültigkeit der folgenden Implikationen?
\begin{align}
Y
\stackrel{n\to\infty}{\longrightarrow}
Y\text{ fast sicher }
\Longrightarrow
\arg\min\limits_{t\in\R} Y(t)
\stackrel{n\to\infty}{\longrightarrow}
\arg\min\limits_{t\in\R} Y(t)\text{ fast sicher}
\end{align}
Ziel: Welche Art der Konvergenz $Y_n\stackrel{n\to\infty}{\longrightarrow} Y$ reicht für obige Implikation aus? Gleichmäßige Konvergenz, gleichmäßige Konvergenz auf Kompakta punktweise Konvergenz oder sogar nur \eqref{eq1.1}?\\
$Y$ besitzt womöglich (unter positiven Wahrscheinlichkeiten) keine eindeutige Minimalstelle. Und dann?\nl
Für die Konstruktion von (asymptotischen) Konfidenzintervallen für $m$ benötigt man \textbf{Verteilungskonvergenz}:
\begin{align}\label{eq1.3}
a_n(\hat{m}_n-m)
\stackrel{\mathcal{L}}{\longrightarrow}\xi\text{ in }\R
\end{align}
wobei $a_n\to\infty$ in $\xi$ Grenzvariable, die es zu identifizieren gilt. Für die Herleitung von \eqref{eq1.3} favorisiere wieder einen \textit{funktionalen Ansatz}. Sei
\begin{align*}
Z_n(t):=\beta_n\cdot\left( Y_n\left(m+\frac{t}{a_n}\right)-Y_n(m)\right)\qquad t\in\R
\end{align*}
der sogenannte \textbf{reskalierte Prozess zu $Y_n$}, wobei $\beta_n$ deine geeignete positive Folge ist. Damit folgt
\begin{align}\label{1.4}
a_n(\hat{m}_n-m)=\arg\min\limits_{t\in\R} Z_n(t)
\end{align}
Klar: $Z_n$ ist wieder ein stetiger stochastischer Prozess und damit $(Z_n)_{n\in\N}$ eine Folge von Zufallsvariablen in $\big(C(\R),d\big)$. Wünschenswert auch hier wäre die Gültigkeit folgender Implikation:
\begin{align}\label{eqSternchen}\tag{$\ast$}
Z_n
\stackrel{\mathcal{L}}{\longrightarrow}
Z\text{ in } \big(C(\R),d\big)
\Longrightarrow
\arg\min\limits_{t\in\R} Z_n(t)
\stackrel{\mathcal{L}}{\longrightarrow}
\arg\min\limits_{t\in\R} Z(t)
\end{align}
Dazu erforderlich ist das Konzept der \textbf{Verteilungskonvergenz} von Zufallsvariablen in metrischen Räumen, damit \eqref{eqSternchen} eine wohldefinierte Bedeutung erhält. Dies folgt später. Natürlich auch hier wieder das Problem: $Z$ besitzt mit positiver Wahrscheinlichkeit mindestens 2. Minimalstellen. Und dann?\nl
Im Falle einer fast sicher eindeutigen Minimalstelle von $Z$ würde aber aus (1.4) und (1.5) folgen:
\begin{align*}
a_n(\hat{m}_n-m)
\stackrel{\mathcal{L}}{\longrightarrow}
\arg\min\limits_{t\in\R} Z(t)
\end{align*}


\section{Konzepte aus metrischen Räumen}
Sei $(\mathcal{S},d)$ metrischer Raum.

\begin{beispiel}[Supremums-Metrik] %2.1
\begin{align*}
\mathcal{S}=C([0,1]):=\big\lbrace f:[0,1]\to\R: f\text{ stetig}\big\rbrace\\
d(f,g):=\sup\limits_{t\in[0,1]}\big|f(t)-g(t)\big|,\qquad\forall f,g\in C([0,1])
\end{align*}
\end{beispiel}

\begin{definition}\
\begin{enumerate}[label={(\arabic*)}]
\item Für $x\in\mathcal{S},~r>0$ ist
\begin{align*}
B(x,r):=B_d(x,r):=\lbrace y\in\mathcal{S}:d(x,y)<r\rbrace
\end{align*}
die offene Kugel um Mittelpunkt $x$ und Radius $r$.
\item Sei $A\subseteq\mathcal{S}$. Dann:
\begin{align*}
\stackrel{\circ}{A}&:=\inner(A):=\text{ das Innere von }A\\
\overline{A}&:=\text{ Abschluss von }A\\
\partial A&:=\overline{A}\cap\overline{A^C}=\overline{A}\setminus\stackrel{\circ}{A}\text{ ist der Rand von }A\\
A^C&:=\mathcal{S}\setminus A
\end{align*}
\item \begin{align*}
\mathcal{G}:=\mathcal{G}(\mathcal{S})&:=\big\lbrace G\subseteq\mathcal{S}: G\text{ ist offen bzgl. }d\big\rbrace\\
&=\big\lbrace G\subseteq\mathcal{S}:\forall x\in G:\exists r>0:B_d(x,r)\subseteq G\big\rbrace
\end{align*}
ist die durch $d$ induzierte Topologie.
\begin{align*}
\mathcal{F}:=\mathcal{F}(\mathcal{S}):=\big\lbrace F\subseteq\mathcal{S}:F\text{ ist abgeschlossen}\big\rbrace
\end{align*}
\item Sei $\emptyset\neq A\subseteq\mathcal{S},~x\in\mathcal{S}$. Dann ist
\begin{align*}
d(x,A):=\inf\lbrace d(x,a):a\in A\rbrace\geq0
\end{align*}
der Abstand von $x$ zu $A$.
\item $C(\mathcal{S}):=\lbrace f:S\to\R:f\text{ stetig}\rbrace$
\begin{align*}
C^b(\mathcal{S}):=\lbrace f\in C(\mathcal{S}):f\text{ beschränkt}\rbrace\\
\Vert f\Vert:=\Vert f\Vert_\infty:=\sup\limits_{x\in\mathcal{S}}|f(x)|
\end{align*}
\end{enumerate}
\end{definition}

\begin{lemma}\label{lemma2.3}\ %2.3
\begin{enumerate}[label={(\arabic*)}]
\item $\begin{aligned}
x\in\overline{A}\Longleftrightarrow d(x,A)=0
\end{aligned}$
\item $\begin{aligned}
\big| d(x,A)-d(y,A)\big|\leq d(x,y)\qquad\forall x,y\in\mathcal{S}
\end{aligned}$
\item $\begin{aligned}
d(\cdot, A):\mathcal{S}\to\R,\qquad x\mapsto d(x,A)
\end{aligned}$ ist gleichmäßig stetig ($A\neq\emptyset$).
\end{enumerate}
\end{lemma}

\begin{proof}
\underline{Zeige (1) ``$\Rightarrow$'':} Sei $x\in\overline{A}$. Dann gilt:
\begin{align*}
&\forall\varepsilon>0:\exists a\in A: d(x,a)<\varepsilon\\
&\implies d(x,A)\leq d(x,a)<\varepsilon~\forall\varepsilon>0\\
&\stackrel{\varepsilon\to0}{\implies}
d(x,A)=0
\end{align*}
\underline{Zeige (1) ``$\Leftarrow$'':}
Sei $d(x,A)=0$. Dann folgt aus der Infimumseigenschaft:
\begin{align*}
&\forall\varepsilon>0:\exists a\in A;0\leq d(x,a)\leq0+\varepsilon=\varepsilon\\
&\implies x\in\overline{A}
\end{align*}
\underline{Zeige (2):} Seien $x,y\in\mathcal{S}$. Dann gilt:
\begin{align*}
&d(x,a)
\stackrel{\Delta\text{Ungl}}{\leq}
d(x,y)+d(y,a)\qquad\forall a\in A\\
&\implies
d(x,A)\leq d(x,y)+d(y,A)\implies d(x,A)-d(y,A)\leq d(x,y)
\end{align*}
Vertauschen von $x$ und $y$ liefert:
\begin{align*}
d(y,A)-d(x,A)\leq d(y,x)=d(x,y)\implies\text{ Behauptung}
\end{align*}
\underline{Zeige (3):} Folgt aus (2) da, die Funktion $d(\cdot,A)$ Lipschitz-stetig und damit gleichmäßig stetig ist.
\end{proof}

\begin{satz}\label{Satz2.4} %2.4
Zu $A\subseteq\mathcal{S}$ und $\varepsilon>0$ existiert ein gleichmäßig stetige Funktion 
\begin{align*}
f:\mathcal{S}\to[0,1]\text{ mit der Eigenschaft} f(x)=\left\lbrace\begin{array}{cl}
1, & \falls x\in A\\
0, & \falls d(x,A)\geq\varepsilon
\end{array}\right.
\end{align*}
\end{satz}
\begin{proof}
Setze
\begin{align*}
\varphi:\R\to[0,1],\qquad \varphi(t):=\left\lbrace\begin{array}{cl}
1 , & \falls t\leq0\\
1-t, & \falls 0<1<1\\
0, & \falls t\geq1
\end{array}\right.
\end{align*}
Dann ist $\varphi$ gleichmäßig stetig auf $\R$. Sei
\begin{align*}
f(x):=\varphi\left(\frac{1}{\varepsilon}\cdot d(x,A)\right)\qquad\forall x\in\S
\end{align*}
Dann hat dieses $f$ die gewünschte Eigenschaft wegen Lemma \ref{lemma2.3}.
\end{proof}

\begin{definition} %2.5
Ein metrischer Raum $(\S,d)$ heißt \textbf{separabel}
\begin{align*}
&:\Longleftrightarrow\exists\text{ abzählbares } S_0\subseteq\S:\S\subseteq\overline{S_0}\\
&\Longleftrightarrow\exists\text{ abzählbares } S_0\subseteq\S:\S=\overline{S_0}\\
&\Longleftrightarrow\exists\text{ abzählbares } S_0\subseteq\S:S_0\text{ liegt dicht in }\S
\end{align*}
\end{definition}

\begin{beispiel}\label{beisp2.6} %2.6
$C([0,1])$ mit Supremums-Metrik ist separabal.
\begin{proof}
\begin{align*}
S_0:=\big\lbrace P:P\text{ ist Polynom mit \underline{rationalen} Koeffizienten}\big\rbrace
\end{align*}
$S_0$ ist abzählbar. Aus dem \textit{Approximationssatz von Weierstraß} und der Dichtheit von $\Q$ folgt die Behauptung.
\end{proof}
\end{beispiel}

\begin{definition} %2.7
$\G_0\subseteq\G$ heißt \textbf{Basis} von $\G:\Longleftrightarrow\forall G\in\G:G$ ist Vereinigung von Mengen aus $\G_0$, so genannte \textbf{$\G_0$-Mengen}.
\end{definition}

\begin{beispiel} %2.8
Die Menge
\begin{align*}
\big\lbrace B(x,r):x\in\S,0<r\in\Q\big\rbrace
\end{align*}
ist Basis von $\G$, denn:
\begin{proof}
Sei $G\in\G$. Dann gilt:
\begin{align*}
&\forall x\in G:\exists 0<r_x\in\Q:B(x,r_x)\subseteq G\\
&\implies
G=\bigcup\limits_{x\in G}\underbrace{\lbrace x\rbrace}_{\subseteq B(x,r_x)}\subseteq\bigcup\limits_{x\in G} \underbrace{B(x,r_x)}_{\subseteq G}\subseteq G\implies G=\bigcup\limits_{x\in G} \underbrace{B(x,r_x)}_{\in\G_0}
\end{align*}
\end{proof}
\end{beispiel}

\begin{satz}\label{satz2.9}
$\S$ separabel $\Longleftrightarrow\G$ hat abzählbare Basis
\end{satz}
\begin{proof}
\underline{Zeige ``$\Rightarrow$'':}\\
Sei $S_0\subseteq\S$ abzählbar und dicht in $\S$. Zeige:
\begin{align*}
\G_0:=\big\lbrace B(x,r):x\in S_0,0<r\in\Q\big\rbrace\subseteq\G\text{ ist Basis.}
\end{align*}
Sei also $G$ offen. Dann folgt aus Beispiel 2.8:
\begin{align}\label{proof2.9Sternchen}\tag{$\ast$}
G=\bigcup\limits_{x\in G} B(x,r_x),\qquad 0<r_x\in\Q,\forall x\in G
\end{align}
Da $\overline{S_0}=\S$ gilt:
\begin{align*}
&\forall x\in G:\exists y_x\in S_0: d(x,y_x)<\frac{r_x}{2}\\
&\implies d(x,y)
\stackrel{\Delta\text{Ungl}}{\leq}
d(x,y_x)+d(y_x,x)< \underbrace{\frac{r_x}{2}+\frac{r_x}{2}}_{=r_x}\qquad\forall y\in B\left(y_x,\frac{r_x}{2}\right)\\
&\implies B\left(y_x,\frac{r_x}{2}\right)\subseteq B(x,r_x)\qquad\forall x\in G\\
&\implies G\stackrel{\eqref{proof2.9Sternchen}}{\supseteq}
\bigcup\limits_{x\in G}\underbrace{B\left(y_x,\frac{r_x}{2}\right)}_{\supseteq\lbrace x\rbrace}
\supseteq\bigcup\limits_{x\in G}\lbrace x\rbrace=G\\
&\implies G=\bigcup	\limits_{x\in G}\underbrace{B\left(y_x,\frac{r_x}{2}\right)}_{\in\G_0}
\end{align*}
Also ist $\G_0$ einen Basis. Da $S_0$ abzählbar ist $\G_0$ abzählbar.\nl
\underline{Zeige ``$\Leftarrow$'':}\\
Sei $\G_0$ abzählbare Basis von $\G$ und sei o.B.d.A. $\emptyset\notin\G_0$. Wähle für jedes $G\in\G_0$ ein $x_G\in G$ fest aus. Setze
\begin{align*}
S_0:=\lbrace x_G:G\in\G_0\rbrace.
\end{align*}
$S_0$ ist auch abzählbar. Bleibt Dichtheit zu zeigen.\\
Sei $x\in\S$ und $\varepsilon>0$. Da $B(x,\varepsilon)$ offen und $\G_0$ Basis, gilt: 
\begin{align*}
&\exists\G_{x,\varepsilon}\subseteq\G_0\mit B(x,\varepsilon)=\bigcup\limits_{G\in\G_{x,\varepsilon}} G\\
&\implies G\subseteq B(x,\varepsilon)\qquad\forall G\in\G_{x\varepsilon}
\end{align*}
Wähle ein $G$ von diesen aus. Dann gilt:
\begin{align*}
x_G\in G\subseteq B(x,\varepsilon)
\implies x_G\in B(x,\varepsilon)
\implies d(\underbrace{x_G}_{\in S_0},x)<\varepsilon
\end{align*}
\end{proof}

\begin{satz}\label{Satz2.10} %2.10
Seien $(\S,d)$ und $(\S',d')$ metrische Räume.
\begin{enumerate}[label={(\arabic*)}]
\item Auf $\S\times\S'$ sind Metriken definiert durch
\begin{align*}
d_1\Big((x,x'),(y,y')\Big)&:=\left( \big(d(x,y)\big)^2+\big(d'(x',y')\big)^2\right)^{\frac{1}{2}} &\forall(x,x'),(y,y')\in \S\times\S'\\
d_2\Big((x,x'),(y,y')\Big)&:=\max \left\lbrace d(x,y),d'(x',y')\right\rbrace &\forall(x,x'),(y,y')\in \S\times\S'\\
d_3\Big((x,x'),(y,y')\Big)&:=d(x,y)+d'(x',y') &\forall(x,x'),(y,y')\in \S\times\S'
\end{align*}
\item Die Metriken $d_1,d_2,d_3$ induzieren dieselbe Topologie $\mathcal{G}(\S\times \S')$ auf $\S\times\S'$, die sogenannte \textbf{Produkttopologie} von $\mathcal{G}(\S)$ und $\mathcal{G}(\S')$.
\item $\begin{aligned}
\mathcal{G}(\S\times\S')=\left\lbrace\bigcup\limits_{\begin{subarray}{c}G\in\mathcal{O}\\ G'\in\mathcal{O}'\end{subarray}}G\times G':\mathcal{O}\subseteq\mathcal{G}(\S),\mathcal{O}'\subseteq\mathcal{G}(\S')\right\rbrace
\end{aligned}$\\
d.h.
\begin{align*}
\big\lbrace G\times G':G\in\mathcal{G}(\S),G'\in\mathcal{G}(\S')\big\rbrace
\end{align*}
bildet eine Basis von $\mathcal{G}(S\times\S')$.
\end{enumerate}
\end{satz}
\begin{proof}\enter
\underline{Zu (1):} Überprüfung der Eigenschaften einer Metrik (zur Übung).\\
\underline{Zu (2):} Punktweise gelten die Beziehungen:
\begin{align*}
d_2\leq d_1\leq\sqrt{2}\cdot d_2,\qquad
\frac{1}{\sqrt{2}}\cdot d_3\leq d_1\leq d_3,\qquad
d_2\leq d_3\leq 2\cdot d_2
\end{align*}
Beachte beim Nachweis, dass die $d_i$'s als Metriken größer Null sind. Aus obigen Beziehungen folgt u. a.:
\begin{align*}
B_{d_2}\left(x,\frac{r}{\sqrt{2}}\right)\subseteq B_{d_1}(x,r)
\end{align*}
denn:
\begin{align*}
r>\sqrt{2}\cdot d_2(y,x)\geq d_1(y,x)
\end{align*}
\underline{Zu (3), zeige ``$\subseteq$'':}\\
Sei $G^\ast\in\mathcal{G}(\S\times\S')$. Dann gilt:
\begin{align*}
\forall x^\ast=(x,y)\in G^\ast:\exists r=r_{x^\ast}>0:
G^\ast=\bigcup\limits_{x^\ast\in G^\ast} B\big(x^\ast,r_{x^\ast}\big)
\end{align*}
Wegen Teil (2) sei o.B.d.A. $\S^\ast:=\S\times\S'$ versehen mit der Metrik $d_2$. Dann gilt:
\begin{align*}
B_{d_2}\big(x^\ast,r_{x^\ast}\big)&=\Big\lbrace(y,y')\in \S\times\S':\max\big\lbrace d(x,y),d'(x',y')\big\rbrace<r_{x^\ast}\Big\rbrace\\
&=\Big\lbrace(y,y')\in\S\times\S':d(x,y)<r_{x^\ast}\wedge d'(x',y')<r_{x^\ast}\Big\rbrace\\
&= \underbrace{B_d\big(x,r_{x^\ast}\big)}_{\in\mathcal{G}(\S)}\times \underbrace{B_{d'}\big(x', r_{x^\ast}\big)}_{\in\mathcal{G}(\S')}
\end{align*}
\underline{Zu (3), zeige ``$\supseteq$'':}\\
Sei zunächst $G\times G'\mit G,G'$ offen und $x^\ast=(x,x')\in G\times G'$. Also ist $x\in G$ und $x'\in G'$ und somit
\begin{align*}
\exists r,r'>0:B_d(x,r)\subseteq G\wedge B_{d'}(x',r')\subseteq G'
\end{align*}
Setze $r^\ast:=\min\lbrace r,r'\rbrace>0$. Damit folgt

\begin{align*}
B_{d_2}\big( x^\ast,r^\ast\big)&\subseteq B_d(x,r)\times B_{d'}\big(x',r'\big)\\
&\subseteq
G\times G'=G^\ast\\
&\implies
G\times G'\in\mathcal{G}(\S\times\S')\\
&\implies
\bigcup\limits_{\begin{subarray}{c} G\in\mathcal{O}\\G'\in\mathcal{O}'\end{subarray}}G\times G'\subseteq\mathcal{G}(\S\times\S')
\qquad\forall\mathcal{O}\subseteq\mathcal{G}(\S),\mathcal{O}'\subseteq\mathcal{G}(\S')
\end{align*}
da die Produkttopologie vereinigungsstabil ist.
\end{proof}

\begin{defi}
Die Metriken $d_1,d_2,d_3$ heißen \textbf{Produktmetriken}. Daher alternative\\ Schreibweise $d\times d'$, also z. B. $d\times d':=\max\lbrace d,d'\rbrace$ usw.
\end{defi}

\begin{bemerkungnr} %2.11
Analog lassen sich Produktmetriken für \underline{endlich viele} metrische Räume $(S_i,d_i)_{i\in\lbrace1,\ldots,k\rbrace}$ definieren, z. B.
\begin{align*}
d_1\times\ldots\times d_k:=\left(\sum\limits_{i=1}^k d_i^2\right)^{\frac{1}{2}},
\end{align*}
die wiederum dieselbe Produkttopologie induzieren.
\end{bemerkungnr}

\section{Zufallsvariablen in metrischen Räumen}
\begin{definition} %3.1
Die \textbf{Borel-$\sigma$-Algebra} auf dem metrischen Raum $(\S,d)$ ist %definiert als
\begin{align*}
\B(\S):=\sigma\big(\mathcal{G}(\S)\big).
\end{align*}
Elemente $B\in\B(\S)$ heißen \textbf{Borel-Mengen} in $\S$.\\
Beachte: $\B(\S)=\B_d(\S)$ hängt i. A. von der Metrik $d$ ab.
\end{definition}

\begin{lemma}\label{Lemma3.2} %3.2
Es gilt:
\begin{enumerate}[label=(\arabic*)]
\item 
$\begin{aligned}
\B(\S)=\sigma\big(\mathcal{F}(\S)\big)
\end{aligned}$
\item $\begin{aligned}
f:(\S,d)\to(\S',d)
\end{aligned}$ ist stetig und damit $\B_d(\S)-\B_d(\S')$ ToDo
\item Sei $\mathcal{G}_0$ abzählbare Basis von $\mathcal{G}(\S)$. Dann gilt:
\begin{align*}
\sigma(\mathcal{G}_0)=\B(\S)
\end{align*}
\end{enumerate}
\end{lemma}

\begin{proof}\enter
\underline{Zu (1), zeige ``$\subseteq$'':}
\begin{align*}
G^C\in\mathcal{F}(\S)\subseteq\sigma\big(\mathcal{F}(\S)\big)
\implies
G=\big(G^C\big)^C\in\mathcal{F}(\S)
\end{align*}
da $\sigma\big(\mathcal{F}(\S)\big)$ Komplement-stabil ist. Also folgt
\begin{align*}
\mathcal{G}\subseteq\sigma(\mathcal{F})
\implies\sigma(\mathcal{G})\subseteq\sigma(\mathcal{F})
\end{align*}
\underline{Zu (1), zeige ``$\supseteq$'':} Analog.\nl
\underline{Zeige (2):}
\begin{align*}
f^{-1}\big(\B_{d'}(\S')\big)&=f^{-1}\Big(\sigma\big(\mathcal{G}(\S')\big)\Big)\\
&=\sigma\Big(\underbrace{f^{-1}\big(\mathcal{G}(\S')\big)}_{\stackrel{f\text{ stetig}}{\subseteq}\mathcal{G}(S)}\Big)\\
&\subseteq\sigma\big(\mathcal{G}(\S)\big)\\
&=\B(\S)
\end{align*}
\underline{Zu (3), zeige ``$\subseteq$'':}\\
Klar wegen $\mathcal{G}_0\subseteq\mathcal{G}$ und $\sigma$ monoton.\\
\underline{Zu (3), zeige ``$\supseteq$'':} Sei $G\in\mathcal{G}$. Dann:
\begin{align*}
G&=\bigcup\limits_{i\in\N} G_i\mit\text{geeigneten }G_i\in\mathcal{G}_0\subseteq\sigma(\mathcal{G}_0)\\
&\implies
G\in\sigma(\mathcal{G}_0)
\end{align*}
Aus der Stabilität unter Vereinigungen folgt die Behauptung.
\end{proof}

\begin{satz}\label{satz3.3} %3.3
Sei $(\S,d)$ separabler metrischer Raum. Dann gilt:
\begin{align*}
\B_{d\times d}(\S\times\S)=\B(S)\otimes\B(\S)
\end{align*}
\end{satz}
\begin{proof}
Seien
\begin{align*}
&\pi_1:\S\times\S\to\S,\qquad \pi_1(x,y):=x\qquad\forall(x,y)\in\S\times S\\
&\pi_2:\S\times\S\to\S,\qquad \pi_2(x,y):=y\qquad\forall(x,y)\in\S\times S
\end{align*}
die \textbf{Projektionsabbildungen}. Dann gilt
\begin{align*}
\B(\S)\otimes\B(\S) 
\overset{\text{Def}}&=
\sigma(\pi_1,\pi_2)\\
\overset{\text{Def}}&=
\sigma\Big(\pi_1^{-1}\big(\sigma(\mathcal{G})\big)\cup\pi_2^{-1}\big(\sigma(\mathcal{G})\big)\Big)\\
\overset{(+)}&=
\sigma\Big(\sigma\big(\pi_1^{-1}(\mathcal{G})\big)\cup\sigma\big(\pi_2^{-1}(\mathcal{G})\big)\Big)\\
&=
\sigma\Big(\pi_1^{-1}(\mathcal{G})\big)\cup\pi_2^{-1}(\mathcal{G})\Big)\\
&=\sigma\Big(\big\lbrace G\times S,S\times G':G,G'\in\mathcal{G}\big\rbrace\Big)\\
&=
\sigma\Big(\big\lbrace \overbrace{G\times G'}^{=(G\times S)\cap(S\times G')}:G,G'1\in\mathcal{G}\big\rbrace\Big)\\
\overset{\text{($\ast$)}}&=
\sigma\left(\left\lbrace\bigcup\limits_{\begin{subarray}{c}G\in\mathcal{O}\\G'\in\mathcal{O}'\end{subarray}}G\times G':\mathcal{O},\mathcal{O}'\subseteq\mathcal{G}\right\rbrace\right)\\
\overset{2.10~(3)}&=
\sigma\Big(\mathcal{G}(\S\times\S)\Big)\\
\overset{\text{Def}}&=
\B(\S\times\S)
\end{align*}
Zum Nachweis von (+):\\
Zeige ``$\supseteq$'': Setze
\begin{align*}
\xi&:=
\underbrace{\sigma\big(\pi_1^{-1}(\mathcal{G})\big)}_{\supseteq \pi_1^{-1}(\mathcal{G})}\cup\underbrace{\sigma\big(\pi_2^{-1}(\mathcal{G})\big)}_{\pi_2^{-1}(\mathcal{G})}\\
&\supseteq
\pi_1^{-1}(\mathcal{G})\cup\pi_2^{-1}(\mathcal{G})\\
&=:\mathcal{H}\\
&\implies\sigma(\xi)\supseteq\sigma(\mathcal{H})
\end{align*}
Zeige ``$\subseteq$'': Es gilt
\begin{align*}
&\pi_1^{-1}(\mathcal{G})\subseteq\big(\pi_1^{-1}(\mathcal{G})\cup\pi_2^{-1}(\mathcal{G})\big)=\mathcal{H}\\
&\implies
\sigma\big(\pi_1^{-1}(\mathcal{G})\big)\subseteq\sigma(\mathcal{H})\text{ und analog }\\
&\implies
\sigma\big(\pi_2^{-1}(\mathcal{G})\big)\subseteq\sigma(\mathcal{H})\\
&\implies
\xi=\underbrace{\sigma\big(\pi_1^{-1}(\mathcal{G})\big)}_{\subseteq\sigma(\mathcal{H})}\cup\underbrace{\sigma\big(\pi_2^{-1}(\mathcal{G})\big)}_{\subseteq\sigma(\mathcal{H})}\subseteq\sigma(\mathcal{H})\\
&\implies
\sigma(\xi)\subseteq\sigma(\mathcal{H})
\end{align*}

Bleibt Nachweis von ($\ast$):\\
``$\subseteq$'': ist klar (gilt auch ohne Separabilität)\\
``$\supseteq$'': Gemäß 2.9 existiert abzählbare Basis $\mathcal{G}_0$  von $\mathcal{G}$. Sei
\begin{align*}
G^\ast&=\bigcup\limits_{\begin{subarray}{c}G\in\mathcal{O}\\G'\in\mathcal{O}'\end{subarray}}G\times G'\text{ und }\mathcal{O},\mathcal{O}'\subseteq\mathcal{G}\\
&\stackeq{(!)}
\bigcup\limits_{\begin{subarray}{c}
G,G'\text{ offen}\\
G,G'\subseteq G^\ast
\end{subarray}}
G\times G'\\
&\stackeq{(!)}
\bigcup\limits_{\begin{subarray}{c}
G_0,G_0'\in\mathcal{G}_0\\
G\times G_0'\subseteq G^\ast
\end{subarray}}
G_0\times G_0'\\
&=\text{ abzählbare Vereinigung, da $\mathcal{G}_0$ abzählbare Basis }\\
&\implies
G^\ast\in\sigma\Big(\big\lbrace G\times G':G,G'\in\mathcal{G}\big\rbrace\Big)
\end{align*}
\end{proof}

\begin{definition} %3.4
Sei $(\Omega,\A)$ ein Messraum. Eine Abbildung
$X:\Omega\to\S$, die $\A$-$\B(\S)$-messbar ist, heißt \textbf{Zufallsvariable (ZV)} in den metrischen Raum $(\S,d)$ über $(\Omega,\A)$.\nl
Sei $\P$ ein Wahrscheinlichkeitsmaß auf $(\Omega,\A)$, also $(\Omega,\A,\P)$ ein Wahrscheinlichkeitsraum. Das Bildmaß
\begin{align*}
\P\circ X^{-1}&:=:\P_X:=:\mathcal{L}:=:\mathcal{L}(X~|~\P)\\
(\P\circ X^{-1})(B)&:=\P\left(X^{-1}(B)\right)=\P\Big(\big\lbrace\omega\in\Omega:X(\omega)\in B\big\rbrace\Big)
=: \P[X\in B]
\qquad\forall B\in\B(\S)
\end{align*}
heißt \textbf{Verteilung} von $X$ unter $\P$.
\end{definition}

\begin{satz}\label{Satz3.5} %3.5
Sei $(\S,d)$ separabler metrischer Raum und seien $X,Y$ Zufallsvariablen in $(\S,d)$ über $(\Omega,\A)$.\\
Dann ist $d(X,Y)$ eine reelle Zufallsvariable.
\end{satz}

\begin{proof}
\begin{align*}
X,Y:(\Omega,\A)\to(\S,\B(\S))\text{ sind messbar }\\
\stackrel{\text{MINT}}{\Longleftrightarrow}
(X,Y):(\Omega,\A)\to\big(\S\times\S,\underbrace{\B(\S)\otimes\B(\S)}_{\stackeq{\ref{satz3.3}}\B(\S\times\S)}\big)\text{ ist messbar}\\
\end{align*}
Jede Metrik ist bekanntlich stetig, also auch
\begin{align*}
d:\big(\S\times\S,\G(\S\times\S)\big)\to\R.
\end{align*}
Dann folgt aus Lemma \ref{Lemma3.2}, dass
\begin{align*}
d:\B(\S\times\S)\to\B(\R)
\end{align*}
messbar ist. Damit folgt die Behauptung, denn $d(X,Y)=d\circ(X,Y)$ ist messbar als Komposition von messbaren Abbildungen.
\end{proof}

\subsection*{Fast sichere Konvergenz} %NoNumber
\begin{definition} %3.6
Seien $X,X_n,n\in\N$ Zufallsvariablen in $(\S,d)$ über $(\Omega,\A,\P)$. Dann:
\begin{align*}
X_n\stackrel{n\to\infty}{\longrightarrow} X\quad\P\text{-fast sicher }:\Longleftrightarrow
\P\Big(\underbrace{\big\lbrace\omega\in\Omega:d\big(X_n(\omega),X(\omega)\big)\stackrel{n\to\infty}{\longrightarrow} 0\big\rbrace}_{=:M}\Big)=1
\end{align*}
Beachte: Die Definition von Konvergenz mengentheoretisch aufgeschrieben (Schnitt $\hat{=}$ ``für alle''; Vereinigung $\hat{=}$ ``Es gibt''):
\begin{align*}
\bigcap\limits_{0<\varepsilon\in\Q}
\bigcup\limits_{m\in\N}
\bigcap\limits_{n\geq m}
\big\lbrace\underbrace{d(X_n,X)}_{=:\xi_n}<\varepsilon\big\rbrace\stackrel{\ref{Satz3.5}}{\in}\A\\
\text{denn }\xi_n^{-1}\big((-\infty,\varepsilon)\big)\in\A
\end{align*}
\end{definition}

Die bekannten Regeln (Ergebnisse) für \underline{reelle} Zufallsvariablen lassen sich mühelos verallgemeinern. Dazu z. B.:

\begin{satz}\label{Satz3.7} %3.7
\begin{align*}
X_n
\stackrel{n\to\infty}{\longrightarrow}
X\quad\P\text{-fast sicher }\wedge
X_n
\stackrel{n\to\infty}{\longrightarrow}
X'\quad\P\text{-fast sicher }
\implies
X=X'\quad\P\text{-fast sicher}
\end{align*}
\end{satz}
\begin{proof}
\begin{align*}
\lbrace X\neq X'\rbrace
&\subseteq\lbrace X_n
\stackrel{n\to\infty}{\not\longrightarrow}
X\rbrace
\cup\lbrace X_n
\stackrel{n\to\infty}{\not\longrightarrow}
X'\rbrace\\
&\implies
\P[X_n\not\to X]+\P[X_n\not\to X']=0+0\\
&\implies
\P[X\neq X']=0
\end{align*}
\end{proof}

\begin{satz}\label{Satz3.8} %3.8
Seien $X,X_n,n\in\N$ Zufallsvariablen im metrischen Raum $(\S,d)$ und sei $f:(\S,d)\to(\S',d')$ messbar und stetig in $X$ $\P$-fast sicher.
Dann gilt:
\begin{align*}
X_n
\stackrel{n\to\infty}{\longrightarrow}
X\quad\P\text{-fast sicher }
\implies
f(X_n)
\stackrel{n\to\infty}{\longrightarrow}
f(X)\quad\P\text{-fast sicher}
\end{align*}
\end{satz}
\begin{proof}
\begin{align*}
\lbrace X_n
\stackrel{n\to\infty}{\longrightarrow}
X\rbrace\cap\lbrace f\text{ stetig in }X\rbrace\rbrace
\stackrel{\text{Folgen-Stetigkeit}}{\subseteq}
\lbrace f(X_n)
\stackrel{n\to\infty}{\longrightarrow}
f(X)\rbrace
\end{align*}
Damit folgt die Behauptung, denn zur Erinnerung:
\begin{align*}
\big(\forall i\in\N:\P(E_i)=1\big)\implies\P\left(\bigcap\limits_{i\in\N} E_i\right)=1
\end{align*}
\end{proof}

\begin{satz}[Konvergenz-Kriterium]\label{Satz3.9} %3.9
\begin{align*}
X_n
\stackrel{n\to\infty}{\longrightarrow}
X\quad\P\text{-fast sicher}
\Longleftrightarrow
\forall\varepsilon>0:\limn\P\left(\sup\limits_{m\geq n} d(X_m,X)>\varepsilon\right)=0
\end{align*}
\end{satz}
\begin{proof}
Man ersetze im Beweis für den Fall reeller Zufallsvariablen $|X_n-X|$ durch $d(X_n,X)$. Und beachte, dass alle Schlussfolgerungen bestehen bleiben.
\end{proof}

Ein sehr nützliches Kriterium ist Folgendes:
\begin{satz}\label{Satz3.10} %3.10
\begin{align*}
\sum\limits_{n\in\N_{>0}}\P\big(d(X_n,X)>\varepsilon\big)<\infty\qquad\forall\varepsilon>0
\implies
X_n
\stackrel{n\to\infty}{\longrightarrow}
X\quad\P\text{-fast sicher}
\end{align*}
\end{satz}
\begin{proof}
Setze
\begin{align*}
A_n(\varepsilon):=\big\lbrace d(X_n,X)>\varepsilon\big\rbrace\stackrel{\ref{Satz3.5}}{\in}\A
\end{align*}
Dann folgt aus dem \textit{ersten Borel-Cantelli-Lemma}:
\begin{align*}
&\P\left(\limsup\limits_{n\to\infty} A_n(\varepsilon)\right)=0\qquad\forall\varepsilon>0
\end{align*}
Mit
\begin{align*}
\liminf\limits_{n\to\infty}\big(A_n(\varepsilon)\big)
\stackeq{\text{Def}}
\bigcup\limits_{m\in\N}\bigcap\limits_{n\geq m}\big(A_n(\varepsilon)\big)^C
=
\bigcup\limits_{m\in\N}\bigcap\limits_{n\geq m}\big\lbrace d(X_n,X)\leq\varepsilon\big\rbrace
\end{align*}
folgt dann
\begin{align*}
1=\P\left(\left(\limsup\limits_{n\to\infty} A_n(\varepsilon)\right)^C\right)
=\P\left(\liminf\limits_{n\to\infty}\big(A_n(\varepsilon)\big)\right)\qquad\forall\varepsilon>0
\end{align*}
Da Abzählbare Durchschnitte von Eins-Mengen (also Mengen mit $\P$-Maß 1) wieder Eins-Mengen sind, folgt schließlich:
\begin{align*}
\P\Bigg(\underbrace{\bigcap\limits_{0<\varepsilon\in\Q}\bigcup\limits_{n\geq m}\big\lbrace d(X_n,X)\leq\varepsilon\big\rbrace}_{\lbrace X_n\to X\rbrace=\lbrace d(X_n,X)\to0\rbrace}\Bigg)=1
\end{align*}
\end{proof}
Weitere Eigenschaften der fast sicheren Konvergenz von Zufallsvariablen in metrischen Räumen finden sich z. B. in \textit{``Wahrscheinlichkeitstheorie'' Gäussler u. Stute (1977), Kapitel 8.2}

\subsection*{Stochastische Konvergenz} %NoNumber
\begin{definition} %3.11
\begin{align*}
X_n
\stackrelnew{n\to\infty}{\P}{\longrightarrow}
X:\Longleftrightarrow\forall\varepsilon>0:
\P\Big(\big\lbrace d(X_n,X)>\varepsilon\big\rbrace\Big)
\stackrel{n\to\infty}{\longrightarrow}
0
\end{align*}
\end{definition}

\begin{satz}\label{Satz3.12}
\begin{align*}
X_n
\stackrel{n\to\infty}{\longrightarrow}
X\quad\P\text{-fast sicher }
\implies X_n
\stackrelnew{n\to\infty}{\P}{\longrightarrow}
X
\end{align*}
\end{satz}
\begin{proof}
\begin{align*}
\forall\varepsilon>0:
0\leq\P\big(d(X_n,X)>\varepsilon\big)
\leq\P\left(\sup\limits_{m\geq n}d(X_m,X)>\varepsilon\right)
\stackrelnew{n\to\infty}{\P}{\longrightarrow}
0
\end{align*}
gemäß Satz \ref{Satz3.9}.
\end{proof}

Die Umkehrung in 3.12 gilt i. A. \underline{nicht}, aber es gilt das folgende Teilfolgenkriterium:

\begin{satz}[Teilfolgenkriterium für stochastische Konvergenz]\label{satz3.13}\enter
Folgende Aussagen sind äquivalent:
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
X_n
\stackrelnew{n\to\infty}{\P}{\longrightarrow}
X\end{aligned}$
\item Zu jeder Teilfolge (TF) $(X_{n'})$ von $(X_n)_{n\in\N}$ existiert eine Teilfolge $(X_{n''})$ von $(X_{n'})$ derart, dass $X_{n''}
\stackrel{n''\to\infty}{\longrightarrow} X$ $\P$-fast sicher.
\end{enumerate}
\end{satz}
\begin{proof}
Wie im Reellen.
\end{proof}

Mit dem Teilfolgenkriterium lassen sich Rechenregeln für fast sichere Konvergenz auf stochastische Konvergenz übertragen.

\begin{korollar}\label{Korollar3.14}\
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
X_n
\stackrelnew{n\to\infty}{\P}{\longrightarrow}
X\wedge X_n
\stackrelnew{n\to\infty}{\P}{\longrightarrow}
X'
\implies X=X'\quad\P\text{-fast sicher}
\end{aligned}$
\item $\begin{aligned}
X_n
\stackrelnew{n\to\infty}{\P}{\longrightarrow}
X\text{ in }(\S,d),~f:(\S,d)\to(\S',d')\text{ messbar mit $f$ stetig in $X$ $\P$-fast sicher }
\end{aligned}$
\begin{align*}
\implies f(X_n)
\stackrelnew{n\to\infty}{\P}{\longrightarrow}
f(X)
\end{align*}
\end{enumerate}
\end{korollar}
\begin{proof}
\underline{Zeige (1):}
\begin{align*}
X_n
\stackrelnew{n\to\infty}{\P}{\longrightarrow}
X
\stackrel{\ref{satz3.13}}{\implies}
\exists\text{ TF }(X_{n'})\subseteq(X_n)_{n\in\N}\mit X_{n'}
\stackrel{n'\to\infty}{\longrightarrow}
X\text{ fast sicher}
\end{align*}
Zu $(X_{n'})$ existiert (wegen $X_n\stackrelnew{n\to\infty}{\P}{\longrightarrow} X'$ und Satz \ref{satz3.3}) eine Teilfolge $(X_{n''})\subseteq(X_{n'})\mit$
\begin{align*}
X_{n''}
\stackrel{n\to\infty}{\longrightarrow} X'\text{ fast sicher}
\stackrel{\ref{Satz3.7}}{\implies}
X=X'\text{ fast sicher }
\end{align*}

\underline{Zeige (2):} Zur Übung.
\end{proof}

\subsection*{Konvergenz in Produkträumen} %noNumber
Seien $(\S,d)$ und $(\S',d')$ separable metrische Räume. Dann ist auch $(\S\times\S',d\times d')$ ein metrischer Raum. Dies folgt z. B. aus dem \textit{Satz von der koordinatenweise Konvergenz}:

\begin{align}\label{eq3.1KoordinatenweissKonvergenz}\tag{3.1}
\big(a_n,a_n'\big)
\stackrelnew{d\times d'}{n\to\infty}{\longrightarrow}
(a,a')
\Longleftrightarrow
(a_n)
\stackrelnew{d}{n\to\infty}{\longrightarrow}
a
\wedge
(a_n')
\stackrelnew{d'}{n\to\infty}{\longrightarrow}
(a')
\end{align}

Es ``stochastische Versionen'' dieses Satzes.

\begin{satz}\label{satz3.15}\
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
(X_n,X_n')
\stackrelnew{}{n\to\infty}{\longrightarrow}
(X,X')~\P\text{-f.s.}
\Longleftrightarrow
X_n
\stackrelnew{}{n\to\infty}{\longrightarrow}
X~\P\text{-f.s. }\wedge
X_n'
\stackrelnew{}{n\to\infty}{\longrightarrow}
X'~\P\text{-f.s.}
\end{aligned}$
\item $\begin{aligned}
(X_n,X_n')
\stackrelnew{n\to\infty}{\P}{\longrightarrow}
(X,X')
\Longleftrightarrow
X_n
\stackrelnew{n\to\infty}{\P}{\longrightarrow}
X\wedge
X_n'
\stackrelnew{n\to\infty}{\P}{\longrightarrow}
X'
\end{aligned}$
\end{enumerate}
\end{satz}
\begin{proof}
\underline{Zu (1):}
\begin{align*}
(1)\text{, linke Seite }
&\stackrel{\eqref{eq3.1KoordinatenweissKonvergenz}}{\Longleftrightarrow}
X_n\to X,~X_n'\to X'\quad\P\text{-fast sicher}\\
&\stackrel{\cap\text{ Eins-Mengen}}{\Longleftrightarrow}
(1),\text{ rechte Seite}
\end{align*}

\underline{Zu (2):}
\begin{align*}
(2)\text{, linke Seite}
&\stackrel{\eqref{eq3.1KoordinatenweissKonvergenz}}{\Longleftrightarrow}
\forall\text{ TF }(X_{n'},X_{n'}')\subseteq(X_n,X_n'):\\
&\qquad\exists\text{ TTF }(X_{n''},X_{n''}')\subseteq(X_{n'},X_{n'}'):
(X_{n''},X_{n''}')\stackrel{n\to\infty}{\longrightarrow}\text{ f.s.}
\end{align*}
Also wegen Teil (1) mit
\begin{align*}
X_{n''}\to X\text{ f.s. und }X_{n''}'\to X'\text{ f.s.}
\end{align*}
Somit:
\begin{align*}
\forall\text{ TF }(X_{n'})\subseteq(X_n):\exists\text{ TTF }(X_{n''})\subseteq(X_{n'}):X_{n''}\to X\text{ f.s. }
&\stackrel{\eqref{eq3.1KoordinatenweissKonvergenz}}{\Longleftrightarrow}
X_n\stackrelnew{n\to\infty}{\P}{\longrightarrow} X
\end{align*}
Und es gilt analog: $X_n'\to X'$.
\end{proof}

\subsection*{Gleichheit in Verteilung} %NoNumber
\begin{definition}\label{def3.16} %3.16
Zufallsvariablen $X,Y$ in $(\S,d)$ über $(\Omega,\A,\P)$ heißen \textbf{gleich in Verteilung}, in Zeichen $X\stackeq{\mathcal{L}} Y$
\begin{align*}
:\Longleftrightarrow \P\circ X^{-1}\equiv\P\circ Y^{-1}
\end{align*}
\end{definition}

\begin{bemerkung} %noNomber
Def 3.16 kann erweitert werden auf Zufallsvariablen $X:(\Omega,\A,\P)\to(\S,d)$ und $Y:(\tilde{\Omega},\tilde{\A},\tilde{\P})\to(\S,d)$ durch
\begin{align*}
X\stackeq{\mathcal{L}} Y:\Longleftrightarrow\P\circ X^{-1}\equiv\tilde{\P}\circ Y^{-1}
\end{align*}
\end{bemerkung}

Charakterisierung von Verteilungsgleichheit in folgendem Satz:

\begin{satz}\label{satz3.17}\
\begin{enumerate}[label=(\arabic*)]
\item Seien $\P,Q$ Wahrscheinlichkeitsmaße auf $\B(\S)$. Dann gilt:
\begin{align*}
\P\equiv Q\Longleftrightarrow
\int\limits f\d\P=\int\limits f\d Q\qquad\forall f\in C^b(\S)\text{ glm. stetig}
\end{align*}
\item $\begin{aligned}
X\stackeq{\mathcal{L}} Y\Longleftrightarrow
\E\big[f(X)\big]=\E\big[f(Y)\big]\qquad\forall f\in C^b(\S)\text{ glm. stetig}
\end{aligned}$
\end{enumerate}
\end{satz}
\begin{proof}
\underline{Zu (1) zeige ``$\implies$'':} Klar.\nl
\underline{Zu (1) zeige ``$\Longleftarrow$'':}
\begin{align*}
\B(\S)\stackeq{\text{3.2 (1)}}\sigma\big(\mathcal{F}(\S)\big)
\end{align*}
und $\F$ ist Durchschnittsstabil. Wegen dem Maßeindeutigkeitssatz reicht es zu zeigen:
\begin{align*}
\P(F)=Q(F)\qquad\forall F\in\mathcal{F}(\S)
\end{align*}
Sei $F\subseteq\S$ abgeschlossen. Setze
\begin{align*}
f_k(x):=\varphi\Big(k\cdot d(x,F)\Big)
\end{align*}
(vgl. Satz 2.4). Aus dem Lemma \ref{lemma2.3} folgt, dass die $f_k$ beschränkt und gleichmäßig stetig sind mit $f_k\stackrel{k\to\infty}{\downarrow}\indi_F$. Also gilt:
\begin{align*}
\P(F)
&\stackeq{\text{Def}}
\int\limits\indi_F\d\P
=\int\limits\lim\limits_{k\to\infty} f_k\d\P
\stackeq{\text{Mono-Konv}}
\lim\limits_{k\to\infty}\int\limits f_k\d\P
\stackeq{\text{Vor}}
\lim\limits_{k\to\infty}\int\limits f_k\d Q\\
&\stackeq{\text{Mono-Konv}}
\int\limits\lim\limits_{k\to\infty} f_k\d Q
=\int\limits\indi_F\d Q
\stackeq{\text{Def}} Q(F)
\end{align*}
Da $F$ beliebig war, folgt die Behauptung.\nl
\underline{Zu (2):} folgt aus (1) mit dem Transformationssatz \eqref{eqTrafo}:
\begin{align*}
X\stackeq{\mathcal{L}} Y
&\stackrel{\ref{def3.16}}{\Longleftrightarrow}
\underbrace{\P\circ X^{-1}}_{}=\underbrace{\P\circ Y^{-1}}_{}\\
&\stackrel{(1)}{\Longleftrightarrow}
\int\limits_{\S} f\d(\P\circ X^{-1})=\int\limits_{\S} f\d(\P\circ Y^{-1})
\end{align*}
und
\begin{align*}
\int\limits_{\S} f\d(\P\circ X^{-1})
\stackeq{\text{Trafo}}
\int\limits_{\Omega}\underbrace{f\circ X}_{=:f(X)}\d\P
\stackeq{\text{Def}}
\E\big[f(X)\big]
\end{align*}
\end{proof}

\section{Verteilungskonvergenz von Zufallsvariablen in metrischen Räumen}
Seien $X,X_n,n\in\N$ Zufallsvariablen in $(\S,d)$ über $(\Omega,\A,\P)$. Dann sind
%st die Verteilung von $X$, 
\begin{align*}
P:=\P\circ X^{-1},\qquad P_n:=\P\circ X_n^{-1},\qquad n\in\N
\end{align*}
sind Wahrscheinlichkeitsmaße auf $\B(\S)$.

\begin{definition}[Verteilungskonvergenz]\label{def4.1}\
\begin{enumerate}[label=(\arabic*)]
\item Seien $P,P_n,n\in\N$ Wahrscheinlichkeitsmaße auf $\B(S)$. Dann \textbf{konvergiert $P_n$ schwach gegen $P$}, in Zeichen
\begin{align*}
P_n\stackrelnew{w}{n\to\infty}{\longrightarrow} P
:\Longleftrightarrow
\int\limits f\d P_n\stackrel{n\to\infty}{\longrightarrow}\int\limits f\d P\qquad\forall f\in C^b(\S)
\end{align*}
Das $w$ steht für ``weakly''.
\item $X_n$ \textbf{konvergiert in Verteilung gegen $X$ in Raum $(\S,d)$}, in Zeichen
\begin{align*}
X_n\stackrel{\mathcal{L}}{\longrightarrow} X\text{ in }(\S,d)
:\Longleftrightarrow
\P\circ X_n^{-1}\stackrelnew{w}{n\to\infty}{\longrightarrow}\P\circ X^{-1}
\end{align*}
Alternative Schreibweise: $X_n\stackrel{\d}{\longrightarrow} X$. Das $\L$ steht für ``law''.
\end{enumerate}
\end{definition}

Äquivalente Charakterisierung von $\stackrelnew{w}{}{\longrightarrow}$ bzw. $\stackrel{\L}{\longrightarrow}$ in folgendem Satz:

\begin{satz}[Portmanteau-Theorem]\enter\label{satz4.2}
Folgende Aussagen sind äquivalent:
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
P_n\stackrelnew{w}{}{\longrightarrow} P
\end{aligned}$
\item $\begin{aligned}
\int\limits f\d P_n\stackrel{}{\longrightarrow}\int\limits f\d P\qquad\forall f\in C^b(\S)\text{ glm. stetig}
\end{aligned}$
\item $\begin{aligned}
\limsup\limits_{n\to\infty} P_n(F)\leq P(F)\qquad\forall F\in\F(\S)
\end{aligned}$
\item $\begin{aligned}
\liminf\limits_{n\to\infty} P_n(G)\geq P(G)\qquad\forall G\in\G(\S)
\end{aligned}$
\item $\begin{aligned}
\limn P_n(B)=P(B)\qquad\forall B\in\B(\S)\mit P(\underbrace{\partial B}_{\in\F(\S)})=0
\end{aligned}$\\
Mengen $B\in\B(\S)$ mit $P(\partial B)=0$ heißen \textbf{$P$-randlos}.
\end{enumerate}
\end{satz}
\begin{proof}
\underline{Zeige (1) $\implies$ (2):}\\
Folgt aus der Definition \ref{def4.1} (1).\nl
\underline{Zeige (2) $\implies$ (3):}\\
Sei $F\in\F(\S)$ (also abgeschlossen), Der Beweis von Satz \ref{satz3.17} zeigt: Es gibt eine Folge $(f_k)_{k\in\N}$ von gleichmäßig stetigen, beschränkten Funktionen auf $\S$ mit $f_k\downarrow\indi_F$. Dann gilt:
\begin{align*}
\limsup\limits_{n\to\infty} P_n(F)
=\limsup\limits_{n\to\infty}\int\limits\underbrace{\indi_F}_{\leq f_k~\forall k\in\N}\d P_n
\overset{\text{Mono}}&{\leq}
\limsup\limits_{n\to\infty}\int\limits f_k\d P_n
\stackrel{\text{Vor (2)}}{=}
\int\limits f_k\d P~~\forall k\in\N\\
\overset{\text{Mono Konv}}&{\implies}
\int\limits f_k\d P\stackrel{k\to\infty}{\longrightarrow}
\int\limits\indi_F\d P=P(F)\\
\overset{k\to\infty}&{\implies}
~(3)
\end{align*}

\underline{Zeige (3) $\Longleftrightarrow$ (4):}\\
Nutze Übergang zum Komplement sowie Rechenregeln für $\liminf$ und $\limsup$:
\begin{align*}
\liminf\limits_{n\to\infty} P_n(G)
&=\liminf\limits_{n\to\infty} \big(1-P_n(G^C)\big)\\
&=1-\underbrace{\limsup\limits_{n\to\infty} P_n(\underbrace{G^C}_{\in\F})}_{\leq P(G^C)}\\
&\geq 1-P(G^C)\\
&=P(G)
\end{align*}

\underline{Zeige (3) $\implies$ (1):}\\
Sei $f\in C^b(\S)$ beliebig. Zu zeigen:
\begin{align}\label{eqProof1.4.2Sternchen}
\limsup\limits_{n\to\infty}\int\limits f\d P_n\leq\int\limits f\d P
\end{align}
\underline{1. Schritt:} Sei $0\leq f<1$. Setze
\begin{align*}
F_i:=\left\lbrace f\geq\frac{i}{k}\right\rbrace=\left\lbrace x\in\S:f(x)\geq\frac{i}{k}\right\rbrace,\qquad \forall 0\leq i\leq k,k\in\N
\end{align*}
Dann gilt $F_i\in\F~\forall i$, da $f$ stetig. Da 
\begin{align*}
\int\limits_{\S}f\d P
\stackeq{\text{Lin}}
\sum\limits_{i=1}^k\int\limits\indi_{\left\lbrace\frac{i-1}{k}\leq f<\frac{i}{k}\right\rbrace}\cdot f\d P
\end{align*}
folgt wegen Monotonie
\begin{align}\label{eqProof1.4.2Plus}
\sum\limits_{i=1}^k\underbrace{\frac{i-1}{k}}_{=\frac{i}{k}-\frac{1}{k}}\cdot P\left(\frac{i-1}{k}\leq f<\frac{i}{k}\right)
\leq
\int\limits f\d P
\leq
\sum\limits_{i=1}^k \frac{1}{k}\cdot P\Big(\underbrace{\frac{i-1}{k}\leq f<\frac{i}{k}}_{F_{i-1}\setminus F_i}\Big)
\end{align}
Die rechte Summe in \eqref{eqProof1.4.2Plus} ist gleich
\begin{align*}
&\frac{1}{k}\cdot\sum\limits_{i=1}^k i\cdot\big( P(F_{i-1}-P(F_i)\big)\\
&=\frac{1}{k}\cdot\Big(P(F_0)-P(F_1)+2\cdot P(F_1-2\cdot P(F_2)+3\cdot P(F_2)-3\cdot P(F_3)+\\
&\qquad+\ldots+(k-1)\cdot P(F_{k-2})-(k-1)\cdot P(F_{k-1})+k\cdot P(F_{k-1})-k\cdot P(F_k)\Big)\\
&=\frac{1}{k}\cdot\Big(\underbrace{P(F_0)}_{=1}+P(F_1)+P(F_2)+\ldots+P(F_{k-1})-k\cdot k\cdot \underbrace{P(F_k)}_{=0}\Big)\\
&=\frac{1}{k}+\frac{1}{k}\cdot\sum\limits_{i=1}^{k-1} P(F_i)
\end{align*}
Da die linke Summe in \eqref{eqProof1.4.2Plus} gleich der rechten Summe minus $\frac{1}{k}$, folgt
\begin{align}\label{eqProof1.4.2DoppelSternchen}
\sum\limits_{i=1}^{k-1} P(F_i)
\leq\int\limits f\d P
\leq\frac{1}{k}+\sum\limits_{i=1}^{k-1} P(F_i)
\end{align}
Beachte, \eqref{eqProof1.4.2DoppelSternchen} gilt für \ul{jedes} Wahrscheinlichkeitsmaß $P$, also auch für $P_n$. Damit folgt:
\begin{align*}
\limsup\limits_{n\to\infty}\int\limits f\d P_n
&\leq\frac{1}{k}+\sum\limits_{i=1}^{k-1}\underbrace{\limsup\limits_{n\to\infty} P_n(F_i)}_{\stackrel{(3)}{\leq}P(F_i)~\forall i}\\
&\leq\frac{1}{k}+\underbrace{\sum\limits_{i=1}^{k-1} P(F_i)}_{\stackrel{\eqref{eqProof1.4.2DoppelSternchen}}{\leq}\int\limits f\d P}\\
&\leq\frac{1}{k}+\int\limits f\d P\qquad\forall k\in\N
\end{align*}
Grenzwertbildung $k\to\infty$ liefert \eqref{eqProof1.4.2Sternchen}.\nl
\ul{2. Schritt:} Da $f\in C^b(\S)$ beliebig, gilt:
\begin{align*}
\exists a<b:a\leq f<b
\implies g(x):=\frac{f(x)-a}{b-a}\text{ ist stetig und } 0\leq g<1
\end{align*}
Daraus folgt
\begin{align*}
\limsup\limits_{n\to\infty}\int\limits f\d P_n
&=\limsup\limits_{n\to\infty}\int\limits (b-a)\cdot g+a\d P_n\\
&=\limsup\limits_{n\to\infty}\left((b-a)\cdot\int\limits g\d P_n+a\right)\\
&\leq(b-a)\cdot\underbrace{\limsup\limits_{n\to\infty}\int\limits g\d P_n}_{\leq\int\limits g\d P\text{, wg. 1. Schritt}}+a\\
&\leq(b-a)\cdot\int\limits g\d P+0\\
\overset{\text{Lin}}&=
\int\limits f\d P
\end{align*}
Damit ist \eqref{eqProof1.4.2Sternchen} gezeigt. Übergang zu $-f$ in \eqref{eqProof1.4.2Sternchen} liefert
\begin{align*}
\int\limits f\d P
&\leq
\liminf\limits_{n\to\infty}\int\limits f\d P_n\\
&=\liminf\limits_{n\to\infty}-\int\limits -f\d P_n\\
&=-\limsup\limits_{n\to\infty}-\int\limits \underbrace{-f}_{\in C^b(\S)}\d P\\
\overset{\eqref{eqProof1.4.2Sternchen}}&{\geq}
-\int\limits -f\d P\\
\overset{\text{Lin}}&=
\int\limits f\d P
\qquad\forall f\in C^b(\S)\\
&\implies(1)
\end{align*}

\underline{Zeige (3) $\implies$ (5):}\\
Sei $B\in\B(\S)\mit P(\partial B)=0$. Dann gilt:
\begin{align*}
P(\overline{B})
\overset{(3)}&{\geq}
\limsup\limits_{n\to\infty} \underbrace{P_n(\overbrace{\overline{B}}^{\supseteq B})}_{\geq P_n(B)}\\
&\geq\limsup\limits_{n\to\infty} P_n(B)\\
\overset{\text{stetig}}&{\geq}
\liminf\limits_{n\to\infty} \underbrace{P_n(\overbrace{B}^{\supseteq\stackrel{\circ}{B}})}_{P_n(\stackrel{\circ}{B}}\\
\overset{(3)\gdw(4)}&{\geq}
P(\stackrel{\circ}{B})\\
&=P(\overline{B})\\
&=P(B),
\end{align*}
denn:
\begin{align}\label{eqProof1.4.2SternchenZwei}
0
=P(\overbrace{\partial B}^{\overline{B}\setminus\stackrel{\circ}{B}}
)=P(\overline{B})-P(\stackrel{\circ}{B})
\implies
P(\stackrel{\circ}{B})\leq P(B)\leq P(\overline{B})=P(\stackrel{\circ}{B})
\end{align}
Da $\liminf=\limsup$ folgt $\limn P_n(B)=P(B)$.\nl
\underline{Zeige (5) $\implies$ (3):}\\
Sei $F\in\F$ (abgeschlossen) beliebig. Dann gilt $\forall\varepsilon>0:$
\begin{align}\label{eqProof1.4.2SternchenUnten}
\partial\Big(\big\lbrace x\in\S:d(x,F)\leq\varepsilon\big\rbrace\Big)
\subseteq\big\lbrace x\in\S:d(x,F)=\varepsilon\big\rbrace
\end{align}
denn: Sei $x\in\partial\Big(\big\lbrace x\in\S:d(x,F)\leq\varepsilon\big\rbrace\Big)$. Dann gilt:
\begin{align*}
&\exists (x_n)_{n\in\N}:\forall n\in\N:d(x_n,F)\leq\varepsilon\wedge \limn x_n=x\\
&\exists (\xi_n)_{n\in\N}:\forall n\in\N:d(\xi_n,F)>\varepsilon\wedge\limn\xi_n=x
\end{align*}
Da $d(\cdot,F)$ stetig gemäß \ref{lemma2.3} (3), folgt
\begin{align*}
\varepsilon\leq d(x,F)\leq\varepsilon.
\end{align*}
Wegen \eqref{eqProof1.4.2SternchenUnten} sind 
\begin{align*}
A_\varepsilon:=\partial\Big(\big\lbrace x\in\S:d(x,F)\leq\varepsilon\big\rbrace\Big)\qquad\forall\varepsilon>0
\end{align*}
paarweise disjunkt, da bereits die Obermengen paarweise disjunkt sind. Dann folgt
\begin{align}\label{eqProof1.4.2DoppelSternchenUnten}
E:=\big\lbrace\varepsilon>0:P(A_\varepsilon)>0\big\rbrace\text{ ist höchstens abzählbar},
\end{align}
denn:
\begin{align*}
E=\bigcup\limits_{n\in\N}\underbrace{\left\lbrace\varepsilon>0:P(A_\varepsilon)\geq\frac{1}{m}\right\rbrace}_{=:E_m}
\end{align*}
Es gilt $|E_m|\leq m$, weil: Angenommen es existieren $0<\varepsilon_1<\ldots<\varepsilon_{m+1}$ mit 
\begin{align*}
&P(A_{\varepsilon_i})\geq\frac{1}{m}\qquad\forall 1\leq i\leq m+1\\
&\implies
1\geq P\left(\bigcup\limits_{i=1}^{m+1} A_{\varepsilon_i}\right)
\stackeq{\text{pw. disj.}}
\sum\limits_{i=1}^{m+1}\underbrace{P\big(A_{\varepsilon_i}\big)}_{\geq\frac{1}{m}}\geq(m+1)\cdot\frac{1}{m}>1
\end{align*}
Das ist ein Widerspruch. Damit ist $E$ höchstens abzählbar unendlich. Damit liegt das Komplement
\begin{align*}
E^C=\big\lbrace\varepsilon>0: P(A_\varepsilon)=0\big\rbrace
\end{align*}
dicht in $[0,\infty)$. (dies kann man auch durch Widerspruch zeigen)\\
Daraus folgt insbesondere:
\begin{align*}
\exists(\varepsilon_k)_{k\in\N}\subseteq\R\mit\varepsilon_k\downarrow0:\forall	 k\in\N: F_k:=\big\lbrace x\in\S:d(x,F)\leq\varepsilon_k\big\rbrace\text{ ist $P$-randlos}
\end{align*}
Beachte $A_{\varepsilon_k}=\partial F_k$. Da $F\subseteq F_k~\forall k\in\N$,gilt:
\begin{align*}
&\limsup\limits_{n\to\infty} P_n(F)
\leq\limsup\limits_{n\to\infty} \underbrace{P_n(F_k)}_{\text{konv.}}
\stackeq{(5)}
P(F_k)\qquad\forall k\in\N\\
&\stackrel{k\to\infty}{\implies}
\limsup\limits_{n\to\N} P_n(F)
\leq\lim\limits_{k\to\infty} P(F_k)
=P(F)
\end{align*}
Die letzte Gleichheit gilt, weil $P$ $\sigma$-stetig von oben ist und $F_k\downarrow F$. $F_k\downarrow F$, denn $F_1\supseteq F_2\supseteq\ldots$, da $\varepsilon_k$ monoton fallende Folge ist und
\begin{align*}
\bigcap\limits_{k\in\N}F_k=F, 
\end{align*}
denn: 
\begin{align*}
x\in\bigcap\limits_{k\in\N}F_k
&\Longleftrightarrow
x\in F_k\qquad\forall k\in\N\\
&\Longleftrightarrow
d(x,F)\leq\varepsilon_k\qquad\forall k\in\N\\
&\implies
d(x,F)=0\\
\overset{\ref{lemma2.3}~(1)}&{\Longleftrightarrow}
x\in \overline{F}\stackeq{F\in\F}F
\end{align*}
\end{proof}

Mitunter folgt schwache Konvergenz aus $P_n(A)\stackrel{n\to\infty}{\longrightarrow} P(A)$ für eine spezielle Klasse von Mengen $A$.

\begin{theorem}\label{theorem4.3}
Sei $\U\subseteq\B(\S)$ mit
\begin{enumerate}[label=(\roman*)]
\item $\begin{aligned}
A,B\in \U\implies A\cap B\in\U
\end{aligned}$, also $\U$ ist endlich $\cap$-stabil
\item Jedes offene $G$ ist abzählbare Vereinigung von Mengen aus $\U$.
\end{enumerate}
Dann gilt:
\begin{align*}
\Big(\forall A\in\U:P_n(A)\stackrel{n\to\infty}{\longrightarrow} P(A)\Big)\implies P_n\stackrelnew{w}{}{\longrightarrow} P
\end{align*}
\end{theorem}

\begin{proof}
Seien $A_1,\ldots,A_m\in\U$. Dann gilt:
\begin{align*}
P_n\left(\bigcup\limits_{i=1}^m A_i\right)
\overset{\text{allg. Add-Formel}}&=
\sum\limits_{k=1}^m (-1)^{k-1}\cdot\sum\limits_{1\leq i_1<\ldots<i_k\leq m}\underbrace{P_n\big(\underbrace{A_{i_1}\cap\ldots\cap A_{i_k})}_{\in\U}}_{\stackrel{n\to\infty}{\longrightarrow} P(A_{i_1}\cap\ldots\cap A_{i_k})}\\
\overset{n\to\infty}&{\longrightarrow}
\sum\limits_{k=1}^m\sum\limits_{1\leq i_1<\ldots<i_k\leq m} P\left(A_{i_1}\cap\ldots\cap A_{i_k}\right)\\
\overset{\text{all. Add.}}&=
P\left(\bigcup\limits_{i=1}^m A_i\right)
\end{align*}
Sei $G\in\G$. Dann gilt wegen Voraussetzung (ii):
\begin{align*}
&\exists(A_i)_{i\in\N}\subseteq\U:G=\bigcup\limits_{i\in\N} A_i\\
&\implies
G_m:=\bigcup\limits_{i=1}^m A_i\uparrow G,~m\to\infty\\
\overset{P~\sigma\text{-stetig}}&{\implies}
\forall \varepsilon>0:\exists m_0\in\N:P(G)-P(G_{m_0})\leq\varepsilon\\
&\implies
P(G)-\varepsilon\leq P(G_{m_0})
\stackeq{\text{s. o.}}
\limn P_n(\underbrace{G_{m_0}}_{\subseteq G})\leq\liminf\limits_{n\to\infty} P_n(G)\qquad\forall\varepsilon>0\\
\overset{\varepsilon\to0}&{\implies}
\liminf\limits_{n\to\infty} P_n(G)\geq P(G)\qquad\forall G\in\G
\end{align*}
Nun folgt die Behauptung aus dem Theorem \ref{satz4.2}.
\end{proof}

\begin{korollar}\label{korollar4.4}
Sei $\U$ endlich Durschnittsstabil mit
\begin{enumerate}[label=(\roman*)]
\item $\begin{aligned}
\forall x\in S,\forall\varepsilon>0:\exists A\in\U:x\in\stackrel{\circ}{A}\subseteq A\subseteq B(x,\varepsilon)
\end{aligned}$
\end{enumerate}
Ist $(\S,d)$ separabel, so gilt
\begin{align*}
\Big(\forall A\in\U:P_n(A)\stackrel{n\to\infty}{\longrightarrow} P(A)\Big)
\implies P_n\stackrelnew{w}{}{\longrightarrow} P
\end{align*}
\end{korollar}

\begin{proof}
Gemäß Satz \ref{satz2.9} hat $\G$ abzählbare Basis. Nach dem Satz von Lindelöf:
\begin{align}\label{eqSatzVonLindelöf}\tag{L}
\text{Für jede offene Überdeckung einer beliebigen Teilmenge von $\S$ existiert}\\\nonumber\text{eine abzählbare Teilüberdeckung.}
\end{align}
Sei nun $G\in\G$ beliebig. Für alle $x\in G$ existiert ein $\varepsilon_x>0$ mit $B(x,\varepsilon_x)\subseteq G$.
Gemäß (i) findet man ein $A_x\in\U$ mit $x\in\stackrel{\circ}{A}_x\subseteq A_x\subseteq B(x,\varepsilon_x)\subseteq G$. Also folgt
\begin{align*}
G=\bigcup\limits_{x\in G}\lbrace x\rbrace\subseteq\bigcup\limits_{x\in G}\stackrel{\circ}{A}_x\subseteq G
\end{align*}
Somit ist $\left\lbrace\stackrel{\circ}{A}_x:x\in G\right\rbrace$ eine offene Teilüberdeckung von $G$. Aus \eqref{eqSatzVonLindelöf} folgt nun: Es existieren $A_{x_i}\in\U,~i\in\N$ mit
\begin{align*}
G\subseteq\bigcup\limits_{i\in\N}\stackrel{\circ}{A}_{x_i}\subseteq 
\bigcup\limits_{i\in\N}A_{x_i}\subseteq 
G
\implies
G=\bigcup\limits_{i\in\N}A_{x_i}
\end{align*}
Also erfüllt $\U$ die Voraussetzung (i) und (ii) in Theorem \ref{theorem4.3} und es folgt die Behauptung.
\end{proof}

Als Anwendung / Beschreibung der schwachen Konvergenz im $\S=\R$. Erinnere an \textbf{schwache Konvergenz von Verteilungsfunktionen (VF)} $(F_n)_{n\in\N}$ gegen $F$, in Zeichen
\begin{align*}
F_n\rightharpoonup F
:\Longleftrightarrow
F_n(x)\stackrel{n\to\infty}{\longrightarrow} F(x)\qquad\forall x\in C_F
\mit C_F:=\big\lbrace x\in\R:F\text{ ist stetig in }x\big\rbrace
\end{align*}

\begin{korollar}\label{korollar4.5}
Seien $P,P_n,n\in\N$ Wahrscheinlichkeitsmaße auf $\B(\R)$ mit zugehörigen Verteilungsfunktionen $F$ und $F_n,n\in\N$. Dann gilt:
\begin{align*}
P_n\stackrelnew{w}{}{\longrightarrow}P
\Longleftrightarrow
F_n\rightharpoonup F
\end{align*}
\end{korollar}
\begin{proof}
\underline{Zeige ``$\implies$'':}\\
Sei $B:=(-\infty,x],~x\in\R$. Dann gilt:
\begin{align*}
P(\underbrace{\partial B}_{=\lbrace x\rbrace})=0
&\Longleftrightarrow P(\lbrace x\rbrace)=F(x)-\underbrace{F(x-0)}{\text{Grenzwert}}=0\\
& x\in C_F
\end{align*}
Somit folgt für $x\in C_F$:
\begin{align*}
F_n(x)&\stackeq{\text{Def}}
P_n\big(\underbrace{(-\infty,x]}_{=B}\big)=P_n(B)\stackrel{n\to\infty}{\longrightarrow} P(B)\stackeq{\text{Def}} F(x)\qquad\forall x\in C_F
\end{align*}
gemäß Satz \ref{satz4.2}.\nl
\underline{Zeige ``$\Longleftarrow$'':} Sei
\begin{align*}
\U:=\big\lbrace (a,b]:a,b\in C_F\big\rbrace.
\end{align*}
Dann ist $\U$ endlich durchschnittsstabil. Ferner: Die Menge 
\begin{align*}
D_F:=\big\lbrace x\in\R: F\text{ \underline{nicht} stetig in }x\big\rbrace
=\big\lbrace x\in\R:P(\lbrace x\rbrace)>0\big\rbrace
\end{align*}
ist höchstens abzählbar (vgl. \eqref{eqProof1.4.2DoppelSternchen} 
%keine Ahnung welches DoppelSternchen er hier meint mit der Referenz To Do
im Beweis von Satz \ref{satz4.2}). Also folgt
\begin{align*}
\forall x\in\R:\forall\varepsilon>0:\exists A=(a,b]\in\U:x\in (a,b)=\stackrel{\circ}{A}\subseteq A=(a,b]\subseteq B(x,\varepsilon)=(x-\varepsilon,x+\varepsilon)
\end{align*}
denn: In $(x-\varepsilon,x+\varepsilon)$ muss ein $a\in C_F$ existieren, denn sonst wäre $(x-\varepsilon, x)\subseteq D_F$. Analog findet man ein $b\in(x,x+\varepsilon)$. Somit erfüllt $\U$ die Voraussetzungen von Korollar \ref{korollar4.4}. Klar: $\S=\R$ ist separabel, da $\Q$ abzählbar und dicht in $\R$. Schließlich gilt:
\begin{align*}
P_n\big((a,b]\big)&=
F_n(\underbrace{b}_{\in C_F})-F_n(\underbrace{a}_{\in C_F})\stackrel{n\to\infty}{\longrightarrow} F(b)-F(a)=P\big((a,b]\big)\qquad\forall a,b\in C_F\\
&\stackrel{\ref{korollar4.4}}{\implies}
P_n\stackrelnew{w}{}{\longrightarrow} P
\end{align*}
\end{proof}

\begin{bemerkung}\ %4.6
\begin{enumerate}[label=(\arabic*)]
\item Seien $X,X_n,n\in\N$ reelle Zufallsvariablen über $(\Omega,\A,\P)$. Dann:
\begin{align}\label{eqBemerkung4.6}\tag{$\ast$} 
X_n\stackrel{\L}{\longrightarrow} X
\stackrel{\text{Def}}{\Longleftrightarrow}
\underbrace{\P\circ X_n^{-1}}_{\hat{=}P_n}
\stackrelnew{w}{}{\longrightarrow} \underbrace{\P\circ X^{-1}}_{\hat{=}P}
\stackrel{\ref{korollar4.5}}{\Longleftrightarrow}
\underbrace{\P(X_n\leq x)}_{\hat{=}F_n(x)}
\stackrel{n\to\infty}{\longrightarrow}
\underbrace{\P(X\leq x)}_{\hat{=}F(x)}
\end{align}
für alle $x$, die Stetigkeitsstellen der Verteilungsfunktion von $X$ sind.
\item Es gibt Verallgemeinerung von \ref{korollar4.5} bzw \eqref{eqBemerkung4.6} auf $\S=\R^k$:\\
Seien 
\begin{align*}
    X=\left(X^{(1)},\ldots, X^{(k)}\right),X_n=\left(X^{(1)}_n,\ldots,X^{(k)}_n\right),\qquad(\Omega,\A)\to\left(\R^k,\B(\R^k)\right)
\end{align*}
Zufallsvariablen in $\R^k$. Dann gilt:
\begin{align*}
X_n\stackrel{\L}{\longrightarrow} X\text{ in }\R^k
\Longleftrightarrow
\P(X_n\leq x)\stackrel{n\to\infty}{\longrightarrow}\P(X\leq x)\qquad\forall x=\big(x_1,\ldots,x_k\big)\in\R^k
\end{align*}
wobei $x_i$ Stetigkeitsstelle der Verteilungsfunktion von $X^{(i)}$ ist für alle $i\in\lbrace1,\ldots,k\rbrace$. Beweis ist analog zu \ref{korollar4.5}.
\end{enumerate}
\end{bemerkung}

Der schwache Limes einer Folge $(P_n)_{n\in\N}$ ist eindeutig, denn es gilt:

\begin{lemma}\label{lemma4.6Einhalb}
\begin{align*}
P_n\stackrelnew{w}{}{\longrightarrow} P,~P_n\stackrelnew{w}{}{\longrightarrow} Q\implies P=Q
\end{align*}
\end{lemma}
\begin{proof}
Gemäß Definition gilt:
\begin{align*}
\int\limits f\d P_n&\stackrel{}{\longrightarrow}\int\limits f\d P\qquad\forall f\in C^b(\S)\\
\int\limits f\d P_n&\stackrel{}{\longrightarrow}\int\limits f\d Q\qquad\forall f\in C^b(\S)
\end{align*}
Der Grenzwert von reellen Zahlenfolgen eindeutig ist, folgt
\begin{align*}
\int\limits f\d P=\int\limits f\d Q\qquad\forall f\in C^b(\S)\\
\stackrel{\ref{satz3.17}}{\implies}
P=Q
\end{align*}
\end{proof}

Im Folgenden ist das Ziel die Übertragung unserer Resultate auf Verteilungskonvergenz.

\begin{satz}[Portmanteau-Theorem]\label{satz4.7}\enter
Folgende Aussagen sind äquivalent:
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
X_n\stackrel{\L}{\longrightarrow} X\text{ in }(\S,d)
\end{aligned}$
\item $\begin{aligned}
\E\big[f(X_n)\big]\stackrel{n\to\infty}{\longrightarrow}\E\big[f(X)\big]\qquad\forall f\in C^b(\S)
\end{aligned}$ gleichmäßig stetig
\item $\begin{aligned}
\limsup\limits_{n\to\infty}\P(X_n\in F)\leq\P(X\in F)\qquad\forall F\in\F
\end{aligned}$
\item $\begin{aligned}
\liminf\limits_{n\to\infty}\P(X_n\in G)\geq\P(X\in G)\qquad\forall G\in\G
\end{aligned}$
\item $\begin{aligned}
\P(X_n\in B)\stackrel{n\to\infty}{\longrightarrow}\P(X\in B)\qquad\forall B\in\B(\S)\mit\P(X\in\partial B)=0
\end{aligned}$
\end{enumerate}
\end{satz}
\begin{proof}
Wende Satz \ref{satz4.2} an auf $P_n:=\P\circ X^{-1}_n,~P:=\P\circ X^{-1}$ (wegen Def $\stackrel{\L}{\longrightarrow}$). Beachte z. B.
\begin{align*}
P_n(F)&=\P\circ X^{-1}_n(F)
\stackeq{\text{Def}}
\P\left(X_n^{-1}(F)\right)
=\P\big(\lbrace\omega\in\Omega:X_n(\omega)\in F\rbrace\big)
=\P(X_n\in F)
\end{align*}
und 
\begin{align*}
\int\limits f\d P_n
=
\int\limits_{\S} f\d(\P\circ X_n^{-1})
\stackeq{\text{Trafo}}
\int\limits_\Omega f(X_n)\d\P
=\E\big[f(X_n)\big]
\end{align*}
\end{proof}

Ferner erhält man unter den jeweiligen Voraussetzungen in Theorem \ref{theorem4.3} bzw. Korollar \ref{korollar4.4}:
\begin{align*}
\P(X_n\in A)\stackrel{n\to\infty}{\longrightarrow}\P(X\in A)\qquad\forall A\in\U\\
\implies X_n\stackrel{\L}{\longrightarrow} X\text{ in }(\S,d)
\end{align*}
Und aus Lemma \ref{lemma4.6Einhalb}:
\begin{align*}
X_n\stackrel{\L}{\longrightarrow} X,X_n\stackrel{\L}{\longrightarrow} X'\implies X\stackeq{\L}X'
\end{align*}

\subsection*{Das Continuous Mapping Theorem (CMT)}
Sei $h:(\S,d)\to(\S',d')$ messbar.\\
\underline{Ziel:} Finde Bedingungen an $h$, so dass gilt:
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
P_n\stackrelnew{w}{}{\longrightarrow}P\implies P_n\circ h^{-1}\stackrelnew{w}{}{\longrightarrow} P\circ h^{-1}
\end{aligned}$
\item $\begin{aligned}
X_n\stackrel{\L}{\longrightarrow} X\text{ in }(\S,d)\implies h(X_n)\stackrel{\L}{\longrightarrow} h(X)\text{ in }(\S',d')
\end{aligned}$
\end{enumerate}
Beachte: 
\begin{align*}
\P\circ\big(h(X_n)\big)^{-1}&=\P\circ(h\circ X_n)^{-1}=\big(\P\circ X_n^{-1}\big)\circ h^{-1}\\
\P\big(h(X)\big)^{-1}&=\big(\P\circ X^{-1}\big)\circ h^{-1}
\end{align*}
Also folgt (2) aus (1). Zunächst gilt (1), wenn $h$ stetig auf $\S$  ist, denn: Sei $f\in C^b(\S')$ beliebig. Dann gilt:
\begin{align*}
\int\limits_{\S'} f\d \big(P_n\circ h^{-1}\big)
\stackeq{\text{Trafo}}
\int\limits_{\S} \underbrace{ f\circ h}_{\in C^b(\S)}\d P_n
\stackrel{\ref{def4.1}}{\longrightarrow}\int\limits f\circ h\d P
\stackeq{\text{Trafo}}
\int\limits f\d(P\circ h^{-1})\\
\stackrel{\ref{def4.1}}{\implies}
P_n\circ h^{-1}\stackrelnew{w}{}{\longrightarrow} P\circ h^{-1}
\end{align*}
Auf Stetigkeit von $h$ kann i. A. nicht verzichtet werden, denn es gilt:
\begin{beispiel} %4.8
Sei $\S=\S'=[0,1],~h:[0,1]\to[0,1]$ mit
\begin{align*}
h(x):=\left\lbrace\begin{array}{cl}
1, &\falls x\in\lbrace 0\rbrace\cup\left\lbrace\frac{1}{2\cdot n}:n\in\N\right\rbrace\\
0, & \sonst
\end{array}\right.
\end{align*}
Sei $P_n:=\delta_{\frac{1}{n}},n\in\N,P:=\delta_{0}$ wobei $\delta_x$ das Dirac-Maß bezeichnet.\\
Dann gilt $P_n\stackrelnew{w}{}{\longrightarrow} P$, denn
\begin{align*}
\int\limits f\d P_n&=f\left(\frac{1}{n}\right)\stackrel{n\to\infty}{\longrightarrow} f(0)=\int\limits f\d P\qquad\forall f\in C^b\big([0,1]\big)
\end{align*}
Aber wegen 
\begin{align*}
\delta_x\circ h^{-1}=\delta_{h(x)}
\end{align*}
gilt für
\begin{align*}
Q_n:=P_n\circ h^{-1}=\delta_{\frac{1}{n}}\circ h^{-1}=\delta_{h\left(\frac{1}{n}\right)},\qquad Q:=\P\circ h^{-1}=\delta_1
\end{align*}
das Folgende:
\begin{align*}
\int\limits f\d Q_n=f\left(h\left(\frac{1}{n}\right)\right)=\left\lbrace\begin{array}{cl}
f(1), & \falls n\text{ gerade }\\
0, & \falls n\text{ ungerade }
\end{array}\right.
\end{align*}
Sei $f\in C^b\big([0,1]\big)$ mit $f(0)\neq f(1)$ (z. B. $f=\id$). Folglich ist die Folge $\left(\int f\d Q_n\right)_{n\in\N}$ divergent. Somit:
\begin{align*}
\implies\int\limits f\d Q_n\not\longrightarrow\int\limits f\d Q\implies Q_n\stackrelnew{w}{}{\not\to} Q
\end{align*}
\end{beispiel}

Die Forderung der Stetigkeit lässt sich aber abschwächen so, dass (1) noch gilt. Dazu definiere die Menge der Unstetigkeitsstellen
\begin{align*}
D_h:=\big\lbrace x\in\S: h\text{ \underline{nicht} stetig in }x\big\rbrace.
\end{align*}

\begin{lemma}\label{lemma4.9}
\begin{align*}
D_h\in\B(\S)\qquad\forall h:\S\to\S'\text{ beliebig (nicht einmal messbar)}
\end{align*}
\end{lemma}
\begin{proof}
Mit der Dreiecksungleichung überlegt man sich leicht:
\begin{align*}
h\text{ stetig in }x
\Longleftrightarrow
&\forall 0<\varepsilon\in\Q:\exists 0<\delta\in\Q:\exists y,z\in\S:\\
&d(x,y)<\delta\wedge d(x,z)<\delta
\implies d'\big(h(y),h(z)\big)<\varepsilon
\end{align*}
Damit folgt:
\begin{align}\label{eqProof4.9Plus}
\bigcup\limits_{0<\varepsilon\in\Q}\bigcap\limits_{0<\delta\in\Q}\underbrace{\big\lbrace x\in\S:\exists y,z\in\S\mit d(x,y)<\delta\wedge d(x,y)<\delta\wedge d'\big(h(y),h(z)\big)\geq\varepsilon\big\rbrace}_{=:A_{\varepsilon,\delta}}
\end{align}
Zeige 
\begin{align}\label{eqProof4.9Stern}
A_{\varepsilon,\delta}\in\G(\S)\qquad\forall\varepsilon,\delta>0
\end{align}
Dazu sei $x_0\in A_{\varepsilon,\delta}$. Dann existieren $y,z\in\S$ mit $d(x_0,y)<\delta$ und $d(z_0,z)<\delta$, aber $d'\big(h(y),h(z)\big)\geq\varepsilon$. Wähle
\begin{align*}
\delta_0:=\min\big\lbrace\delta- d(x_0,y),\delta-d(x_0,z)\big\rbrace>0
\end{align*}
Ferner gilt:
\begin{align}\label{eqProof4.9i}
B(x_0,\delta_0)\subseteq A_{\varepsilon,\delta},
\end{align}
denn: Sei $x\in B(x_0,\delta_0)$. Dann gilt
\begin{align*}
d(x,y)\leq d(x,x_0)+d(x_0,y)<\delta_0+d(x_0,y)<\delta
\end{align*}
und analog
\begin{align*}
d(x,z)<\delta.
\end{align*}
Also folgt $x\in A_{\varepsilon,\delta}$, denn $d'\big((h(y),h(z)\big)\geq\varepsilon$. Wegen \eqref{eqProof4.9i} ist $x_0$ innerer Punkt von $A_{\varepsilon,\delta}$. Also folgt \eqref{eqProof4.9Stern} und mit \eqref{eqProof4.9Plus} dann die Behauptung.
\end{proof}



\begin{satz}[Continuous Mapping Theorem (CMT)]\enter\label{satz4.10ContinuousMappingTheorem}
Sei $h:(\S,d)\to(\S',d')$ $\B(\S)$-$\B(\S)$-messbar. Dann gilt:
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
P_n\stackrelnew{w}{}{\longrightarrow} P\wedge P(D_h)=0
\implies P_n\circ h^{-1}\stackrelnew{w}{}{\longrightarrow} P\circ h^{-1}
\end{aligned}$
\item $\begin{aligned}
X_n\stackrel{\L}{\longrightarrow}\text{ in }(\S,d)\wedge\P(X\in D_h)=0
\implies h(X_n)\stackrel{\L}{\longrightarrow} h(X)\text{ in }(\S',d') 
\end{aligned}$
\end{enumerate}
\end{satz}
\begin{proof}
\underline{Zeige (1):}\\
Sei $F\in\F(\S')$ (d. h. $F\subseteq\S'$ abgeschlossen) beliebig. Dann gilt:
\begin{align}\label{eqProof1.4.10(i)}
\limsup\limits_{n\to\infty} P_n\circ h^{-1}(F)
&=\limsup\limits_{n\to\infty} P_n\big(\underbrace{h^{-1}(F)}_{\subseteq \overline{h^{-1}(F)}}\big)\\\nonumber
&\leq \limsup\limits_{n\to\infty} P_n\big(\underbrace{\overline{h^{-1}(F)}}_{\in\F(\S)}\big)\\\nonumber
\overset{\ref{satz4.2}}&{\leq}
P\big(\overline{h^{-1}(F)}\big)
\end{align}
Es gilt
\begin{align}\label{eqProof1.4.10(ii)}
\overline{h^{-1}(F)}\subseteq h^{-1}(F)\cup D_h,
\end{align}
denn: Sei $x\in\overline{h^{-1}(F)}$.\nl
\underline{Fall 1: $x\in D_h$}
\begin{align*}
x\in D_h\implies x\in h^{-1}(F)\cup D_h
\end{align*}
\underline{Fall 2: $x\not\in D_h$}\\
Also ist $x\in C_h$, d. h. $h$ ist stetig in $x$. Ferner existiert eine Folge $(x_n)_{n\in\N}\subseteq h^{-1}(F)$ mit $x_n\stackrel{n\to\infty}{\longrightarrow} x$. Wegen der Stetigkeit gilt
\begin{align*}
\underbrace{h(x_n)}_{\in F~\forall n}\stackrel{n\to\infty}{\longrightarrow} h(x)
\implies h(x)\in\overline{F}=F\implies x\in h^{-1}(F)
\implies x\in h^{-1}(F)\cup D_h
\end{align*}
Mit \eqref{eqProof1.4.10(ii)} folgt:
\begin{align*}
\P\Big(\overline{h^{-1}(F)}\Big)
\stackrel{\eqref{eqProof1.4.10(ii)}}{\leq}
P\Big(h^{-1}(F)\cup D_h\Big)
\leq P\Big(h^{-1}(F)\Big)+\underbrace{P(D_h)}_{=0}
=P\circ h^{-1}(F)\\
\stackrel{\eqref{eqProof1.4.10(i)},\ref{satz4.2}(3)}{\implies} 
P_n\circ h^{-1}\stackrelnew{w}{}{\longrightarrow} P\circ h^{-1}
\end{align*}
\underline{Zeige (2):} folgt aus (1).
\end{proof}

\subsection*{Die Cramér'schen Sätze}
Zusammenstellung einiger Eigenschaften der Verteilungskonvergenz.

\begin{satz}[Teilfolgenprinzip für schwache Konvergenz]\label{satz4.11}\
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
Q_n\stackrelnew{w}{}{\longrightarrow} Q
\Longleftrightarrow
\text{Jede TF }(Q_{n'})\subseteq(Q_n)_{n\in\N}\text{ enthält TF }(Q_{n''})\subseteq(Q_{n'}):Q_{n''}\stackrelnew{w}{}{\longrightarrow} Q
\end{aligned}$
\item $\begin{aligned}
X_n\stackrel{\L}{\longrightarrow} X\Longleftrightarrow\text{Jede TF }(X_{n'})\subseteq(X_n)_{n\in\N}\text{ enthält TF }(X_{n''})\subseteq(X_{n'}):X_{n''}\stackrel{\L}{\longrightarrow} X
\end{aligned}$
\end{enumerate}
\end{satz}
\begin{proof}
Wir zeigen hier nur (1):\nl
\underline{Zeige ``$\implies$'':}\\
Folgt aus der Definition \ref{def4.1} und dem Teilfolgenprinzip für $(\int f\d Q)_{n\in\N}$ für alle $f\in C^b(\S)$.\nl
\underline{Zeige ``$\Longleftarrow$'':}\\
Angenommen $Q_n\stackrelnew{w}{}{\not\longrightarrow} Q$. Dann gilt:
\begin{align*}
\exists f\in C^b(\S):\int\limits f\d Q_n\not\longrightarrow\int\limits f\d Q
\end{align*}
Also existiert ein $\varepsilon>0$ und eine TF $(X_{n'})\subseteq(X_n)_{n\in\N}$ so, dass
\begin{align}\label{eqProof4.11Stern}
\left|\int\limits f\d Q_{n'}-\int\limits f\d Q\right|\geq\varepsilon\qquad\forall n'
\end{align}
im Widerspruch zur Voraussetzung, da \eqref{eqProof4.11Stern} insbesondere für alle $n''$ gilt.
\end{proof}

\begin{satz}\label{satz4.12}
\begin{align*}
X_n\stackrel{\P}{\longrightarrow} X\implies X_n\stackrel{\L}{\longrightarrow} X
\end{align*}
\end{satz}
\begin{proof}
Sei $(X_{n'})$ eine Teilfolge von $(X_n)_{n\in\N}$. Dann existiert gemäß \ref{satz3.13} eine TF $(X_{n''})$ von $(X_{n'})$ mit $X_{n''}\stackrel{n''\to\infty}{\longrightarrow} X$ $\P$-fast sicher.
Damit folgt aus Satz \ref{Satz3.8}:
\begin{align*}
&f\big(X_{n''}\big)\stackrel{n''\to\infty}{\longrightarrow}f(X)~\P\text{-fast sicher}&\forall f\in C^b(\S)\\
\overset{\text{dom Konv}}&{\implies}
\E\Big[f\big(X_{n''}\big)\Big]\stackrel{n''\to\infty}{\longrightarrow}\E\Big[f\big(X\big)\Big]&\forall f\in C^b(\S)\\
\overset{\ref{satz4.7}}&{\implies}
X_{n''}\stackrel{\L}{\longrightarrow} X\\
\overset{\ref{satz4.11}(2)}&{\implies}
X_n\stackrel{\L}{\longrightarrow} X
\end{align*}
\end{proof}
Einfache Beispiele zeigen, dass in \ref{satz4.12} die Umkehrung im Allgemeinen \underline{nicht} gilt. Aber:

\begin{satz}\label{satz4.13}
Sei $X_n\stackrel{\L}{\longrightarrow}X$ mit $X$ fast sicher konstant.\\
Dann gilt $X_n\stackrel{\P}{\longrightarrow} X$.
\end{satz}
\begin{proof}
Nach Voraussetzung existiert eine Konstante $x\in\S$ mit $\P(X=c)=1$. Wir verwenden das folgende Lemma:

\begin{lem}[Lemma von Uryson]\enter\label{lemmaVonUryson}
Zu $A,B\in\F(\S)$ mit $A\cap B=\emptyset$ existiert stetige 
\begin{align*}
f:\S\to[0,1]\qquad\mit\qquad f(x)=\left\lbrace\begin{array}{cl}
0, &\falls x\in A\\
1, &\falls x\in B
\end{array}\right.
\end{align*}
\end{lem}
Sei $\varepsilon>0$. Dann sind $A:=\lbrace c\rbrace, B=\big(B(c,\varepsilon)\big)^C\in\F(\S)\mit A\cap B=\emptyset$. Nun wenden wir das Lemma von Uryson an und erhalten die Existenz einer Abbildung $f:\S\to[0,1]$ stetig (also $f\in C^b(\S)$) mit der Eigenschaft
\begin{align*}
f(x)&=\left\lbrace\begin{array}{cl}
0, &\falls x=c\\
1, &\falls d(x,c)\geq\varepsilon
\end{array}\right.\\
\implies 0 \leq \P\big(d(X_n,X)>\varepsilon\big)
\overset{\text{Vor}}&=
\P\big(d(X_n,c)>\varepsilon\big)\\
&=\E\Big[\underbrace{\indi_{\lbrace d(X_n,c)\geq\varepsilon\rbrace}}_{\leq f(X_n)}\Big]\\
&\leq\E\big[f(X_n)\big]\stackrelnew{n\to\infty}{\text{Vor}}{\longrightarrow}\E\big[\underbrace{f(X)}_{=\underbrace{f(c)}_{=0}\text{ f.s.}}\big]\\
&=0
\end{align*}
Das Einschließ-Kriterium liefert
\begin{align*}
\P\big(d(X_n,X)\geq\varepsilon\big)\stackrel{n\to\infty}{\longrightarrow}0\qquad\forall\varepsilon>0
\end{align*}
\end{proof}

\begin{satz}[Cramér]\enter\label{satz4.14Cramer}
Seien $(X_n)_{n\in\N},(Y_n)_{n\in\N}$ zwei Folgen in separablen metrischen Raum $(\S,d)$, die \textbf{stochastisch äquivalent sind}, d. h.
\begin{align}\label{eq4.14StochastischAquivalent}\tag{Ä}
d(X_n,Y_n)\stackrel{\P}{\longrightarrow}0
\end{align}
Dann gilt:
\begin{align*}
X_n\stackrel{\L}{\longrightarrow} X\Longleftrightarrow Y_n\stackrel{\L}{\longrightarrow} X
\end{align*}
\end{satz}
\begin{proof}
Sei $f\in C^b(\S)$ gleichmäßig stetig, d. h.
\begin{align*}
    \forall\varepsilon>0\colon
        \exists\delta_\varepsilon>0\colon
        \forall x,y\in\S:\quad
        d(x,y) \leq \delta_\varepsilon \implies \left|f(x)-f(y)\right| \leq \varepsilon
\end{align*}
Damit folgt
\begin{align*}
&\Big|\E\big[f(X_n)\big]-\E\big[f(Y_n)\big]\Big|\\
\overset{\text{Lin}}&=
\left|\int\limits f(X_n)-f(Y_n)\d\P\right|\\
\overset{\text{DU}}&{\leq}
\int\limits\big|f(X_n)-f(Y_n)\big|\d\P\\
\overset{\text{Lin}}&=
\int\limits\underbrace{\indi_{\big\lbrace d(X_n,Y_n)\leq\delta\big\rbrace}\cdot\big|f(X_n)-f(Y_n)\big|}_{\leq\varepsilon\text{ wegen glm. Stetigkeit}}\d\P\\
&\qquad+
\int\limits\indi_{\big\lbrace d(X_n,Y_n)>\delta\big\rbrace}\cdot\underbrace{\big|f(X_n)-f(Y_n)\big|}_{\leq\big|f(X_n)\big|+\big|f(Y_n)\big|\leq2\cdot\Vert f\Vert_\infty}\d\P\\
&\leq\varepsilon+2\cdot\Vert f\Vert_\infty\cdot\underbrace{\P\big(d(X_n,Y_n)>\delta_\varepsilon\big)}_{\stackrel{n\to\infty}{\longrightarrow}0}\\
\implies
0&\leq\liminf\limits_{n\to\infty}\Big|\E\big[f(X_n)\big]-\E\big[f(Y_n)\big]\Big|\\
&\leq\limsup\limits_{n\to\infty}\Big|\E\big[f(X_n)\big]-\E\big[f(Y_n)\big]\Big|\\
&\leq\varepsilon+2\cdot\Vert f\Vert_\infty\cdot 0\\
&=\varepsilon\qquad\forall\varepsilon>0\\
&\stackrel{\varepsilon\to0}{\implies}
\E\big[f(X_n)\big]-\E\big[f(Y_n)\big]\stackrel{n\to\infty}{\longrightarrow} 0\qquad\forall f\in C^b(\S)\text{ glm. stetig}
\end{align*}
Es folgt:
\begin{align*}
X_n\stackrel{\L}{\longrightarrow} X
&\stackrel{\ref{satz4.7}}{\Longleftrightarrow}
\E\big[f(X_n)\big]\longrightarrow\E\big[f(X)\big]\qquad\forall f\in C^b(\S)\text{ glm. stetig}\\
&\Longleftrightarrow
\underbrace{\E\big[f(X_n)\big]}_{=\E\big[f(X_n)\big]+\Big(\E\big[f(Y_n)\big]-\E\big[ f(X_n)\big]\Big)\stackrel{\text{oben}}{\longrightarrow}0}\stackrel{}{\longrightarrow}\E\big[f(X)\big]~\forall f\in C^b(\S)\text{ glm. stetig}\\
&\stackrel{\ref{satz4.7}}{\Longleftrightarrow}
Y_n\stackrel{\L}{\longrightarrow} X
\end{align*}
\end{proof}

\pagebreak[4]
\begin{satz}[Cramér-Slutsky]\label{satz4.15CramerSlutsky}\enter
Seien $(\S,d)$, $(\S',d')$ separable metrische Räume. Dann gilt:
\begin{align*}
X_n\stackrel{\L}{\longrightarrow} X\wedge Y_n\stackrel{\L}{\longrightarrow} Y\wedge Y\text{ f.s. konstant}
\implies (X_n,Y_n)\stackrel{\L}{\longrightarrow}(X,Y)\text{ in }(\S\times\S',d\times d')
\end{align*}
\end{satz}
\begin{proof}
Nach Voraussetzung existiert ein $c\in\S'$ mit $\P(Y=c)=1$. Mit Satz \ref{satz4.13} gilt:
\begin{align}\label{eqProof4.15Stern}
Y_n\stackrel{\L}{\longrightarrow} c
\end{align}
Ferner:
\begin{align*}
&d\times d'\Big((X_n,Y_n),(X_n,c)\Big)
=\underbrace{d(X_n,X_n)}_{=0}+d'(Y_n,c)
=d'(Y_n,c)\\
&\implies\P\Big(d\times d'\big((X_n,Y_n),(X_n,c)\big)>\varepsilon\Big)
=\P\big(d'(Y_n,c)>\varepsilon\big)
\stackrel{n\to\infty}{\longrightarrow} 0\quad\forall\varepsilon>0\text{ wg. } \eqref{eqProof4.15Stern}\\
&\implies
\big((X_n,Y_n)\big)_{n\in\N}\text{ und }\big((X_n,c)\big)_{n\in\N}\text{ sind stochastisch äquivalent}
\end{align*}
Gemäß Satz \ref{satz4.14Cramer} reicht es zu zeigen, dass
\begin{align}\label{eqProof4.15Plus}
(X_n,c)\stackrel{\L}{\longrightarrow}(X,c)\stackrelnew{\L}{\text{f.s.}}{=}(X,Y)\text{ in }(\S\times\S',d\times d')
\end{align}
gilt. Dazu sei $f\in C^b(\S\times\S')$ beliebig und $f_c:\S\to\R,~f_c(x):=f(x,c)$. Damit folgt $f_c\in C^b(\S)$. Somit:
\begin{align*}
\E\big[f(X_n,c)\big]
&\stackeq{\text{Def}}
\E\big[f_c(X_n)\big]
\stackrel{n\to\infty}{\longrightarrow}\E\big[f_c(X)\big]\stackeq{\text{Def}}
\E\big[f(X,c)\big]\\
\stackrel{\text{Def}}{\implies}
\eqref{eqProof4.15Plus}
\end{align*}
\end{proof}

\begin{bemerkungnr}\label{bemerkung4.16} %4.16
Auf die Forderung, dass $Y$ f.s. konstant ist, kann \underline{nicht} verzichtet werden. Also ist die Schlussfolgerung
\begin{align*}
X_n\stackrel{\L}{\longrightarrow} X\text{ in }\S\wedge Y_n\stackrel{\L}{\longrightarrow} Y\text{ in }\S'
\implies(X_n,Y_n)\stackrel{\L}{\longrightarrow}(X,Y)\text{ in }\S\times\S'
\end{align*}
ist im Allgemeinen \underline{nicht} richtig!
\end{bemerkungnr}

\begin{korollar}\label{korollar4.17}
Seien $\S,\S'$ separable metrische Räume und sei $T$ beliebiger metrischer Raum. Dann gilt:
\begin{align*}
X_n\stackrel{\L}{\longrightarrow} X\wedge Y_n\stackrel{\P}{\longrightarrow} c\mit c\in\S'\text{ Konstante }
\implies h(X_n,Y_n)\stackrel{\L}{\longrightarrow} h(X,c)\text{ in }T
\end{align*}
wobei $h:\S\times\S'\to T$ messbar mit $\P\big((X,c)\in D_h\big)=0$
\end{korollar}
\begin{proof}
Folgt aus \ref{satz4.15CramerSlutsky} und Satz \ref{satz4.10ContinuousMappingTheorem}.
\end{proof}

\begin{beispiel}\label{beisp4.18}
\begin{align*}
X_n\stackrel{\L}{\longrightarrow} X\text{ in }\R^k\wedge Y_n\stackrel{\P}{\longrightarrow} c\text{ in }\R^k
\end{align*}
Dann gilt:
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
X_n+Y_n\stackrel{\L}{\longrightarrow} X+c\text{ in }\R^k
\end{aligned}$
\item $\begin{aligned}
\langle X_n,Y_n\rangle\stackrel{\L}{\longrightarrow}\langle c,X\rangle\text{ in }\R
\end{aligned}$
\item Für $k=1$ speziell:
$\begin{aligned}
Y_n\cdot X_n\stackrel{\L}{\longrightarrow}c\cdot X\text{ in }\R
\end{aligned}$ und für $c=0$:
$\begin{aligned}
Y_n\cdot X_n\stackrel{\L}{\longrightarrow}0\text{ in }\R
\end{aligned}$
\item $\begin{aligned}
\frac{X_n}{Y_n}\stackrel{\L}{\longrightarrow} \frac{X}{c},&\falls c\neq0\wedge\forall n\in\N:Y_n\neq0
\end{aligned}$
\item Konstante (= nicht zufällige) $Y_n$, d. h.
\begin{align*}
Y_n(\omega)=c_n\qquad\forall\omega\in\Omega,\forall n\in\N
\end{align*}
sind natürlich zugelassen:
\begin{align*}
(X_n,Y_n)\stackrel{\L}{\longrightarrow}(X,c)\text{ in }\R^k\times\R^k
\end{align*}
\end{enumerate}
\end{beispiel}

Es gilt:
\begin{align}\label{eqUnder4.18}
(X_n,Y_n)\stackrel{\L}{\longrightarrow}(X,Y)\text { in }\S\times S'
\implies X_n\stackrel{\L}{\longrightarrow} X\text{ in }\S\wedge
Y_n\stackrel{\L}{\longrightarrow} Y\text{ in }\S'
\end{align}
denn aus dem CMT folgt
\begin{align*}
X_n=\pi_1(X_n,Y_n)\stackrelnew{n\to\infty}{\L}{\longrightarrow}\pi_1(X,Y)=X,
\end{align*}
da die \textbf{Projektion}
\begin{align*}
\pi_1:\S\times\S'\to\S,~\pi_1(x,y):=x\qquad\forall (x,y)\in\S\times\S'
\end{align*}
stetig ist. Analog: $Y_n\stackrel{\L}{\longrightarrow} Y$.\\
Die Umkehrung in \eqref{eqUnder4.18} gilt im Allgemeinen \underline{nicht}! Vergleiche \ref{bemerkung4.16}. Sie gilt tatsächlich, falls $Y$ fast sicher konstant ist, vergleiche Satz \ref{satz4.15CramerSlutsky}.\\
Ziel: Finde eine andere zusätzliche Forderungen neben der linken Seite von \eqref{eqUnder4.18}, die die umgekehrte Implikation gestattet. Dazu:\\
Seien $(\S_1,d_1)$ und $(\S_2,d_2)$ zwei separable metrische Räume und $\S:=S_1\times\S_2$ mit $d:=d_1\times d_2$ der zugehörige Produktraum. (Es folgt, dass $(\S,d)$ separabel ist.) Eine geringfügige Modifikation  des Beweises von \ref{satz3.3} zeigt:
\begin{align*}
\B_d(\S_1\times\S_2)=\B_{d_1}(\S_1)\otimes\B_{d_2}(\S_2)
\end{align*}
Für ein Wahrscheinlichkeitsmaß $\P$ auf $\B(\S_1\times\S_2)$ definiere 
\begin{align*}
\P_1(A_1)&:P(A_1\times\S_2) &\forall A_1\in\B(\S_1)\\
\P_2(A_2)&:=P(\S_1\times A_2) &\forall A_2\in\B(\S_2)
\end{align*}
$\P_1$ und $\P_2$ heißen \textbf{Randverteilungen von $\P$}.

\begin{theorem}\label{theorem4.19}
Sei $(\S,d)$ separabel. Dann sind folgende Aussagen äquivalent:
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
\P_n\stackrelnew{\omega}{}{\longrightarrow} \P 
\end{aligned}$
\item $\begin{aligned}
\P_n(A_1\times A_2)\stackrel{n\to\infty}{\longrightarrow} \P(A_1\times A_2)\qquad\forall A_i\in\B(\S_i)~\P_i\text{-randlos mit } i=1,2
\end{aligned}$
\end{enumerate}
\end{theorem}
\begin{proof}
Seien $\partial,\partial_1,\partial_2$ die Randoperatoren in $(\S,d),(\S_1,d_1),(\S_2,d_2)$, respektive.\nl
\underline{Zeige (1) $\implies$ (2):}\\
Sei $A:=A_1\times A_2$ mit $A_i\in(2)$. Es gilt
\begin{align}\label{eqProof4.19Stern}\tag{$\ast$}
\partial A\subseteq((\partial_1 A_1)\times\S_2)\cup(\S_1\times(\partial_2 A_2))
\end{align}
%TODO Hier Skizze einfügen
Daraus folgt
\begin{align*}
&\P(\partial A)\leq \underbrace{\P((\partial A_1)\times\S_2)}{=\P_1(\partial A_1)=0}+\underbrace{\P(\S_1\times(\partial_2 A_2))}_ {=\P_2(\partial_2 A_2)=0}=0\\
&\implies
A\in\B(\S_1\times\S_2)\text{ ist $\P$-randlos}\\
&\stackrel{\ref{satz4.2}(5)}{\implies}(2)
\end{align*}

\underline{Zeige (2) $\implies$ (1):}\\
Wir wollen Korollar \ref{korollar4.4} anwenden. Dazu:\\
Wähle $d$ wie folgt aus:
\begin{align}\label{eqProof4.19Plus}\tag{+}
d\big((x_1,x_2),(y_1,y_2)\big)&:=\max\big\lbrace d_1(x_1,y_1),d_2(x_2,y_2)\big\rbrace
\end{align}
Sei
\begin{align*}
\U:=\big\lbrace A_1\times A_2:A_i\in\B(\S_i)~\P_i\text{-randlos}, i\in\lbrace1,2\rbrace\big\rbrace 
\end{align*}
$\U$ ist durchschnittsstabil, denn: Seien $A:=A_1\times A_2,B=B_1\times B_2\in\U$. Dann gilt
\begin{align}\label{eqProof4.19.1}\tag{1}
A\cap B=(A_1\cap B_1)\times(A_2\times B_2)
\end{align}
Ferner gilt
\begin{align}\label{eqProof4.19.2}\tag{2}
\partial_i(A_i\cap B_i)\subseteq(\partial_i A_i)\cup(\partial_i B_i)\qquad\forall i\in\lbrace1,2\rbrace
\end{align}
Aus \eqref{eqProof4.19.1} und \eqref{eqProof4.19.2} folgt: $\U$ ist durchschnittsstabil. Sei nun $x=(x_1,x_2)\in\S=\S_1\times\S_2$ und $\varepsilon>0$ beliebig. Aus \eqref{eqProof4.19Plus} folgt:
\begin{align*}
B_d(x,\varepsilon)=B_{d_1}(x_1,\varepsilon)\times B_{d_2}(x_2,\varepsilon)
\end{align*}
Sei 
\begin{align*}
A_\delta:=B_{d_1}(x_1,\delta)\times B_{d_2}(x_2,\delta)
\stackeq{\eqref{eqProof4.19Plus}} B_d(x,\delta)\qquad\forall\delta>0
\end{align*}
Somit gilt (vgl. Beweis von \ref{satz4.2})
\begin{align*}
\partial_i B_{d_i}(x_i,\delta)\subseteq\big\lbrace y\in\S_i:d_i(y,x_i)=\delta\big\rbrace\qquad\forall i\in\lbrace1,2\rbrace
\end{align*}
Folglich sind die Mengen $\partial_i B_{d_i}(x_i,\delta),\delta>0$ sind paarweise disjunkt. Die Mengen
\begin{align*}
E_i:=\Big\lbrace\delta>0:\P_i\big(\partial_i B_{d_i}(x_i,\delta)\big)>0\Big\rbrace\qquad\forall i\in\lbrace1,2\rbrace
\end{align*}
sind höchstens abzählbar (vgl. Beweis von \ref{satz4.2}). Folglich ist die Vereinigung $E_1\cup E_2$ höchstens abzählbar und somit liegt $(E_1\cup E_2)^C$ dicht in $[0,\infty)$. Mit
\begin{align*}
(E_1\cup E_2)^C&=E_1^C\cap E_2^C=\big\lbrace\delta>0:B_{d_i}(x_i,\delta)\text{ ist $\P_i$-randlos},i\in\lbrace1,2\rbrace\big\rbrace\\
\implies\exists\delta&\in(0,\varepsilon):B_{d_1}(x_1,\delta)~\P_1\text{-randlos und }B_{d_2}(x_2,\delta)~\P_2\text{-randlos}\\
\implies A_\delta&=B_{d_1}(x_1,\delta)\times B_{d_2}(x_2,\delta)\in\U\\
\overset{\eqref{eqProof4.19Plus}}&=
B_d(x,\delta)\subseteq B_d(x,\varepsilon)
\end{align*}
Also offene Kugel ist $A_\delta$ offen, also $\stackrel{\circ}{A_\delta}=A_\delta$. Schließlich folgt
\begin{align*}
x\in\stackrel{\circ}{A_\delta}=A_\delta\subseteq B_d(x,\varepsilon)
\end{align*}
Also erfüllt $\U$ die Voraussetzungen von Korollar \ref{korollar4.4}. Daraus folgt nun die Behauptung.
\end{proof}

Theorem \ref{theorem4.19} liefert ein nützliches Resultat für Produktmaße. Seien $\P^{(i)},\P_n^{(i)},n\in\N$ Wahrscheinlichkeitsmaße auf $\B(\S_i)$ mit $i\in\lbrace1,2\rbrace$. Dann sind
\begin{align*}
\P:=\P^{(1)}\otimes\P^{(2)},\qquad\P_n:=\P_n^{(1)}\otimes\P_n^{(2)}\qquad\forall n\in\N
\end{align*}
Produktmaße auf $\B(\S_1)\otimes\B(\S_2)=\B(\S_1\times\S_2)$ bei Separabilität.

\begin{theorem}\label{thoerem4.20}
Sei $\S=\S_1\times\S_2$ separabler Produktraum. Dann sind äquivalent:
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
\P_n\stackrelnew{w}{}{\longrightarrow}\P
\end{aligned}$
\item $\begin{aligned}
\P_n^{(1)}\stackrelnew{w}{}{\longrightarrow}\P^{(1)}\wedge
\P_n^{(2)}\stackrelnew{w}{}{\longrightarrow}\P^{(2)}
\end{aligned}$
\end{enumerate}
\end{theorem}
\begin{proof}
Gemäß Definition gilt:
\begin{align*}
\P(A_1\times A_2)&\stackeq{\text{Def}}\P^{(1)}(A_1)\cdot\P^{(2)}(A_2)\\
\P_n(A_1\times A_2)&\stackeq{\text{Def}}\P_n^{(1)}(A_1)\cdot\P_n^{(2)}(A_2)\\
\end{align*}
\underline{Zeige (1) $\implies$ (2):}
\begin{align*}
\P_n^{(1)}(A_1)&=\P_n(A_1\times\S_2)\stackrel{}{\longrightarrow}\P(A_1\times\S_2)=\P^{(1)}(A_1)
\qquad\forall A_1\in\B(\S_1)~\P^{(1)}\text{-randlos}
\end{align*}
gemäßig \ref{theorem4.19}, denn $\partial\S_2=\emptyset$. Also ist $\S_2\in\B(\S_2)~\P^{(2)}$-randlos. Also folgt aus \ref{satz4.2}(5):
\begin{align*}
\P_n^{(1)}\stackrelnew{w}{}{\longrightarrow}\P_n^{(2)}
\end{align*}
Analog zeigt man: $\P_n^{(2)}\stackrelnew{w}{}{\longrightarrow}\P^{(2)}$.\nl
\underline{Zeige (2) $\implies$ (1):}
\begin{align*}
&\P_n(A_1\times A_2)=\underbrace{\P_n^{(1)}(A_1)}_{\stackrel{n\to\infty}{\longrightarrow}\P^{(1)}(A_1)}\cdot\underbrace{\P_n^{(2)}(A_2)}_{\stackrel{n\to\infty}{\longrightarrow}\P^{(2)}(A_2)}\qquad\forall A_i~\P_i\text{-randlos},i=1,2\\
&\implies
\P^{(1)}(A_1)\cdot\P^{(2)}(A_2)=\P(A_1\times A_2)\qquad\forall A_i~\P_i\text{-randlos},i\in\lbrace1,2\rbrace\\
&\stackrel{\ref{theorem4.19}}{\implies}\P_n\stackrel{n\to\infty}{\longrightarrow}\P
\end{align*}
\end{proof}

Mit Theorem \ref{thoerem4.20} erhalten wir neben Satz \ref{satz4.15CramerSlutsky} ein weiteres Resultat über die\\ ``\ul{koordinatenweise Verteilungskonvergenz}'' (die ja im Allgemeinen nicht gilt).

\begin{satz}\label{satz4.21}
Sei $\S=\S_1\times\S_2$ separabel, $X,X_n,n\in\N$ Zufallsvariablen in $\S_1$, $Y,Y_n,n\in\N$ Zufallsvariablen in $\S_2$ über $(\Omega,\A,\P)$ und gelte
\begin{enumerate}
\item $X_n$ und $Y_n$ sind unabhängig für alle $n\in\N$
\item $X$ und $Y$ sind unabhängig
\end{enumerate}
Dann gilt:
\begin{align*}
X_n\stackrel{\L}{\longrightarrow} X\text{ in }\S_1\wedge Y_n\stackrel{\L}{\longrightarrow} Y\text{ in }\S_2
\Longleftrightarrow (X_n,Y_n)\stackrel{\L}{\longrightarrow}(X,Y)\text{ in }\S_1\times\S_2
\end{align*}
\end{satz}
\begin{proof}
Da
\begin{align*}
\P\circ (X_n,Y_n)^{-1}&\stackeq{\text{unab}}\P\circ X_n^{-1}\otimes\P\circ Y_n^{-1}\\
\P\circ (X,Y)^{-1}&\stackeq{\text{unab}}\P\circ X^{-1}\otimes\P\circ Y^{-1}
\end{align*}
folgt die Behauptung aus \ref{thoerem4.20} und Definition \ref{def4.1}.
\end{proof}

\begin{bemerkungnr}\label{bemerkung4.22}
Die Aussage in \ref{satz4.21} lässt sich problemlos auf $\S_1\times\ldots\times\S_d$ mit $d\geq2$ übertragen.
\end{bemerkungnr}

\section*{Anwendung in der Statistik}
Seien $X_i,i\in\N$ i.i.d. über $(\Omega,\A,\P)$, quadrat-integrierbar, $\mu:=\E[X_i]\in\R$,\\ $\sigma^2:=\Var(X_i)\in(0,\infty)$.\\ Das arithmetische Mittel konvergiert fast sicher gegen den Erwartungswert gemäß dem starken Gesetz der großen Zahlen (SGGZ / SLLN, Kolmogorov), also
\begin{align*}
\overline{X}_n=\frac{1}{n}\cdot\sum\limits_{i=1}^n X_i\stackrel{}{\longrightarrow}\mu~\P\text{-fast sicher}
\end{align*}
%fast sichere Konvergenz => stochastische Konvergenz => Verteilungskonvergenz?
Folglich gilt
\begin{equation}
\label{eqAnwendungInDerStatistik}
\begin{aligned}
\sqrt{n}\cdot\big(\overline{X}_n-\mu\big)
&=\sqrt{n}\cdot\frac{1}{n}\cdot\sum\limits_{i=1}^n(X_i-\mu)\\
&=\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^n(X_i-\mu)\\
&=\sigma\cdot\underbrace{\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^n\left(\frac{X_i-\mu)}{\sigma}\right)}_{\stackrel{\L}{\longrightarrow}\mathcal{N}(0,1)}\stackrel{\L}{\longrightarrow}\sigma\cdot\mathcal{N}(0,1)\stackeq{\L}\mathcal{N}(0,\sigma^2)
\end{aligned}
\end{equation}
wobei die linke Konvergenz aus dem zentralen Grenzwertsatz (TGWS / CLT) und die rechte Konvergenz auf dem CMT folgt (da $x\mapsto\sigma\cdot x$ stetig). Also folgt:
\begin{align}\label{eqAnwendungInDerStatistikStern}\tag{$\ast$}
\sqrt{n}\cdot\big(\overline{X}_n-\mu\big)\stackrel{\L}{\longrightarrow}\mathcal{N}(0,\sigma^2)
\end{align}
Die \textbf{empirische Varianz} ist 
\begin{align*}
S_n^2&:=\frac{1}{n}\cdot\sum\limits_{i=1}^n(X_i-\overline{X}_n)^2\\
&=\frac{1}{n}\cdot\sum\limits_{i=1}^n\big((X_i-\mu)-(\overline{X}_n-\mu)\big)^2\\
&=\frac{1}{n}\cdot\sum\limits_{i=1}^n\Big((X_i-\mu)^2-2\cdot(X_i-\mu)\cdot(\overline{X}_n-\mu)+(\overline{X}_n-\mu)^2\Big)\\
&=\frac{1}{n}\cdot\sum\limits(X_i-\mu)^2-2\cdot\underbrace{\frac{1}{n}\cdot\sum\limits_{i=1}^n(X_i-\mu)}_{=(\overline{X}_n-\mu)}\cdot(\overline{X}_n-\mu)+\underbrace{\frac{1}{n}\cdot\sum\limits_{i=1}^n(\overline{X}_n-\mu)^2}_{=(\overline{X}-\mu)^2}\\
\end{align*}
Man erhält schließlich:
\begin{align}\label{eqEmpVarAlternativePlus}\tag{+}
S_n^2&=\underbrace{\frac{1}{n}\cdot\sum\limits_{i=1}^n(X_i-\mu)^2}_{\stackrel{\text{SGGZ}}{\longrightarrow}\E\big[(X_1-\mu)^2\big]=\sigma^2~\P\text{-f.s.}}-\underbrace{(\overline{X}_n-\mu)^2}_{\stackrel{\text{SGGZ+\ref{Satz3.8}}}{\longrightarrow}0\text{ f.s.}}\\\nonumber
&\implies
S_n^2\stackrel{\ref{satz3.15}+\ref{Satz3.8}}{\longrightarrow}\sigma^2
\end{align}
(Hierbei wird bei der Anwendung der Sätze \ref{satz3.15} und \ref{Satz3.8} benutzt, dass $(x,y)\mapsto x+y$ stetig ist)

%Skorokhod (russischer Mathematiker), cadlag, rcll (right continues with left limit), in einem Skorokhod-Raum ist die Addtion NICHT stetig. Also ist obiger Schluss i.A. nicht richtig.
Und ähnlich: ($\sqrt{n}$ ist die \textbf{normalisierende Folge})
\begin{align*}
T_n&:=\sqrt{n}\cdot\left(S_n^2-\sigma^2\right)
\stackeq{\eqref{eqEmpVarAlternativePlus}}
\underbrace{\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^n\Big((X_i-\mu)^2-\sigma^2\Big)}_{=:V_n}\underbrace{-\sqrt{n}\cdot\big(\overline{X}-\mu\big)^2}_{=:R_n}
=V_n+R_n
\end{align*}
Mit CLT folgt (analog zur Herleitung von \eqref{eqAnwendungInDerStatistik})
\begin{align*}
V_n\stackrel{\L}{\longrightarrow}\mathcal{N}(0,\tau^2),\qquad\tau^2:=\Var\Big((X_1-\mu)^2\Big)
\end{align*}
falls $\E\big[|X-1|\big]^4<\infty$.
\begin{align*}
-R_n&=\underbrace{\Big(\sqrt{n}\cdot(\overline{X}_n-\mu)\Big)}_{\stackrelnew{\eqref{eqAnwendungInDerStatistikStern}}{\L}{\longrightarrow}\mathcal{N}(0,\sigma^2)}\cdot\underbrace{(\overline{X}_n-\mu)}_{\stackrelnew{\ref{Satz3.12}}{\P}{\longrightarrow}0}\stackrelnew{\ref{beisp4.18}(3)}{\P}{\longrightarrow}0\\
&\implies \underbrace{R_n}_{=|T_n-V_n|}\stackrel{\P}{\longrightarrow}0\\
&\implies(T_n)_{n\in\N},(V_n)_{n\in\N}\text{ sind stochastisch äquivalent}\\
&\stackrel{\ref{satz4.14Cramer}}{\implies}
T_n\stackrel{\L}{\longrightarrow}\mathcal{N}(0,\tau^2)
\end{align*}
\textbf{Zusammenfassung:}
\begin{align*}
\big(\overline{X}_n,S_n^2\big)\stackrel{n\to\infty}{\longrightarrow}\big(\mu,\sigma^2\big)\text{ in }\R^2\text{ fast sicher}
\end{align*}
d. h. $\big(\overline{X}_n,S_n^2\big)_{n\in\N}$ ist \textbf{stark konsistente Schätzerfolge} für den Parameter $(\mu,\sigma^2)$.
Fernen sind $(\overline{X}_n)_{n\in\N}$ und $(S_n^2)_{n\in\N}$ \textbf{asymptotisch normal}, d.h.
%FUN: Der persönliche Held von Ferger ist Skorokhot. Er hat ihn ca. 2000 mal auf einer Tagung und es gab sogar ein Foto von den beiden, dass aber durch Datenverlust verloren ging
\begin{align*}
\sqrt{n}\cdot\big(\overline{X}_n-\mu\big)\stackrel{\L}{\longrightarrow}\mathcal{N}(0,\sigma^2),\qquad
\sqrt{n}\cdot\big(S_n^2-\sigma^2\big)\stackrel{\L}{\longrightarrow}\mathcal{N}(0,\tau^2)
\end{align*}
Wie sieht es aus mit dem Vektor 
\begin{align*}
\begin{pmatrix}
\sqrt{n}\cdot\big(\overline{X}_n-\mu\big)\\
\sqrt{n}\cdot\big(S_n^2-\sigma^2\big)
\end{pmatrix}
=\sqrt{n}\cdot\begin{pmatrix}
\overline{X}_n-\mu\\
S_n^2-\sigma^2
\end{pmatrix}
\stackrel{\L}{\longrightarrow}?
\end{align*}

\section{Verteilungskonvergenz in \texorpdfstring{$\R^d$}{R\textasciicircum d}}
Korollar \ref{korollar4.5} bzw. dessen Erweiterungen \ref{lemma4.6Einhalb} (2) liefern eine \textbf{analytische} Methode zum Nachweis von $X_n\stackrel{\L}{\longrightarrow}X\text{ in }\R^d$. Eine weitere Methode fußt auf

\begin{definition}\label{def5.1}
Sei $X$ Zufallsvariable in $\R^d$ über $(\Omega,\A,\P)$ und
\begin{align*}
\langle x,y\rangle:=:x'y:=\sum\limits_{i=1}^d x_i\cdot y_i\qquad x=(x_1,\ldots,x_d),y=(y_1,\ldots,y_d)\in\R^d
\end{align*}
das Standard-Skalarprodukt in $\R^d$. Dann heißt
\begin{align*}
\varphi_X(t):=\E\Big[\exp\big(i\cdot\langle t,X\rangle\big)\Big]\qquad\forall t\in\R^d
\end{align*}
\end{definition}
heißt \textbf{charakteristische Funktion} von $X$.

\begin{satz}[Eindeutigkeitssatz]\label{satz5.2Eindeutigkeitssatz}
\begin{align*}
X\stackeq{\L} Y\Longleftrightarrow\varphi_X\equiv\varphi_Y
\end{align*}
\end{satz}
\begin{proof}
Siehe Buch \textit{Essentials in Probability} von Jacod und Protter (2000), Seite 107-108.
\end{proof}

\begin{satz}[Stetigkeitssatz]\label{satz5.3Stetigkeitssatz}
\begin{align*}
X_n\stackrel{\L}{\longrightarrow} X\text{ in }\R^d\Longleftrightarrow\forall t\in\R^d: \varphi_{X_n}(t)\stackrel{n\to\infty}{\longrightarrow}\varphi_X(t)
\end{align*}
\end{satz}
\begin{proof}
Siehe Vorlesung Wahrscheinlichkeitstheorie (Bachelor) oder Jacod und Protter (2000), Seite 163 ff.
\end{proof}

Sehr nützlich ist:

\begin{satz}[Cramér-Wold-Device]\label{satz5.4CramerWoldDevice}\enter
%Device bedeutet u. A. Trick. Das ist kein Name.
Folgende Aussagen sind äquivalent:
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
X_n\stackrel{\L}{\longrightarrow} X\text{ in }\R^d
\end{aligned}$
\item $\begin{aligned}
\langle t, X_n\rangle\stackrel{\L}{\longrightarrow}\langle t, X\rangle\text{ in }\R\qquad\forall t\in\R^d
\end{aligned}$
\end{enumerate}
\end{satz}
\begin{proof}
\begin{align*}
\varphi_X(t)
&\stackeq{\text{Def}}\E\Big[\exp\big(i\cdot\langle t,X\rangle\big)\Big]\qquad\forall t\in\R^d
\stackeq{d=1}\E\big[\exp(i\cdot X\cdot t)\big]
\end{align*}
\underline{Zeige (1) $\implies$ (2):}\\
Sei $t\in\R^d$. Dann ist $x\mapsto\langle t,x\rangle$ stetig auf $\R^d$. Und aus Satz \ref{satz4.10ContinuousMappingTheorem} (CMT) folgt dann (2).\nl
\underline{Zeige (2) $\implies$ (1):}
\begin{align*}
&\varphi_{X_n}(t)
\stackeq{\text{Def}}\E\Big[\exp\big(i\cdot\langle t,X_n\rangle\cdot 1\big)\Big]
\stackeq{\text{Def}}\varphi_{\langle t,X_n\rangle}(1)
\stackrelnew{\ref{satz5.3Stetigkeitssatz}}{n\to\infty}{\longrightarrow}\underbrace{\varphi_{\langle t,X\rangle}(1)}_{=\varphi_X(t)}\\
&\implies\varphi_{X_n}\stackrel{n\to\infty}{\longrightarrow}\varphi_X\text{ auf }\R^d
\stackrel{\ref{satz5.3Stetigkeitssatz}}{\implies}(1)
\end{align*}
\end{proof}

\section{Der multivariate zentrale Grenzwertsatz (ZGWS) für Dreiecksschemata} %6
Wir betrachen zunächst den \underline{univariaten} Fall. es gelte
\begin{align}\label{eq6.1}\tag{6.1}
X_{n,1},X_{n,2},\ldots,X_{n,n}\text{ sind unabhängige \ul{reelle} ZV }\forall n\in\N
\end{align}
Die Kollektion
\begin{align*}
\big\lbrace X_{n,k}:1\leq k\leq n,n\in\N\rbrace
\end{align*}
heißt \textbf{Dreiecksschema / $\Delta$-Schema}.
\begin{align*}
\begin{matrix}
X_{1,1}\\
X_{2,1} & X_{2,2}\\
X_{3,1} & X_{3,2} & X_{3,3}\\
\vdots & \vdots & \vdots & \ddots\\
X_{n,1} & X_{n,2} & \hdots & \hdots & X_{n,n}\\
\vdots &&&&\vdots & \ddots
\end{matrix}
\end{align*}

Sei 
\begin{align}\label{eq6.2}\tag{6.2}
\E[X_{n,k}]=0,~\sigma_{n,k}^2:=\E\left[X_{n,k}^2\right]<\infty~\forall n,k\in\N\text{ und }
s_n^2:=\sum\limits_{k=1}^n\sigma_{n,k}^2~\forall n\in\N
\end{align}

\begin{satz}[Lindeberg, 1922]\label{satz6.1Lindeberg1922}\enter
Es gelten \eqref{eq6.1} und \eqref{eq6.2} sowie
\begin{align}\label{eqSatz6.1LindebergLB}\tag{LB}
\sum\limits_{k=1}^n\E\left[X_{n,k}^2\cdot\indi_{\big\lbrace|X_{n,k}|>\varepsilon\big\rbrace}\right]\stackrel{n\to\infty}{\longrightarrow}0\qquad\forall\varepsilon>0.
\end{align}
Falls zusätzlich
\begin{align*}
s_n^2\stackrel{n\to\infty}{\longrightarrow}\sigma^2\in(0,\infty)
\end{align*}
gilt, so gilt:
\begin{align*}
\sum\limits_{k=1}^n X_{n,k}\stackrel{\L}{\longrightarrow}\mathcal{N}(0,\sigma^2)
\end{align*}
\end{satz}
\begin{bemerkung}
\begin{align*}
s_n^2=\Var\left(\sum\limits_{k=1}^n X_{n,k}\right)
\end{align*}
\end{bemerkung}
\begin{proof}
So ähnlich wie Beweis von klassischem zentralen Grenzwertsatz in der Vorlesung Wahrscheinlichkeitstheorie (Bachelor), nur technisch etwas komplizierter. Vergleiche auch Billingsley (1995), \textit{Probability and Measure}, Seite 359 ff.
\end{proof}

Im Folgenden betrachten wir die Verallgemeinerung auf den \underline{multivariaten} Fall.\\
Sei $\lbrace X_{n,k}:k\leq n,n\in\N\big\rbrace$ ein $\Delta$-Schema von Zufallsvariablen.
\begin{align*}
X_{n,k}=\left(X_{n,k}^{(1)},\ldots,X_{n,k}^{(d)}\right)\text{ in }\R^d
\end{align*}
Es gelte die \textbf{zeilenweise Unabhängigkeit}:
\begin{align}\label{eq6.3}\tag{6.3}
X_{n,1},\ldots,X_{n,n}\text{ sind unabhängig}\qquad\forall n\in\N
\end{align}
%Ferger arbeitet auch am Buß- und Bettag in der Uni. Er ist evangelisch getauft worden und sogar konfirmiert. 
Also die Vektoren seien unabhängig. Daraus folgt nicht, dass deren Komponenten unabhängig sind.
\begin{align}\label{eq6.4}\tag{6.4}
\E\big[X_{n,k}\big]:=\left(\E\left[X_{n,k}^{(j)}\right]\right)_{1\leq j\leq d}=0:=(0,\ldots,0)\qquad\forall k,n\in\N\\
\label{eq6.5}\tag{6.5}
\E\left[\left(X_{n,k}^{(j)}\right)\right]<\infty\qquad\forall 1\leq j\leq d,\forall n,k\in\N
\end{align}
Wegen \eqref{eq6.4} und \eqref{eq6.5} ist die so genannte \textbf{Kovarianzmatrix}
\begin{align*}
\Cov\left(X_{n,k}\right):=\Big(\underbrace{\Cov\left(X_{n,k}^{(i)},X_{n,k}^{(j)}\right)}_{=\E\left(X_{n,k}^{(i)}\cdot X_{n,k}^{(j)}\right)}\Big)_{1\leq i,j\leq d}\in\R^{d\times d}
\end{align*}

\begin{satz}[Multivariater Zentraler Grenzwertsatz (ZGWS)]\label{satz6.2MultivariaterZGWS}\enter
Es gelten \eqref{eq6.3}, \eqref{eq6.4}, \eqref{eq6.5} sowie
\begin{align}\label{eqSatz6.2LB}\tag{LB}
\sum\limits_{k=1}^n\E\left[\Vert X_{n,k}\Vert^2\cdot\indi_{\big\lbrace\Vert X_{n,k}\Vert>\varepsilon\big\rbrace}\right]\stackrel{n\to\infty}{\longrightarrow}0\qquad\forall\varepsilon>0
\end{align}
(Hierbei ist $\Vert\cdot\Vert$ die euklidische Norm auf $\R^d$.)\\
Falls die Normierungsbedingung
\begin{align}\label{eqSatz6.2NB}\tag{NB}
\sum\limits_{k=1}^n\Cov\left(X_{n,k}\right)\stackrelnew{\text{komponentenweise}}{n\to\infty}{\longrightarrow}\Gamma\qquad\mit\Gamma\in\R^{d\times d}\text{ positiv definit}
\end{align}
erfüllt ist, so gilt:
\begin{align*}
\sum\limits_{k=1}^n X_{n,k}\stackrel{\L}{\longrightarrow}\mathcal{N}_d(0,\Gamma)\text{ in }\R^d
\end{align*}
\end{satz}
%Ferger kennt die Dichte die Normalverteilung im Mehrdimensionalen nicht und versucht sie trotzdem aus dem Gedächtnis an die Tafel zu schreiben. Wieder 5 Minuten weg :D
%"Mir persönlich reicht es, das Ding hat eine Dichte!" :D
%"Manchmal ist weniger mehr!"
\begin{proof}
Sei $N\sim\mathcal{N}_d(0,\Gamma)$. Mit \ref{satz5.4CramerWoldDevice} bleibt zu zeigen:
\begin{align}
\label{eqProof6.2Stern}\tag{$\ast$}
\left\langle t,\sum\limits_{k=1}^n X_{n,k}\right\rangle\stackrel{\L}{\longrightarrow}\langle t,N\rangle\text{ in }\R\qquad\forall t\in\R^d
\end{align}
Sei $t\in\R^d\setminus\lbrace0\rbrace$ (für $t=0$ ist \eqref{eqProof6.2Stern} trivialerweise erfüllt). Es folgt
\begin{align*}
\left\langle t,\sum\limits_{k=1}^n X_{n,k}\right\rangle
\overset{\text{Lin}}&=
\sum\limits_{k=1}^n\underbrace{\left\langle t,X_{n,k}\right\rangle}_{=:Y_{n,k}=:Y_{n,k}(t)}
=\sum\limits_{k=1}^n Y_{n,k}\\
Y_{n,k}
&=\sum\limits_{k=1}^d t_j\cdot X_{n,k}^{(j)}
\end{align*}
Folglich erfüllt $\big\lbrace Y_{n,k}:1\leq k\leq n,n\in\N\big\rbrace$ die Bedingung \eqref{eq6.1} wegen Blockungslemma und \eqref{eq6.2} wegen \eqref{eq6.4} und Linearität sowie Cauchy-Schwarz-Ungleichung.
%Hauptsatz des Mannschaftssport: "Jede Kombination von Nullen ist immer Null."
\begin{align*}
\big|Y_{n,k}^2\big|&=\big|\langle t,X_{n,k}\rangle\big|^2
\leq\Vert t\Vert^2\cdot\Vert X_{n,k}\Vert^2
=\Vert t\Vert^2\cdot\sum\limits_{j=1}^d\left(X_{n,k}^{(j)}\right)^2
\end{align*}
%Neu
Da $\langle t,N\rangle=t\cdot N\sim\mathcal{N}(0,t\cdot\Gamma)$ (vgl. Klaus Schmidt \textit{Wahrscheinlichkeit und Maß}, Bsp. 13.2.2), folgt \eqref{eqProof6.2Stern}. Der Nachweis von 
\begin{align}\label{eqProof6.2SternStern}\tag{$\ast\ast$}
\sum\limits_{k=1}^n Y_{n,k}\stackrel{\L}{\longrightarrow}\mathcal{N}(0,t'\cdot\Gamma\cdot t)
\end{align}
erfolgt mit Satz \ref{satz6.1Lindeberg1922}. Es gilt:
\begin{align*}
s_n^2
&=\sum\limits_{k=1}^n\E\left[Y_{n,k}^2\right]\\
&=\sum\limits_{k=1}^n\E\left[\sum\limits_{i,j=1}^d t_i\cdot t_j\cdot X_{n,k}^{(i)}\cdot X_{n,k}^{(j)}\right]\\
&=\sum\limits_{k=1}^n\sum\limits_{i,j=1}^d t_i\cdot t_j\cdot\E\left[X_{n,k}^{(i)}\cdot X_{n,k}^{(j)}\right]\\
&=\sum\limits_{k=1}^n\sum\limits_{i,j=1}^d t_i\cdot\Big(\Cov(X_{n,k}\Big)_{i,j}\cdot t_j\\
&=\sum\limits_{k=1}^n t'\cdot\Cov(X_{n,k})\cdot t\\
\overset{\text{Distri}}&=
t'\cdot\underbrace{\sum\limits_{k=1}^n\Cov(X_{n,k})\cdot t}_{\stackrel{n\to\infty}{\longrightarrow}\Gamma\text{ wg. \eqref{eqSatz6.2NB}}}\stackrel{n\to\infty}{\longrightarrow} t'\cdot \Gamma\cdot t=:\sigma^2\stackrel{\Gamma\text{ p.d.}}{>}0
\end{align*}
Ggemäß Cauchy-Schwarz-Ungleichung (CSU) gilt:
\begin{align}\label{eqProof6.2t}\tag{t}
&|Y_{n,k}|=\big|\langle t,X_{n,k}\rangle\big|\stackeq{\text{CSU}}{\leq}\Vert t\Vert\cdot\Vert X_{n,k}\Vert\\\nonumber
&\implies\sum\limits_{k=1}^n\E\Big[\underbrace{|Y_{n,k}|^2}_{\leq\Vert t\Vert\cdot\Vert X_{n,k}\Vert}\cdot\indi_{\big\lbrace\underbrace{|Y_{n,k}|}_{\leq\Vert t\Vert\cdot\Vert X_{n,k}\Vert}>\varepsilon\big\rbrace}\Big]
\stackrel{\eqref{eqProof6.2t}}{\leq}
\Vert t\Vert^2\cdot\sum\limits_{k=1}^n\E\Big[\Vert X_{n,k}\Vert^2\cdot\indi_{\left\lbrace\Vert X_{n,k}>\frac{\varepsilon}{\Vert t\Vert}\right\rbrace}\Big]\stackrel{n\to\infty}{\longrightarrow}0
\end{align}
Dies gilt gemäß \ref{satz6.1Lindeberg1922} für alle $\varepsilon>0$. Somit folgt aus Satz \ref{satz6.1Lindeberg1922} dann \eqref{eqProof6.2SternStern} und dadurch \eqref{eqProof6.2Stern} und somit die Behauptung.
\end{proof}

\begin{korollar}\label{korollar6.3}
Sei $(X_i)_{i\in\N}$ mit $X_i$ iid Zufallsvariable in $\R^d$ mit
\begin{align*}
&\E\left[\left(X_1^{(j)}\right)^2\right]<\infty\qquad\forall 1\leq j\leq d\\
\mu&:=\E\big[X_1\big]=\left(\E\left(X_1^{(1)}\right),\ldots,\E\left(X_1^{(d)}\right)\right)\in\R^d,\\
\Gamma&:=\Cov(X_1)
=\left(\Cov\left(X_1^{(i)},X_1^{(j)}\right)\right)_{i,j=1}^d
=\left(\E\left[\left(X_1^{(i)}-\mu_i\right)\cdot\left(X_1^{(j)}-\mu_j\right)\right]\right)_{i,j=1}^d\\&\text{ positiv definit}
\end{align*}
Dann gilt:
\begin{align*}
\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^n(X_i-\mu)\stackrel{\L}{\longrightarrow}\mathcal{N}_d(0,\Gamma)
\end{align*}
\end{korollar}
\begin{proof}
\begin{align}\label{eqProof6.3Stern}\tag{$\ast$}
\frac{1}{\sqrt{n}}\cdot\sum\limits_{k=1}^n(X_k-\mu)
&=\sum\limits_{k=1}^n\underbrace{\frac{1}{\sqrt{n}}\cdot(X_k-\mu)}_{=:X_{n,k}}\qquad\forall 1\leq k\leq n,\forall n\in\N
\end{align}
Dann sind \eqref{eq6.3}, \eqref{eq6.4} und \eqref{eq6.5} erfüllt und es gilt
\begin{align*}
\Cov(X_{n,k})
&=\frac{1}{n}\cdot\Gamma\implies\sum\limits_{k=1}^n\Cov(X_{n,k})=\Gamma
\end{align*}
Somit ist \eqref{eqSatz6.2NB} erfüllt. Zu \eqref{eqSatz6.2LB}:
\begin{align*}
\sum\limits\E\Big[\Vert X_{n,k}\Vert^2\cdot\indi_{\big\lbrace\Vert X_{n,k}\Vert>\varepsilon\big\rbrace}\Big]
&\stackeq{\eqref{eqProof6.3Stern}}
\frac{1}{n}\cdot\sum\limits_{k=1}^n\E\Big[\Vert X_k-\mu\Vert^2\cdot\indi_{\big\lbrace\Vert X_k-\mu\Vert>\varepsilon\cdot\sqrt{n}\big\rbrace}\Big]\\
&\stackeq{\text{iid}}
\frac{1}{n}\cdot\sum\limits_{k=1}^n\E\Big[\Vert X_1-\mu\Vert^2\cdot\indi_{\big\lbrace\Vert X_1-\mu\Vert>\varepsilon\cdot\sqrt{n}\big\rbrace}\Big]\\
&=\E\Big[\underbrace{\Vert X_1-\mu\Vert^2\cdot\indi_{\big\lbrace \Vert X_1-\mu\Vert>\varepsilon\cdot\sqrt{n}\big\rbrace}}_{\stackrel{n\to\infty}{\longrightarrow}0~\forall\varepsilon>0}\Big]
\stackrelnew{\text{domKonv}}{n\to\infty}{\longrightarrow}0
\end{align*}
Hierbei geht der Satz der dominierten Konvergenz ein, denn $\Vert X_1-\mu\Vert^2$ ist Dominante und integrierbar, da 
\begin{align*}
\Vert X_1-\mu\Vert^2\leq\big(\Vert X_1\Vert+\Vert\mu\Vert\big)^2\leq 2\cdot\left(\Vert X_1\Vert^2+\Vert\mu\Vert^2\right)
\end{align*}
und 
\begin{align*}
\E\left[\Vert X_1\Vert^2\right]
&=\E\left[\sum\limits_{j=1}^d\left(X_1^{(j)}\right)^2\right]
=\sum\limits_{j=1}^d\underbrace{\E\left[\left(X_1^{(j)}\right)^2\right]}_{<\infty~\forall j}<\infty
\end{align*}
Somit ist \eqref{eqSatz6.2LB} erfüllt und es folgt mit \ref{satz6.2MultivariaterZGWS} die Behauptung.
\end{proof}

Für $d=1$ liefert Korollar \ref{korollar6.3} den klassischen ZGWS.

\section{Verteilungskonvergenz im Raum stetiger Funktionen} %7
Sei $I:=[a,b]$ mit $a<b$ und $C:=C(I):=\big\lbrace f:I\to\R\mid f\text{ stetig }\big\rbrace$ versehen mit der Supremumsmetrik
\begin{align*}
d(f,g)&:=\sup\limits_{t\in I}\big|f(t)-g(t)\big|\qquad\forall f,g\in C.
\end{align*}
Gemäß Beispiel \ref{beisp2.6} ist $(C,d)$ separabel. Die durch $d$ induzierte Borel-$\sigma$-Algebra
%\begin{align*}
$\B(C):=\B_d\big(C(I)\big)$
%\end{align*}
gestattet einfache Beschreibung. Dazu:

\begin{definition}\ %7.1
\begin{enumerate}[label=(\arabic*)]
\item Sei $t\in I$. Die Abbildung 
\begin{align*}
\pi_t:C\to\R,\qquad\pi_t(f):=f(t)\qquad\forall f\in C
\end{align*}
heißt \textbf{Projektion in $t$}.
\item Sei $T:=\big\lbrace t_1,\ldots,t_k\big\rbrace\subseteq I,k\in\N$. Die Abbildung
\begin{align*}
\pi_T:C\to\R^k,\qquad \pi_T(f):=\Big(f(t_1),\ldots,f(t_k)\Big)=:
\Big(f(t)\Big)_{t\in T}\qquad\forall f\in C
\end{align*}
heißt \textbf{Projektion in $T$}.
\end{enumerate}
\end{definition}

\begin{satz}\label{satz7.2}
\begin{align*}
\B(C)=\sigma\Big(\pi_t:t\in I\Big)=\sigma\Big(\pi_T:T\subseteq I,T\text{ endlich}\Big)=\sigma\Big(\big\lbrace \pi_t^{-1}(B):t\in I,B\in\B(\R)\big\rbrace\Big)
\end{align*}
\end{satz}
\begin{proof}
Da $\pi_t=\pi_{\lbrace t\rbrace}$ und $T=\lbrace t\rbrace$, folgt
\begin{align}\label{eqProof7.2(1)}\tag{1}
\sigma\Big(\pi_t:t\in I\Big)
\subseteq\sigma\Big(\pi_T:T\subseteq I\text{ endlich}\Big)
\end{align}
Weiterhin gilt:
\begin{align*}
&\big\Vert\pi_T(f)-\pi_T(g)\big\Vert_\infty
=\max\limits_{1\leq i\leq k}\big|f(t_i)-g(t_i)\big|
\leq d(f,g)\qquad\forall f,g\in C,\forall T=\lbrace t_1,\ldots,t_k\rbrace\subseteq I\\
&\implies\pi_T:(C,d)\to\R^d\text{ ist stetig}
\end{align*}
Aus Satz \ref{Lemma3.2} (2) folgt, dass $B_T:\B(C)\to\B(\R^k)$ messbar ist für alle endlichen Teilmengen $T\subseteq I$. Da $\sigma\big(\pi_T:T\subseteq I\text{ endlich}\big)$ die \ul{kleinste} $\sigma$-Algebra auf $C$ ist, bzgl. der \ul{alle} $\pi_T,T\subseteq I$ endlich, messbar sind, folgt
\begin{align}\label{eqProof7.2(2)}\tag{2}
\sigma\Big(\pi_T:T\subseteq I,T\text{ endlich }\Big)\subseteq\B(C)
\end{align}
Wegen \eqref{eqProof7.2(1)} und \eqref{eqProof7.2(2)} bleibt zu zeigen:
\begin{align}\label{eqProof7.2(3)}\tag{3}
\B(C)\subseteq\sigma\Big(\pi_t:t\in I\Big)
\end{align}
Dazu sei $G\in\G(C)$, d.h. $G\subseteq C$ offen. Gemäß Satz \ref{satz2.9} existieren $f_i\in C$, $\varepsilon_i>0$, $i\in\N$ mit
\begin{align}\label{eqProof7.2(4)}\tag{4}
G=\bigcup\limits_{i\in\N}B\big(f_i,\varepsilon_i\big)\text{ (offene Kugel)}.
\end{align}
Für jede offene Kugel
\begin{align}\label{eqProof7.2(5)}\tag{5}
B(f,\varepsilon)
&=\big\lbrace g\in C:d(g,f)<\varepsilon\big\rbrace
=\bigcup\limits_{k\in\N}\underbrace{\left\lbrace g\in C:d(g,f)\leq\varepsilon-\frac{1}{k}\right\rbrace}_{=\text{ abgeschlossene Kugel}}
\end{align}
Betrachte daher eine beliebige \ul{abgeschlosssene} Kugel
\begin{align*}
\overline{B}(f,\delta)
&=\Big\lbrace g\in C:\sup\limits_{t\in I}\big|g(t)-f(t)\big|\leq\delta\Big\rbrace\\
&=\Big\lbrace g\in C:\sup\limits_{t\in I\cap\Q}\big|g(t)-f(t)\big|\leq\delta\Big\rbrace\\
&=\bigcap\limits_{t\in I\cap\Q}\Big\lbrace g\in C:\big|g(t)-f(t)\big|\leq\delta\Big\rbrace\\
&=\bigcap\limits_{t\in I\cap\Q}\underbrace{\Big\lbrace g\in C:f(t)-\delta\leq \overbrace{g(t)}^{=\pi_t(g)}\leq f(t)+\delta\Big\rbrace}_{\pi_t^{-1}\Big(\big[f(t)-\delta,f(t)+\delta\big]\Big)\in\tilde{\B}}\\
&\implies
\overline{B}(f,\delta)\in\tilde{\B}\qquad\forall f\in C,\forall\delta>0\\
&\stackrel{\eqref{eqProof7.2(5)}}{\implies}
B(f,\varepsilon)\in\tilde{\B}\qquad\forall f\in C,\forall\varepsilon>0\\
&\stackrel{\eqref{eqProof7.2(4)}}{\implies}
G\in\tilde{\B}\\
&\implies
\G(C)\subseteq\tilde{\B}\\
&\implies\sigma\big(\G(C))=\B(C)\subseteq\tilde{\B}
\end{align*}
\end{proof}
%Ferger hat Borel-Mengen am Anfang des Studiums gehasst, weil er sie nicht verstanden hat. Jetzt sieht er das anders.

Ein stochastischer Prozess $\big\lbrace X(t):t\in I\big\rbrace$ heißt \textbf{indiziert nach $I$ (über $(\Omega,\A)$)}
\begin{align*}
:\Longleftrightarrow X(t):(\Omega,\A)\to(\R,\B(\R)),
\qquad\omega\mapsto X(t)(\omega)\quad
\text{ist messbar}\qquad\forall t\in I
\end{align*}
%Ferger: "Trage ich zu einer Verwirrung bei?"
Schreibweise: $X(t,\omega):=X(t)(\omega)$\\
Falls die \textbf{Pfade / Trajektorien}
\begin{align*}
X(\cdot,\omega):I\to\R,\qquad t\mapsto X(t,\omega)\qquad\forall \omega\in\Omega
\end{align*}
stetig sind (für alle $\omega\in\Omega$), so liefert die Zuordnung $\omega\mapsto X(\cdot,\omega)$ eine Abbildung $X:\Omega\to C(I)$. Diese Abbildung $X$ heißt \textbf{Pfadabbildung} des stochastischen Prozesses und wird mit diesem identifiziert.\nl
%TODO Hier kann man eine Skizze einfügen
Satz \ref{satz7.2} liefert ein sehr handliches Kriterium für die Messbarkeit von allgemeinen Abbildungen $X:\Omega\to X$.

\begin{satz}\label{satz7.3}
Sei $(\Omega,\A)$ messbarer Raum und $C:=C(I)$ versehen mit der Borel-$\sigma$-Algebra $\B(C)$ sowie $X:\Omega\to C$. Dann gilt:
\begin{align*}
X\text{ ist }A\text{-}\B(C)\text{-messbar}\Longleftrightarrow\forall t\in I:\pi_t\circ X\text { ist }\A\text{-}\B(R)\text{-messbar}
\end{align*}
\end{satz}
\begin{proof}
Erinnere an das Messbarkeitslemma:\nl
\textbf{Lemma. (Messbarkeitslemma)}\\
Sei $(\Omega,\A)$ messbarer Raum, $\Omega'\neq\emptyset$, $(f_t)_{t\in T}$ eine Familie von Abbildungen\\ $f_t:\Omega'\to(\Omega,\A)$ wobei $(\Omega_t,\A_t)$ messbarer Raum für alle $t\in T$, $T\neq\emptyset$ beliebige Indexmenge. Ferner sei
\begin{align*}
\A':=\sigma\big(f_t:t\in T\big)\stackeq{\text{Def}}\sigma\left(\bigcup\limits_{t\in T} f_t^{-1}(\A_t)\right)
\end{align*}
Dann gilt für eine Abbildung $Z:\Omega\to\Omega'$:
\begin{align*}
Z\text{ ist }\A\text{-}\A'\text{-messbar}\Longleftrightarrow\forall t\in T: f_t\circ Z\text{ ist }\A\text{-}\A_t\text{-messbar}
\end{align*}
Im Falle von Satz \ref{satz7.3}: $\Omega'=C, f_t=\pi_t, T=I,\A'=\sigma\big(\pi_t:T\in I\big)\stackeq{\ref{satz7.2}}\B(C),Z=X$
\end{proof}

Stochastische Prozesse mit stetigen Pfaden nennt man \textbf{stetige} stochastische Prozesse. Satz \ref{satz7.3} besagt nun: Die Pfadabbildung eines jeden stetigen stochastischen Prozesses ist $\A\text{-}\B(C)$-messbar. Insbesondere folgt:\\
Jeder stetige stochastische Prozess indiziert nach $I$ kann aufgefasst werden als Zufallsvariable im metrischen Raum $\big(C(I),d\big)$.

\begin{beispiel}\label{beispiel7.4}
Sei $(\xi_i)_{i\in\N}$ eine Folge von reellen Zufallsvariablen über $(\Omega,\A)$ und für $n\in\N$ sei
\begin{align*}
S_k:=\sum\limits_{i=1}^k\xi_i\qquad\forall0\leq k\leq n\qquad S_0:=0
\end{align*}
Der Polygonzug $X_n$ durch die Punkte $\left(\frac{k}{n},\frac{1}{\sqrt{n}}\cdot S_k\right),0\leq k\leq n$ heißt \textbf{$n$-ter Partialsummenprozess}.
%TODO Hier Skizze einfügen (ist echt eine schöne Skizze)
Mit der Zwei-Punkte-Formel der Geradengleichung folgt:
\begin{align*}
X_n(t)=\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^{\lfloor n\cdot t\rfloor}\xi_i+\frac{1}{\sqrt{n}}\cdot\big(n\cdot t-\lfloor n\cdot t\rfloor\big)\cdot\xi_{\lfloor n\cdot t\rfloor+1}\qquad\forall t\in[0,1]=I
\end{align*}
Hierbei ist die Gaußklammer (Abrundefunktion) wie folgt definiert:
\begin{align*}
\lfloor u\rfloor:=[u]:=\max\big\lbrace l\in\Z:l\leq n\big\rbrace
\end{align*}
$X_n(t)$ ist eine reelle Zufallsvariable für alle $t\in I$. Gemäß Konstruktion (Polygonzug) ist jeder Pfad von $X_n$ stetig auf $[0,1]$. Aus Satz \ref{satz7.3} folgt, dass $X_n~\A\text{-}\B(C)$-messbar ist, also Zufallsvariable in $\Big(C\big([0,1]\big),d\Big)$. Weitere Anwendung von Satz \ref{satz7.2}:
\end{beispiel}

\begin{satz}\label{satz7.5}\
\begin{enumerate}[label=(\arabic*)]
\item $P,Q$ seien Wahrscheinlichkeitsmaße auf $\B(C)$. Dann gilt:
\begin{align*}
P=Q\Longleftrightarrow\forall T\subseteq I\text{ endlich }: P\circ\pi_T^{-1}=Q\circ\pi_T^{-1}
\end{align*}
\item $X,Y$ seien Zufallsvariablen in $\big(C(I),d\big)$. Dann gilt:
\begin{align*}
X\stackeq{\L}Y\Longleftrightarrow \pi_T(X)\stackeq{\L}\pi_T(Y)
\end{align*}
\end{enumerate}
\end{satz}
\begin{proof}
\underline{Zu (1), zeige ``$\implies$'':} Trivial.\nl
\underline{Zu (1), zeige ``$\Longleftarrow$'':}\\
Gemäß Satz \ref{satz7.2} ist
\begin{align*}
\mathcal{E}:=\left\lbrace\pi_T^{-1}(A):A\in\B(\R^{|T|},T\subseteq I\text{ endlich}\right\rbrace
\end{align*}
ein Erzeuger von $\B(C)$, d.h. $\B(C)=\sigma(\mathcal{E})$. Erinnerung:
\begin{align*}
g^{-1}(\mathcal{C})&:=\big\lbrace g^{-1}(C):C\in\mathcal{C}\big\rbrace\text{ für Mengenfamilie }\mathcal{C}\\
\B(C)&\stackeq{\ref{satz7.2}}\sigma\big(\pi_T:T\subseteq I\text{ endlich}\big)
=\sigma\Bigg(\underbrace{\bigcup\limits_{\begin{subarray}{c}T\subseteq I\\T\text{ endlich}\end{subarray}}\pi_T^{-1}\left(\B\left(\R^{|T|}\right)\right)}_{=\mathcal{E}}\Bigg)
\end{align*}
Nach Voraussetzung gilt:
\begin{align*}
\underbrace{P\circ\pi_T^{-1}(A)}_{\stackeq{\text{Def}}P\left(\pi_T^{-1}(A)\right)}&=\underbrace{Q\circ\pi_T^{-1}(A)}_{=Q\left(\pi_T^{-1}(A)\right)}\qquad\forall A\in\B\left(\R^{|T|}\right)\\
\implies
P|_\mathcal{E}&=Q|_\mathcal{E}
\end{align*}
$\mathcal{E}$ ist durchschnittsstabil (nachrechnen!). Jetzt liefert der \textit{Maßeindeutigkeitssatz}, dass $P=Q$ auf $\sigma(\mathcal{E})=\B(C)$.
(Der Maßeindeutigkeitssatz besagt ungefähr: zwei Wahrscheinlichkeitsmaße, die auf einen Schnittstabilen Mengensystem überein stimmen, stimmen auch auf dessen Erzeugnis überein.)\nl
\underline{Zeige (2):}
\begin{align*}
X\stackeq{\L} X
\overset{\text{Def}}&{\Longleftrightarrow}
\P\circ X^{-1}=\P\circ Y^{-1}\\
\overset{(1)}&{\Longleftrightarrow}
\underbrace{\left(\P\circ X^{-1}\right)\circ \pi_T^{-1}}_{=\P\circ\left(\pi_T\circ X\right)^{-1}}
=\underbrace{\left(\P\circ Y^{-1}\right)\circ\pi_T^{-1}}_{=\P\circ\left(\pi_T\circ Y\right)^{-1}}
&\forall T\subseteq T\text{ endlich}\\
&\Longleftrightarrow
\underbrace{\pi_T\circ X}_{=\pi_T(X)}\stackeq{\L}\underbrace{\pi_T\circ Y}_{=\pi_T(Y)}
&\forall T\subseteq I\text{ endlich}
\end{align*}
\end{proof}

\begin{bemerkungnr} %7.6
Die Wahrscheinlichkeitsmaße $\P\circ\pi_T^{-1}$ bzw. die Verteilungen
\begin{align*}
\pi_T(X)=\big(X(t_1),\ldots,X(t_k)\big),\qquad T=\big\lbrace t_1,\ldots,t_k\rbrace\subseteq I
\end{align*}
heißen \textbf{endlich dimensionale Randverteilungen von $\P$} bzw. von $X$.\\
Insbesondere ist gemäß \ref{satz7.5} (2) die Verteilung eines stetigen stochastischen Prozesses $X$ aufgefasst als Zufallsvariable in $C$ eindeutig durch die Verteilungen der Vektoren
\begin{align*}
\big(X(t_1),\ldots,X(t_k)\big),\qquad t_1,\ldots,t_k\in I,k\in\N
\end{align*}
festgelegt.
\end{bemerkungnr}

Nächstes Ziel: Handhabbare Kriterien für den Nachweis der Verteilungskonvergenz in $C$. Dazu:

\begin{definition} %7.z
Für eine Funktion $f\colon I\to\R$ und $\delta>0$ definiere den \textbf{Stetigkeitsmodul / Oszillationsmodul}
\begin{align*}
\omega(f,\delta):=\sup\limits\Big\lbrace \big|f(s)-f(t)\big|:s,t\in I\mit |s-t|\leq\delta\Big\rbrace
\end{align*}
\end{definition}

Aus der Analysis ist bekannt:
\begin{align*}
f\in C(I)\Longleftrightarrow\omega(f,\delta)\stackrel{\delta\to0}{\longrightarrow}0
\end{align*}

\begin{lemma}\label{lemma7.8}
$\omega(\cdot,\delta)\colon (C,d)\to\R$ ist stetig für jedes $\delta>0$ und damit gemäß Lemma \ref{Lemma3.2} (2) auch $\B(C)$-$\B(\R)$-messbar.
\end{lemma}
\begin{proof}
Sei $g\in C$  und $s,t\in I$ mit $|s-t|\leq\delta$. Dann gilt:
\begin{align*}
\big| f(s)-f(t)\big|
&=\big|f(s)-g(s)+g(s)-g(t)+g(t)-f(t)\big|\\
\overset{\text{DU}}&{\leq}
\underbrace{\big|f(s)-g(s)\big|}_{\leq d(f,g)}+\underbrace{\big| g(s)-g(t)\big|}_{\leq\omega(g,\delta)}+\underbrace{\big| g(t)-f(t)\big|}_{\leq d(f,g)}\\
&\leq
\omega(g,\delta)+2\cdot d(f,g)\\
\overset{\sup}&{\implies}
\omega(f,\delta)
\leq\omega(g,\delta)+2\cdot d(f,g)\\
&\implies \omega(f,\delta)-\omega(g,\delta)
\leq 2\cdot d(f,g) &\forall f,g\\
&\implies \omega(g,\delta)-\omega(f,\delta)
\leq 2\cdot d(g,f)=2\cdot d(f,g)\\
&\implies
\big|\omega(f,\delta)-\omega(g,\delta)\big|\leq 2\cdot d(f,g)
\end{align*}
Das heißt, $\omega(\cdot,\delta)$ ist sogar Lipschitz-stetig.
\end{proof}

%Ferger: "Ich war übrigens gestern in der Stadt. Da bin ich eigentlich nie. Ich war in mindestens 7 oder Schuhgeschäften. Es war noch jemand dabei. Ich selber habe kein Schuhe gekauft. Und in jedem Geschäft war die Verweildauer sehr lang. Und jedes Schuhgeshcäft wurde GESCANNT: Jeder Schuh wurde angefasst und begutachtet. [...] Weihnachten ist was Schönes!"

%Ferger: "Wenn man sich rote Schuhe kauft, muss dazu auch noch eine passende rote Tasche kaufen. Wussten Sie das?"

Erstes Kriterium in:

\begin{satz}\label{satz7.9}
Seien $X,X_n,n\in\N$ Zufallsvariablen in $C$ über $(\Omega,\A,\P)$. Falls
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
\big(X_n(t_1),\ldots,X_n(t_k)\big)\stackrel{\L}{\longrightarrow}\big(X(t_1),\ldots,X(t_k)\big)
\qquad\forall t_1,\ldots,t_k\in I,\forall k\in\N
\end{aligned}$\\
so genannte \textbf{Konvergenz der fidis} (finite dimensional distributions).\\
Notation: $X_n\stackrelnew{\text{fd}}{}{\longrightarrow} X$
\item $\begin{aligned}
\lim\limits_{k\to\infty}\limsup\limits_{n\to\infty}\P\Big(\omega\big(X_n,\delta_k\big)>\varepsilon\Big)=0\qquad\forall\varepsilon>0
\end{aligned}$\\
für eine Folge $(\delta_k)_{k\in\N}\subseteq(0,\infty)\mit\delta_k\downarrow0$
\end{enumerate}
so folgt:
\begin{align*}
X_n\stackrel{\L}{\longrightarrow}X\text{ in }(C,d)
\end{align*}
\end{satz}
\begin{proof}
Siehe Gänssler und Stute (1977) \textit{Wahrscheinlichkeitstheorie}, Seite 348
\end{proof}

\begin{beispiel}\label{beispiel7.10} Sei 
\begin{align*}
X_n(t):=A_n+B_n\cdot t+C_n\cdot t^2\qquad\forall t\in I
\qquad\mit \big(A_n,B_n,C_n)\stackrel{\L}{\longrightarrow}(A,B,C)\in\R^3
\end{align*}
Dann gilt:
\begin{align*}
X_n\stackrel{n\to\infty}{\longrightarrow} X\text{ in }(C,d)\qquad\mit\qquad X(t)=A+B\cdot t+C\cdot t^2
\end{align*}
\begin{proof}
Seien $t_1,\ldots,t_k\in I$ beliebig. Für Voraussetzung (1) in Satz \ref{satz7.9} reicht es gemäß Cramér-Wold-Device \ref{satz5.4CramerWoldDevice} zu zeigen:
\begin{align*}
\sum\limits_{j=1}^k\lambda_j\cdot X_n(t_j)\stackrel{\L}{\longrightarrow}\sum\limits_{j=1}^k\lambda_j\cdot X(t_j)\text{ in }\R\qquad\forall\lambda=\big(\lambda_1,\ldots,\lambda_k\big)\in\R^k
\end{align*}
%Ich wurde von Prof Ferger bemitleidet, weil ich in Tex nicht alles umsetzen kann, was in der Tafel durch Wisch-Technik erzeugt.
Dazu: 
\begin{align*}
\sum\limits_{j=1}^k\lambda_j\cdot X_n(t_j)
&=A_n\cdot\sum\limits_{j=1}^n\lambda_j+B_n\cdot\sum\limits_{j=1}^k\lambda_j\cdot t_j+C_n\cdot\sum\limits_{j=1}^k\lambda_j\cdot t_j^2\\
\overset{\L,\ref{satz4.10ContinuousMappingTheorem}}&{\longrightarrow}
A\cdot\sum\limits_{j=1}^k\lambda_j+B\cdot\sum\limits_{j=1}^k\lambda_j\cdot t_j+C\cdot\sum\limits_{j=1}^k\lambda_j\cdot t_j^2
=\sum\limits_{j=1}^k\lambda_j\cdot X(t_j)
\end{align*}
Zu Voraussetzung (2) in Satz \ref{satz7.9}:
\begin{align*}
\big|X_n(s)-X_n(t)\big|
&=\Big|B_n\cdot(s-t)+C_n\cdot\underbrace{\big(s^2-t^2\big)}_{(s-t)\cdot(s+t)}\Big|\\
\overset{\text{DU}}&{\leq}
|B_n|\cdot\underbrace{|s-t|}_{\leq\delta}+|C_n|\cdot\underbrace{|s-t|}_{\leq\delta}\cdot\underbrace{|s+t|}_{\leq|s|+|t|\leq:K}\\
&\leq
|B_n|\cdot\delta+K\cdot|C_n|\cdot\delta\qquad\forall s,t\in I\mit |s-t|\leq\delta\\
\overset{\sup}{\implies}
\omega(X_n,\delta)
&\leq|B_n|\cdot\delta+K\cdot|C_n|\cdot\delta\\
\implies
\P\Big(\omega\big(X_n,\delta\big)>\varepsilon\Big)
&\leq\P\Big(|B_n|\cdot\delta+K\cdot|C_n|\cdot\delta>\varepsilon\Big)\\
\overset{\eqref{eqProofBeispiel7.10}}&{\leq}
\P\Big(|B_n|\cdot\delta>\frac{\varepsilon}{2}\Big)+\P\Big(K\cdot|C_n|\cdot\delta>\frac{\varepsilon}{2}\Big)\\
&=
\P\Big(|B_n|>\frac{\varepsilon}{2\cdot\delta}\Big)+\P\Big(|C_n|>\frac{\varepsilon}{2\cdot K\cdot\delta}\Big)\\
&=
1-F_n\left(\frac{\varepsilon}{2\cdot\delta}\right)+1-G_n\left(\frac{\varepsilon}{2\cdot K\cdot\delta}\right)
\end{align*}
%Ferger: "Warum habe ich das eigentlich so gemacht!?"
Hierbei ist $F_n$ die Verteilungsfunktion von $|B_n|\stackrel{\L}{\longrightarrow}|B|$ und $G_n$ die Verteilungsfunktion von $|C_n|$.
Erinnerung:
\begin{align}\label{eqProofBeispiel7.10}
\big\lbrace U+V>\varepsilon\big\rbrace\subseteq\left\lbrace U>\frac{\varepsilon}{2}\right\rbrace\cup\left\lbrace V>\frac{\varepsilon}{2}\right\rbrace
\end{align}
Mit Korollar \ref{korollar4.5} folgt, dass es eine Fologe $(\delta_k)_{k\in\N}$  mit $\delta_k\downarrow0$, so dass $\frac{\varepsilon}{2\cdot\delta_k}$ und $\frac{\varepsilon}{2\cdot K\cdot\delta_k}$ Stetigkeitsstellen der jeweiligen Grenz-Verteilungfunktionen sind,
%Ferger: "Na gut, einen Nobelpreis für Literatur kriege ich jetzt nicht."
\begin{align*}
\limsup\limits_{n\to\infty}\P\Big(\omega\big(X_n,\delta_k\big)>\varepsilon\Big)\leq\P\Big(|B|>\underbrace{\frac{\varepsilon}{2\cdot\delta_k}}_{\stackrel{k\to\infty}{\longrightarrow}\infty}\Big)
+\P\Big(|C|>\underbrace{\frac{\varepsilon}{2\cdot K\cdot\delta_k}}_{\stackrel{k\to\infty}{\longrightarrow}\infty}\Big)
\qquad\forall k\in\N
\end{align*}
%Ferger: "Hach ich bin so doof. Ich hätte mir das Leben leichter machen können."
$k\to\infty$ liefert dann (2).\\
\textbf{Aufgabe:} Zeige dieses Beispiel durch CMT \ref{satz4.10ContinuousMappingTheorem}.
\end{proof}
\end{beispiel}
 
Zweite Möglichkeit für den Nachweis der Verteilungskonvergenz in $C$ liefert:

\begin{satz}[Momentenkriterium von Kolmogoroff]\label{satz7.11MomentenkriteriumVonKolmogoroff}\enter
Seien $X,X_n,n\in\N$ Zufallsvariablen in $C$ mit
\begin{align}\label{eqSatz7.11Vor1}\tag{1}
X_n\stackrelnew{\text{fd}}{}{\longrightarrow} X\qquad\text{(vgl. (1) in \ref{satz7.9})}
\end{align}
Falls es eine Konstante $\gamma>0$ und $\alpha>1$ sowie eine stetige und monoton wachsende Funktion $F:I\to\R$ gibt, derart, dass
\begin{align}\label{eqSatz7.11VorM}\tag{M}
\E\Big[|X_n(s)-X_n(t)|^\gamma\Big]\leq\big(F(s)-F(t)\big)^\alpha\qquad\forall s>t,s,t\in I
\end{align}
Dann gilt:
\begin{align*}
X_n\stackrel{\L}{\longrightarrow} X\text{ in }\big(C(I),d\big)
\end{align*}
\end{satz}

\begin{proof}
Siehe Billingsley (1968), \textit{Convergence of probability measures}, Seite 96.
%Ferger: "Das Buch hier war früher meine Fibel."
%Ferger: "Jim Morrison von den Doors. Das war mein Held. Neben Skorokott."
\end{proof}

\ul{Ziel:} Verteilungskonvergenz der Partialsummenprozesse $X_n$ aus dem Beispiel \ref{beispiel7.4}. Dazu:

\begin{definition}\label{def7.12} %7.12
Sei $I=[0,b]\mit b>0$ und $B:=\big\lbrace B(t):=B(t,\omega)\in I\big\rbrace$ ein stetiger stochastischer Prozess über $(\Omega,\A,\P)$ mit 
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
B(0)=B(0,\omega)=0\qquad\forall\omega\in\Omega
\end{aligned}$
\item $\begin{aligned}
\forall 0=:t_0\leq t_1<\ldots<t_r\leq b
\end{aligned}$ sind die Zuwächse $B(t_i)-B(t_{i-1}),1\leq i\leq r$\\ \ul{unabhängig}
\item $\begin{aligned}
0\leq s<t\leq b\implies B(t)-B(s)\sim\mathcal{N}(0,t-s)
\end{aligned}$
\end{enumerate}
Dann heißt $B$ \textbf{Brownsche Bewegung (BB)} auf $I$.\\
Vollkommen analog definiert man eine BB auf $I=[0,\infty)$.
\end{definition}

%Ferger: "Das ist wie so ein Wunschzettel. Es gelte 1, 2 und 3. Passend zur Weihnachtszeit. Wenn's dumm läuft, kann es noch sein, dass mein Wunsch nicht erfüllt wird, weil er nicht erfüllt werden kann."
%Ferger: "In der Hoffnung das Sie nicht schonmal hier gesessen haben: Weil ich immer dieselben Geschichten erzähle. Ich bin jetzt in so einem Alter wo man alles mehrfach erzählt."
%Ferger: "Da hat mal jemand eine Doktorarbeit geschrieben über eine tolle Fukntionenklasse. Und dann kam jemand daher, dass die Funktionenklasse nur aus der Eins-Funktion besteht. Das war echt ein Griff ins Klo. Wäre noch heftiger gewesen, wenn die Funktionenklasse leer gewesen wäre. Aber: So sind wir ja auch alle gestrickt. So werden wir konditionert. Weil, wenn der Mathematiker irgendwas hört, fragt der Mathematiker ständig "Existiert das?""

\begin{satz}[Lévy]\label{satz7.13Levy}
Eine BB existiert.
\end{satz}
\begin{proof}
Siehe Vorlesung \textit{Stochastische Prozesse}.
%Ferger: "Es gibt einen konstruktiven Beweis, der ist auch lehrrreich, aber da haben wir jetzt keine Zeit für.
%Anmerkung des Autors: Geschichten erzhählen ist wohl wichtiger
\end{proof}

\begin{lemma}\label{lemma7.14}
Sei $B$ eine BB auf $I$ und $t_1<\ldots<t_r$ aus $I$. Dann gilt:
\begin{align*}
\Big(B(t_1),\ldots, B(t_r)\Big)^T\sim\mathcal{N}_r(0,\Gamma)\text{ wobei }\\
\Gamma:=\Big(\Cov\big(B(t_i),B(t_j)\big)\Big)_{1\leq i,j\leq r}=\big(t_i\wedge t_j\big)_{1\leq i,j,\leq r}
\end{align*}
% \wedge meint Minimum
\end{lemma}

\begin{proof}
Sei $t_0:=0$. Dann gilt:
\begin{align*}
B(t_j)
\overset{\text{\ref{def7.12}(1)}}=
B(t_j)-\underbrace{B(t_0)}_{=0}
\overset{\text{Teles}}{=}
\sum\limits_{i=1}^j\big(B(t_i)-B(t_{i-1})\big)
=\sum\limits_{i=1}^j\sqrt{t_i-t_{i-1}}\cdot\underbrace{\frac{B(t_i)-B(t_{i-1})}{\sqrt{t_i-t_{i-1}}}}_{=:Z_i}
\end{align*}
Da $B(t_i)-B(t_{i-1})\sim\mathcal{N}(0,t_i-t_{i-1})$ folgt, dass die $Z_i$ i.i.d. $\sim\mathcal{N}(0,1)$ sind.
Also ist der Vektor
\begin{align*}
\begin{pmatrix}
B(t_1)\\
\vdots\\
B(t_r)
\end{pmatrix}&=\begin{pmatrix}
\sqrt{t_1} & 0 & \hdots & \hdots & 0\\
\sqrt{t_1} & \sqrt{t_2-t_1} & 0 & \hdots & 0\\
\vdots & \vdots & \ddots & \ddots & \vdots\\
\sqrt{t_1} & \sqrt{t_2-t_1} & \sqrt{t_3-t_2} & \hdots & 0\\
\end{pmatrix}\cdot\begin{pmatrix}
Z_1\\
\vdots\\
Z_r
\end{pmatrix}\implies\begin{pmatrix}
B(t_1)\\
\vdots\\
B(t_r)
\end{pmatrix}\sim\mathcal{N}_r(\mu,\Gamma)\\
\mu_i:&=\E\Big[B(T_i)\Big]=\E\Big[\underbrace{B(t_i)-B(t_0)}_{\sim\mathcal{N}(0,t_i-t_0)}\Big]\qquad\forall i\\
\implies\mu_i :&=\big(\mu_1,\ldots,\mu_r\big)=0
\end{align*}
Ferner sei $i\leq j$. Dann gilt:
\begin{align*}
\Cov\big(B(t_i),B(t_j)\big)
&=\E\Big[B(t_i)\cdot B(t_j)\Big]\\
&=\E\Big[B(t_i)\cdot\big( B(t_j)-B(t_i)+B(t_i)\big)\Big]\\
&=\E\Big[\underbrace{B(t_i)\cdot\big(B(t_j)-B(t_i)\big)}_{\text{beide Faktoren sind unabh.}}+\big(B(t_i)\big)^2\Big]\\
&=\E\Big[B(t_i)\cdot\big(B(t_j)-B(t_i)\big)\Big]+\E\Big[\big(B(t_i)\big)^2\Big]\\
\overset{\text{unab}}&=
\underbrace{\E\big[B(t_i)\big]}_{=}\cdot\underbrace{\E\big[B(t_j)-B(t_i)\big]}_{=0}+\underbrace{\E\Big[\big(B(t_i)\big)^2\Big]}_{=\Var\big(B(t_i)\big)}\\
\overset{\text{Vert}}&=
\Var\big(\mathcal{N}(0,t_i)\big)\\
&=t_i\\
\overset{i\leq j}&{=}
t_i\wedge t_j
\end{align*}
Analog behandle den Fall $i\geq j$.
\end{proof}

\begin{korollar}\label{korollar7.15Folgerung}
Die Verteilung einer BB ist eindeutig bestimmt. 
\end{korollar}
\begin{proof}
Seien $B$ und $\tilde{B}$ zwei BBs (i.d.R. über unterschiedlichen Wahrscheinlichkeitsräumen definiert). Dann gilt:
\begin{align*}
B\stackeq{\L}\tilde{B}
\overset{\ref{satz7.5}}&{\Longleftrightarrow}
\big(B(t_1),\ldots,B(t_r)\big)^T\stackeq{\L}\big(\tilde{B}(t_1),\ldots,\tilde{B}(t_r)\big)^T
&\forall t_1<\ldots<t_r\in I,\forall r\in\N\\
\overset{\ref{lemma7.14}}&\Longleftrightarrow
\mathcal{N}_r(0,\Gamma)=\mathcal{N}_r(0,\Gamma)
&\forall t_1<\ldots<t_r\in I,\forall r\in\N
\end{align*}
Die letzte Aussage gilt aber, weil jede mehrdimensionale Normalverteilung $\mathcal{N}_r(\mu,\Gamma)$ eindeutig durch $\mu$ und $\Gamma$ festgelegt ist:
\begin{align*}
\mathcal{N}(\mu,\sigma^2)=\mathcal{N}(m,s^2)\Longleftrightarrow m=\mu\text{ und }s^2=\sigma^2
\end{align*}
\end{proof}

Die Verteilung $\P\circ B^{-1}=:W$ von  einer Brownschen Bewegung $B$ heißt \textbf{Wiener-Maß} (nach Norbert Wiener).
%Ferger: "Absoluter Schlaukopf. Ein Ammi. Ein US-Amerikaner. Der hat 21 promoviert. Da wo wir noch mit den Klötzchen spielen, hat der schon promoviert."
\begin{align*}
B:(\Omega,\A,\P)\to\Big(C(I),\B_d\big(C(I)\big)\Big)
\end{align*}
%Ferger: "Die Brownsche Bewegung wird genannt Brownsche Bewegeung, weil folgendes passiert ist: es gab einen schottischen Bonatiker. Der hat also eine Flüssigkeit genommen, z. B. Wasser und dann ganz kleine Pollenkörner oder Staubkörnchen, also so was ganz klitzekleines oder eie kleine Minischuppe in sein Töpfchen getan, in seine Flüssigkeit und hat draufgeguckt und gesagt "Boah, da ist ja was los." [...] Ja ich erzähl das so wie bei der Sendung mit der Maus, weil man das so gut versteht. [...] Und er stellt fest, dass da viel los ist. Die Beobachtung hatten andere Leute wohl auch schon gemacht. Die ersten Leute, jeder von denen hat sich natürlich gefragt: Was steckt dahinter? Die ersten Leute haben als erklärungsversuch gegeben, dass es biologische Energie in sich hat. Der Robert Braun hat gesagt: Nein, das ist nicht so. Können Sie ja mal googeln. Entscheidend ist Folgendes: Das war ca. 182?. Dann kam Albert Einstein und hat gesagt: Nenene bzw. jajaja, was hier passiert ist Folgendes: So eine Flüssigkeit besteht ja aus ganz vielen Molekülen, die in Bewegung sind. Und die schießen wie wild hin und her. Die Moleküle sind aber klitzeklitzeklein. Das kleine Teilchen ist aber im mirkoskopischen Bereich, also im Vergleich zu den Molekülen ein Riesen-Oschi. Und die kleinen Teilchen hauen die ganze Zeit dagegen. Der Einstein ist hergegangen und hat das für Physiker-Verhältnisse sehr mathematisch beschrieben. Der Wiener hat das nochmal auf saubere, mathematische Füße gestellt (Funtionenraum, Verteilungen und solche Sachen...) Die haben das ein oder andere wohl handwaving gemacht und Norbert Wiener hat das dann mal mathematisch sauberr gemacht. Deshalb heißt die Verteilung einer Brownschen auch Wiener Maß.
%Ferger: Sind ja immernoch 7 Minuten. Mist. Na da kann ich ja noch eine Geschichte erzählen. 

\begin{satz}[Donsker]\label{satz7.16Donsker}%\enter
Seien $(\xi)_{i\in\N}$ i.i.d. mit $\E[\xi_1]=0$ und $\Var(\xi_1)=1$.
\begin{align*}
S_k&:=\sum\limits_{i=1}^k\xi_i&\forall& k\in\N_0\\
X_n(t)&:=\frac{1}{\sqrt{n}}\cdot S_{\lceil n\cdot t\rceil}+\frac{1}{\sqrt{n}}\cdot\big(n\cdot t-\lceil n\cdot t\rceil\big)\cdot\xi_{\lceil n\cdot t\rceil+1} &\forall& t\in I=[0,b]
\end{align*}
($b>0$ fest), d.h. $X_n$ ist der Polygonzug durch die Punkte $\left(\frac{k}{n},\frac{1}{\sqrt{n}}\cdot S_k\right)_{0\leq k\leq b\cdot n}$. Dann gilt:
\begin{align*}
X_n\stackrel{\L}{\longrightarrow} B\text{ in }\big(C(I),d\big)
\end{align*}
wobei $B$ eine BB auf $I=[0,b]$.
\end{satz}

\begin{proof}
Anwendung von Satz \ref{satz7.11MomentenkriteriumVonKolmogoroff}. Zeige also dessen Voraussetzungen:\nl
\underline{\eqref{eqSatz7.11Vor1} Zeige Konvergenz der fidis:}\\
Seien $0\leq t_1<\ldots<t_r\leq b$. Dann gilt:
\begin{align*}
\left\Vert\big(X_n(t_i)\big)_{1\leq i\leq r}-\left(\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=1}^{\lceil n\cdot t_i\rceil}\xi_j\right)_{1\leq i\leq r}\right\Vert
&=\Bigg(\sum\limits_{i=1}^r\underbrace{\left|X_n(t_i)-\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=1}^{\lceil n\cdot t_i\rceil}\xi_j\right|^2}_{\frac{1}{\sqrt{n}}\cdot\underbrace{\big|n\cdot t_i-\lceil n\cdot t_i\rceil\big|}_{\leq1}\cdot\xi_{\lceil n\cdot t_i\rceil}}\Bigg)^{\frac{1}{2}}\\
&\leq
\frac{1}{\sqrt{n}}\cdot\left(\sum\limits_{i=1}^r\xi_{\lceil n\cdot t_i\rceil+1}^2\right)^{\frac{1}{2}}
\implies\\
\P\left(\left\Vert\big(X_n(t_i)\big)_{1\leq i\leq r}-\left(\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=1}^{\lceil n\cdot t_i\rceil}\xi_j\right)_{1\leq i\leq r}\right\Vert\right)
&\leq\P\left(\sum\limits_{i=1}^r\xi^2_{\lceil n\cdot t_i\rceil+1}>n\cdot\varepsilon^2\right)\\
\overset{\text{Markov}}&\leq
\frac{1}{n}\cdot\varepsilon^{-2}\cdot\sum\limits_{i=1}^r\underbrace{\E\left[\xi^2_{\lceil n\cdot t_i\rceil+1}\right]}_{=1~\forall i}\\
&=
\frac{1}{n}\cdot\varepsilon^{-2}\cdot r\stackrel{n\to\infty}{\longrightarrow}0\qquad\forall\varepsilon>0
\end{align*}
Folglich sind die beiden Folgen
\begin{align*}
\big(X_n(t_i)\big)_{1\leq i\leq r}\qquad\text{und}\qquad\left(\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=1}^{\lceil n\cdot t_i\rceil}\xi_j\right)_{1\leq i\leq r}
\end{align*}
stochastisch äquivalent. Wegen Cramér (Satz \ref{satz4.14Cramer} genügt reicht es
\begin{align}\label{eqProof7.16fd}\tag{fd}
\left(\frac{1}{\sqrt{n}}\cdot S_{\lceil n\cdot t_i\rceil}\right)_{1\leq i\leq r}\stackrel{\L}{\longrightarrow}\big(B(t_1),\ldots,B(t_r)\big)
\end{align}
zu zeigen. Aber \eqref{eqProof7.16fd} ist äquivalent zu 
\begin{align}\label{eqProof7.16Stern}\tag{$\ast$}
\frac{1}{\sqrt{n}}\cdot\left(S_{\lceil n\cdot t_i\rceil}-S_{\lceil n\cdot t_{i-1}\rceil}\right)_{1\leq i\leq r}\stackrel{\L}{\longrightarrow}\big(B(t_i)-B(t_{i-1})\big)_{1\leq i\leq r}
\end{align}
denn:
\begin{align*}
h:\R^r\to\R^r,\qquad h(x_1,\ldots,x_r):=\big(x_1,x_2-x_1,x_3-x_2,\ldots,x_r-x_{r-1}\big)
\end{align*}
ist stetig auf $\R^r$ und hat stetige Inverse $h^{-1}$ mit 
\begin{align*}
h^{-1}(y_1,\ldots,y_r)=\big(y_1,y_1+y_2,\ldots,y_1+\ldots+y_r\big)
\end{align*}
Also ist \eqref{eqProof7.16fd} äquivalent zu \eqref{eqProof7.16Stern} gemäß CTM \ref{satz4.10ContinuousMappingTheorem}.\\
Beachte wegen Blockungslemma sind die Komponenten $\frac{1}{\sqrt{n}}\cdot\left(S_{\lceil n\cdot t_i\rceil}-S_{\lceil n\cdot t_{i-1}\rceil}\right)_{1\leq i\leq r}$ \underline{unabhängig} und gemäß \ref{def7.12} (2) sind auch $\big(B(t_i)-B(t_{i-1})\big)$, $a\leq i\leq r$ unabhängig. Somit ist gemäß Satz \ref{satz4.21} \eqref{eqProof7.16Stern} äquivalent zu 
\begin{align}\label{eqProof7.16SternStern}\tag{$\ast\ast$}
\frac{1}{\sqrt{n}}\cdot\left(S_{\lceil n\cdot t_i\rceil}-S_{\lceil n\cdot t_{i-1}\rceil}\right)
\stackrel{\L}{\longrightarrow}
\big(B(t_i)-B(t_{i-1})\big)\text{ in }\R\qquad\forall 1\leq i\leq r
\end{align}
Dazu setze $k_n:=\lceil n\cdot t_i\rceil-\lceil n\cdot t_{i-1}\rceil$. Dann gilt:
\begin{align*}
&\frac{1}{\sqrt{n}}\cdot\left(S_{\lceil n\cdot t_i\rceil}-S_{\lceil n\cdot t_{i-1}\rceil}\right)\\
&=\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=\lceil n\cdot t_{i-1}\rceil+1}^{\lceil n\cdot t_i\rceil}\xi_j
=\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=1}^{k_n}\xi_{\lceil n\cdot t_{i-1}\rceil+j}
\overset{\L}{=}
\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=1}^{k_n}\xi_j\\
&=\underbrace{\sqrt{\frac{k_n}{n}}}_{=\sqrt{\frac{\lceil n\cdot t_i\rceil-\lceil n\cdot t_{i-1}\rceil}{n}}}\cdot\frac{1}{\sqrt{k_n}}\cdot\sum\limits_{j=1}^{k_n}\xi_j\\
&=\underbrace{\sqrt{\frac{\lceil n\cdot t_i\rceil-\lceil n\cdot t_{i-1}\rceil}{n}}}_{\stackrel{n\to\infty}{\longrightarrow}\sqrt{t_i-t_{i-1}}}\cdot\underbrace{\frac{1}{\sqrt{k_n}}\cdot\sum\limits_{j=1}^{k_n}\xi_j}_{\stackrelnew{\text{ZGWS}}{n\to\infty}{\longrightarrow}\mathcal{N}(0,1)}\stackrelnew{\ref{beisp4.18}(3)}{\L}{\longrightarrow}\sqrt{t_i-t_{i-1}}\cdot\mathcal{N}(0,1)
\overset{\L}{=}\underbrace{\mathcal{N}(0,t_i-t_{i-1})}_{
\overset{\ref{def7.12}}{=}B(t_i)-B(t_{i-1})}
\end{align*}
Damit ist Voraussetzung \eqref{eqSatz7.11Vor1} aus \ref{lemma7.14} gezeigt.\nl
Wir zeigen \eqref{eqSatz7.11VorM} für $\gamma=4$ und $\alpha=2$, falls $\mu_4:=\E\big[\xi_1^4\big]<\infty$ (das ist eine stärkere Voraussetzung)\\
Seien $s>t$ aus $I=[0,b]$. Dann gilt:
\begin{align}\label{eqProof7.16Plus}\tag{+}
&X_n(s)-X_n(t)\\
&=\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=\lceil n\cdot t\rceil+1}^{\lceil n\cdot s\rceil}\xi_j+\frac{1}{\sqrt{n}}\cdot\big(n\cdot s-\lceil n\cdot s\rceil\big)\cdot\xi_{\lceil n\cdot s\rceil+1}
-\frac{1}{\sqrt{n}}\cdot\big(n\cdot t-\lceil n\cdot t\rceil\big)\cdot\xi_{\lceil n\cdot t\rceil+1}\nonumber
\end{align}
Da 
\begin{align}\label{eqProof7.16PlusPlus}\tag{++}
\lceil n\cdot t\rceil\leq n\cdot t\leq\lceil n\cdot t\rceil+1\qquad\forall t\geq0
\end{align}
folgt
\begin{align*}
\frac{k}{n}\leq t<\frac{k+1}{n},\qquad\frac{l}{n}\leq s<\frac{l+1}{n},\qquad k=\lceil n\cdot t\rceil,\qquad l=\lceil n\cdot s\rceil
\end{align*}
\underline{Fall 1: $s-t\leq\frac{1}{n}$}
\begin{enumerate}[label=(\roman*)]
\item $s$ und $t$ liegen im selben Intervall $\left[\frac{k}{n},\frac{k+1}{n}\right)$
\item $s$ und $t$ liegen in zwei aufeinanderfolgenden Intervallen $t\in\left[\frac{k}{n},\frac{k\cdot n}{n}\right]$, $s\in\left[\frac{k+1}{n},\frac{k+2}{n}\right)$
\end{enumerate}
%Ferger: Habe ich schon erwähnt, dass ich in Deutsch richtig schlecht war?
\ul{Fall (i)}: $\lceil n\cdot s\rceil=\lceil n\cdot t\rceil$ und
\begin{align*}
X_n(s)-X_n(t)
&=\frac{1}{\sqrt{n}}\cdot(n\cdot s-n\cdot t)\cdot\xi_{\lceil n\cdot t\rceil+1}
=\sqrt{n}\cdot(s-t)\cdot\xi_{\lceil n\cdot t\rceil+1}\\
\implies
\E\Big[\big|X_n(s)-X_n(t)\big|^4\Big]
&=n^2\cdot(s-t)^4\cdot\mu_4
=\mu_4\cdot(s-t)^2\cdot\underbrace{(s-t)^2}_{<\frac{1}{n^2}}\cdot n^2\\
&\leq\mu_4\cdot(s-t)^2
\end{align*}
%Ferger: Sie haben vielleicht mitbekommen es wird momentan von der Digitalisierung geredet. Und der Bund will 5 mrd € zur Verfügung stellen. Aber die Lehrer sagen: "Wir müssen die Kinder in das Zeitalter der Digitalisierung bringen". Gestern, ich lag gerade so auf meiner Couch mit einem Glas Wein und ein Journalist sagte dann "Man darf nicht zurück in die Kreidezeit (Lehrer an der Tafel)" Ich bin so kurz zusammengezuckt und habe mir gedacht: "Ach du scheiße, was machst du denn in Dresden..." Also es gibt tatsächlich Leite, die sich auch damit beschäftigen und sagen das ist Mühsam mit der Tafel. Aber indem man schreibt, beschäftigt man sich schon intensiver mit Stoff. 
%Ferger: Manchmal ist weniger mehr.
\ul{Fall (ii):} $k=\lceil n\cdot t\rceil$ und $l=k+1=\lceil n\cdot s\rceil$. Dann folgt aus \eqref{eqProof7.16Plus}:
\begin{align*}
X_n(s)-X_n(t)
&=\frac{1}{\sqrt{n}}\cdot\xi_{k+1}+\frac{1}{\sqrt{n}}\cdot\big(n\cdot s-(k+1)\big)\cdot\xi_{k+2}-\frac{1}{\sqrt{n}}\cdot(n\cdot t-k)\cdot\xi_{k+1}\\
&=\sqrt{n}\cdot\left(s-\frac{k+1}{n}\right)\cdot\xi_{k+2}+\sqrt{n}\cdot\xi_{k+1}\cdot\left(\frac{k+1}{n}-t\right)
\end{align*}

\begin{lem}[$c_r$-Ungleichung]\enter
Seien $a_1,\ldots,a_m\in\R$ (paarweise verschieden) und $r\geq 1$. Dann gilt:
\begin{align}\label{eqCrUngleichung}\tag{$C_r$}
\left|\sum\limits_{i=1}^m a_i\right|^r\leq c_r\cdot\sum\limits_{i=1}^m|a_i|^r\mit c_r:= m^{r-1}
\end{align}
\end{lem}

\begin{proof}
Sei $Z$ diskrete Zufallsvariable mit $\P(Z=a_i)=\frac{1}{m}$ für $1\leq i\leq m$. Dann gilt:
\begin{align*}
\big|\underbrace{\E[Z]}_{=\frac{1}{n}\cdot\sum\limits_{i=1}^m a_i}\big|^r
\overset{\text{Jensen}}&\leq
\underbrace{\E\Big[|Z|^r\Big]}_{=\frac{1}{m}\cdot\sum\limits_{i=1}^m|a_i|^r}
\implies
m^{-r}\cdot\left|\sum\limits_{i=1}^m a_i\right|^r\leq\frac{1}{m}\cdot\sum\limits_{i=1}^m|a_i|^r
\end{align*}
\end{proof}

Mit $m=2$ und $r=4$ folgt:
\begin{align*}
\big|X_n(s)-X_n(t)\big|^4
&\leq 8\cdot\bigg(n^2\cdot\xi^4_{k+2}\cdot\Big(\underbrace{s-\frac{k+1}{n}}_{\leq s-t}\Big)^4+n^2\cdot\xi^4_{k+1}\cdot\Big(\underbrace{\frac{k+1}{n}-t}_{\leq s-t}\Big)^4\bigg)\\
\implies
\E\left[\big|X_n(s)-X_n(t)\big|^4\right]
&\leq 16\cdot\mu_4\cdot \underbrace{n^2\cdot(s-t)^2}_{\overset{\text{Fall 1}}{\leq} 1}\cdot(s-t)^2
\leq
16\cdot\mu_4\cdot(s-t)^2
\end{align*}

\underline{Fall 2: $s-t\geq\frac{1}{n}$}\\
Aus \eqref{eqProof7.16Plus} und \eqref{eqCrUngleichung} mit $r=4$ und $m=3$ folgt wegen $\big|\lceil n\cdot s-\lceil n\cdot s\rceil\big|\leq1$:
\begin{align*}
\E\Big[\big|X_n(s)-X_n(t)\big|^4\Big]
\overset{}&{\leq}
27\cdot\left(\frac{1}{n^2}\cdot\E\left[\left|\sum\limits_{i=\lceil n\cdot t\rceil+1}^{\lceil n\cdot s\rceil}\xi_i\right|^4\right]+\underbrace{\frac{1}{n^2}}_{\leq(s-t)^2}\cdot\mu_4+\underbrace{\frac{1}{n^2}}_{\leq(s-t)^2}\cdot\mu_4\right)
\end{align*}

\begin{lem}[Momenten-Ungleichung]\enter
Seien $\xi_1,\ldots,\xi_n$ i.i.d., zentriert mit $\mu_4:=\E[\xi_1^4]<\infty$ und $\mu_2:=\E[\xi_1^1]$. Dann gilt:
\begin{align*}
\E\left[\left|\sum\limits_{i=1}^n\xi_i\right|^4\right]=n\cdot\mu_4+3\cdot n\cdot(n-1)\cdot\mu_2^2
\leq 4\cdot\mu_4\cdot n^2
\end{align*}
\end{lem}

\begin{proof}
Zeige zuerst das Gleichheitszeichen:
\begin{align*}
\E\left[\left|\sum\limits_{i=1}^n\xi_i\right|^4\right]
&=\E\left[\left(\sum\limits_{i=1}^n\xi_i\right)^4\right]\\
&=\E\left[\sum\limits_{1\leq i,j,k,l\leq n}\xi_i\cdot\xi_j\cdot\xi_k\cdot\xi_l\right]\\
&=\sum\limits_{1\leq i,j,k,l\leq n}\underbrace{\E\Big[\xi_i\cdot\xi_j\cdot\xi_k\cdot\xi_l\Big]}_{=:\mu_{i,j,k,l}}
\end{align*}
Die Tupel $(i,j,k,l)\in\lbrace1,\ldots,n\rbrace^4$ mit mindestens drei verschiedenen Komponenten liefern $\mu_{i,j,k,l}=0$, denn z. B. (verschiedene Buchstaben repräsentieren verschiedene Zahlen):
\begin{align*}
\mu_{i,j,k,j}
&=\E\Big[\xi_i\cdot\xi_j\cdot\xi_k\cdot\xi_j\Big]
=\E\Big[\xi_i\cdot\xi_j^2\cdot\xi_k\Big]=\underbrace{\E\big[\xi_i\big]}_{=0}\cdot\E\big[\xi_i^2\big]\cdot\E\big[\xi_k\big]
\end{align*}
Folglich reduziert sich die obige auf (!)
\begin{align*}
&\sum\limits_{i=1}^4\underbrace{\E\Big[\xi_i^4\Big]}_{=\mu_4}
+6\cdot\sum\limits_{1\leq i\leq j\leq n}
\underbrace{\E\Big[\xi_i^2\cdot\xi_j^2\Big]}_{=\mu_2^2}
+\underbrace{\cdot\sum\limits_{1\leq i\leq j\leq n}\underbrace{\E\Big[\xi_i\Big]}_{=0}\cdot\E\Big[\xi_j^3\Big]+4\cdot\sum\limits_{1\leq i\leq j\leq n}\E\Big[\xi_i^3\Big]\cdot\underbrace{\E\Big[\xi_j\Big]}_{=0}}_{=0}\\
&=n\cdot\mu_4+6\cdot\mu_2^2\cdot\underbrace{\begin{pmatrix}
n\\ 2
\end{pmatrix}}_{=\frac{n\cdot(n-1)}{2}}\\
&=\underbrace{n}_{\leq n^2}\cdot\mu_4+3\dot n\cdot\underbrace{(n-1)}_{\leq n}\cdot\underbrace{\mu_2^2}_{=\big(\E[\xi_1^2]\big)^2\stackrel{\text{Jensen}}{\leq}\mu_4}\\
&\leq 4\cdot n^2\cdot\mu_4
\end{align*}
Siehe auch \textit{Turkish Journal of Mathematics, Moment equalities via integer partitions} (2014) von Dietmar Ferger für mehr Hintergründe.
\end{proof}

Mit diesem Lemma folgt:
\begin{align*}
\frac{1}{n^2}\cdot\E\left[\left|\sum\limits_{i=\lceil n\cdot t\rceil+1}^{\lceil n\cdot s\rceil}\xi_i\right|^4\right]
&=
\frac{1}{n^2}\cdot\E\left[\left|\sum\limits_{i=1}^{\lceil n\cdot s\rceil-\lceil n\cdot t\rceil}\xi_{i+\lceil n\cdot t\rceil}\right|^4\right]\\
&\leq
\frac{1}{n^2}\cdot 4\cdot\mu_4\cdot\Big(\underbrace{\lceil n\cdot s\rceil-\overbrace{\lceil n\cdot t\rceil}^{>n\cdot t-1}}_{\leq\underbrace{ n\cdot s-n\cdot t}_{=n\cdot(s-t)}+\underbrace{1}_{\stackrel{\text{2. Fall}}{\leq}n\cdot(s-t)}\leq2\cdot n\cdot(s-t)}\Big)^2\\
&\leq
\frac{1}{n^2}\cdot 4\cdot\mu_4\cdot 4\cdot n^2\cdot(s-t)^2\\
&=16\cdot\mu_4\cdot(s-t)^2\\
\implies
\E\Big[\big|X_n(s)-X_n(t)\big|^4\Big]
&\leq 27\cdot18\cdot\mu_4\cdot(s-t)^2\qquad\forall s>t\in I\\
&=\Big(F(s)-F(t)\Big)^2
\end{align*}
wobei $F(s):=\sqrt{27\cdot18\cdot\mu_4}\cdot s$. Offenbar ist $F$ stetig und streng monoton wachsend auf $I=[0,b]$. Folglich ist \eqref{eqSatz7.11VorM} aus Satz \ref{satz7.11MomentenkriteriumVonKolmogoroff} erfüllt mit $\mu=4$ und $\alpha=2>1$.\nl
Wir haben Donsker \ref{satz7.16Donsker} gezeigt, allerdings unter der \ul{stärkeren} Voraussetzung $\E\big[\xi_1^4]<\infty$. Den allgemeinen Fall $\E[\xi_1^2]<\infty$ zeigt man mit der sogenannten \textit{Methode des Stutzens (truncation)}, vergleiche Achim Klenke (2008) \textit{Wahrscheinlichkeitstheorie}
\end{proof}

%Ferger: Ich bin mit meinem Leben ja sehr zufrieden. [...] Das anstregendste als Professor ist das Tafelwischen. Ansonsten ist der Job leicht verdientes Geld. 

\setcounter{satz}{15}
\begin{bemerkungnr}\label{bemerkung7.16Einhalb} %7.16. Einhalb
%\begin{bemerkung}
Unser Beweis lässt sich sofort übertragen auf Dreiecksschemata\\ $\big\lbrace\xi_{i,n}:1\leq i\leq n,n\in\N\big\rbrace$ mit $\xi_{1,n},\ldots,\xi_{n,n}$ i.i.d. $\sim H$ (verteilt nach Verteilungsfunktion $H$), wobei die Verteilungsfunktion $H$ nicht von $n$ abhängt und $\E\big[\xi_{i,n}\big]=0$ und $\E\big[\xi_{i,n}^4\big]<\infty$ und $\Var(\xi_{i,n})=1$. \nl
Prokhorov (1956), \textit{Convergence of random processes and limit theorems in probability theory, Theory of Probability and its applications 1}, Seite 157-214, zeigt, dass auch hier die Existenz zweiter Momente $\E\big[\xi_{i,n}^2\big]<\infty$ ausreicht.
%\end{bemerkung}
\end{bemerkungnr}

Seien $(\xi_i)_{i\geq1}$ i.i.d. $\sim F$ mit $\E\big[\xi_1\big]=\mu\in\R$ und $\sigma^2:=\Var(\xi_1)\in(0,\infty)$. Dann ist Donsker \ref{satz7.16Donsker} anwendbar auf die \textbf{standardisierten Zufallsvariablen}
\begin{align*}
\tilde{\xi}_i:=\frac{\xi_i-\mu}{\sigma},\qquad\forall i\geq1
\end{align*}
Beachte: Die Grenzverteilung $W$ in Satz \ref{satz7.16Donsker} (also das Wiener-Maß) hängt \ul{nicht} von $F$ ab. Die Grenzverteilung ist also invariant unter $F$. Deshalb heißt Satz \ref{satz7.16Donsker} auch \textbf{Invarianzprinzip}. (Andere Formulierung: \textbf{Funktionaler Grenzwertsatz})\nl
Sei $h\colon C\big([0,b]\big)\to\R$ messbar und $W$-fast überall stetig. Dann gilt wegen Satz \ref{satz7.16Donsker} und \ref{satz4.10ContinuousMappingTheorem}:
\begin{align}\label{eqUnder7.16Eins}\tag{1}
h(X_n)\stackrel{\L}{\longrightarrow} h(B)
\end{align}
Kennt man die Verteilung von $h(B)$ (=Funktional der Brownschen Bewegung, dazu existiert umfangreiche Literatur, z.B. Borodin und Salminen (2002), \textit{Handbook of Brownian motion}), so auch die Grenzverteilung von $h(X_n)$. Dies macht man sich in der asymptotischen Statistik zunutze. (siehe Beispiel später) Umgekehrt lässt sich oft die Grenzverteilung $h(W)$ für besonders einfache Verteilungfunktionen $F$ bestimmen.
\begin{align*}
h\big(X_n\big)\stackrel{\L}{\longrightarrow}Z
\overset{\eqref{eqUnder7.16Eins}~\&~\ref{lemma4.6Einhalb}}{\implies}
h(B)=Z
\end{align*}
Damit ist der Satz \ref{satz7.16Donsker} von Donsker auch nützlich in der Wahrscheinlichkeitstheorie.

\subsection{Anwendung von Donsker in der Statistik}
\begin{notation}
Sei $X$ eine reelle Zufallsvariable. Dann schreibe
\begin{align*}
X\sim(\mu,\sigma^2):\Longleftrightarrow\E[X]=\mu\qquad\text{und}\qquad\Var(X)=\sigma^2
\end{align*}
\end{notation}

Wir betrachten das \underline{Change-Point-Problem:}
% mit diesem Problem hat Ferger seine Uni-Karriere gestartet
$X_{1,n},\ldots,X_{n,n},n\in\N$ unabhängig mit 
\begin{align*}
\left\lbrace\begin{array}{cl}
X_{i,n}\text{ i.i.d.}\sim(\mu,\sigma^2), &\falls 1\leq i\leq\tau_n\\
X_{i,n}\text{ i.i.d.}\sim(\mu,\tau^2), &\falls \tau_n< i\leq n
\end{array}\right.
\end{align*}
wobei $\tau_n\in\lbrace1,\ldots,n\rbrace$ \ul{unbekannt} (der sogenannte \textbf{Change-point\\ (moment of change)}) 

\begin{align*}
\underbrace{X_{1,n},\ldots,X_{\tau_n,n}}_{\text{i.i.d.}\sim(\mu,\sigma^2)}\qquad\underbrace{X_{\tau_n+1,n},\ldots,X_{n,n}}_{\text{i.i.d.}\sim(\nu,\tau^2)}
\end{align*}
\underline{Annahmen:} $\mu,\sigma^2$ bekannt, $\nu,\tau^2$ unbekannt, $\nu>\mu$\nl
\underline{Ziel:} Finde Test für 
$H_0:\tau_n=n$, d.h. es hat kein Wechsel stattgefunden vs. $H_1:1\leq\tau_n<n$. Dazu betrachte
\begin{align*}
S_k:=S_{k,n}&:=\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^k\underbrace{\frac{X_{i,n}-\mu}{\sigma}}_{=:\xi_{i,n}}&\forall& 0\leq k\leq n\\
\E[S_k]&=0\qquad	&\forall& 0\leq k\leq\tau_n
\end{align*}
da
\begin{align*}
S_k&=S_{\tau_n}+\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=\tau_n+1}^k\frac{X_{i,n}-\mu}{\sigma}
\end{align*}
folgt
\begin{align*}
\E[S_k]
&=0+\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=\tau_n+1}^k\frac{\overbrace{\E[X_{i,n}]}^{=\nu}-\mu}{\sigma}
=\frac{1}{\sqrt{n}}\cdot\big(k-\tau_n\big)\cdot\frac{\nu-\mu}{\sigma}\qquad\forall\tau_n<k\leq n
\end{align*}
%TODO Hier Plot von $(k,\E[S_k)]$ einfügen 
%für k\leq\tau_n ist es immer 0,
%für k>\tau_n wächst es monoton
%TODO Hier Plot von $(k,S_k)$ einfügen, unter H_1:
%für k\leq\tau_n ist es ein "rumzappeln" um die X-Achse
%für k>\tau_n gibt es dann einen "Drift" nach oben
%TODO Hier Plot von $(k,S_k)$ einfügen, unter H_0:
%hier kein Drift nach oben, nur herumzappeln um die x_Achse

Planversibler Test:
$H_0$ verwerfen $:\Longleftrightarrow T_n:=\max\limits_{0\leq k\leq n} S_k>k_\alpha$\nl
\underline{Ziel:} Bestimme kritischen Wert $k_\alpha$

\begin{lemma}\label{lemma7.17}
Sei $\underline{y}=\big(y_0,y_1,\ldots,y_n\big)\in\R^{n+1}$ und $h(\ul{y})$ der Polygonzug durch die Punkte $\left(\frac{k}{n},y_k\right)_{0\leq k\leq n}$. Dann gilt:
\begin{enumerate}[label=(\arabic*)]
\item $\begin{aligned}
\max\limits_{0\leq k\leq n}y_k=\max\limits_{0\leq t\leq 1} h(\ul{y})(t)
\end{aligned}$
%TODO Hier  könnte man eine Skizze einfügen
\item $\begin{aligned}
\max\limits_{0\leq k\leq n}\big|y_k\big|=\max\limits_{0\leq t\leq 1} \big|h(\ul{y})(t)\big|
\end{aligned}$
\item $\begin{aligned}
\ul{y},\ul{z}\in\R^{n+1}\implies h\big(\ul{y}+\ul{z}\big)=h\big(\ul{y}\big)+h\big(\ul{z}\big)
\end{aligned}$
\end{enumerate}
\end{lemma}

\begin{proof}
\underline{Zeige (1):}
Gemäß der Zwei-Punkte-Formel gilt:
\begin{align}\label{eqProof7.17Stern}\tag{$\ast$}
h\big(\ul{y}\big)(t)
&=y_{k-1}+\big(n\cdot t-(k-1)\big)\cdot\big(y_k-y_{k-1}\big)
\qquad\forall t\in\left[\frac{k-1}{n},\frac{k}{n}\right],\forall 1\leq k\leq n
\end{align}
Da
\begin{align*}
h\big(\ul{y}\big)\left(\frac{k}{n}\right)\overset{\text{Def}}{=}y_k\qquad\forall 0\leq k\leq n
\end{align*}
gilt in (1) und (2) in jedem Fall "$\leq$". Umgekehrt sei $t\in[0,1]$ beliebig. Dann:
\begin{align*}
\exists 1\leq k\leq n:t\in\left[\frac{k-1}{n},\frac{k}{n}\right]
\end{align*}
\underline{Fall 1:} $y_k\geq y_{k-1}$\\
Dann ist $h\big(\ul{y}\big)$ monoton wachsend auf $\left[\frac{k-1}{n},\frac{k}{n}\right]$ und es gilt
\begin{align*}
h\big(\ul{y}\big)(t)\leq h\big(\ul{y}\big)\left(\frac{k}{n}\right)=y_k\leq\max\limits_{0\leq k\leq n} y_k
\end{align*}
\underline{Fall 2:} $y_k<y_{k-1}$\\
Dann ist $h\big(\ul{y}\big)$ monoton fallend und es gilt
\begin{align*}
h\big(\ul{y}\big)(t)\leq h\big(\ul{y}\big)\left(\frac{k-1}{n}\right)=y_{k-1}\leq\max\limits_{0\leq k\leq n}y_k
\end{align*}

\underline{Zeige (2):} In Fall 1 gilt:
\begin{align*}
h\big(\ul{y}\big)(t)&\leq y_k\leq\big|y_k\big|\leq\max\limits_{0\leq k\leq n}\big|y_k\big|\\
h\big(\ul{y}\big)(t)&\geq y_{k-1}\geq-\big|y_{k-1}\big|\geq-\max\limits_{0\leq k\leq n}\big|y_k\big|\\
\implies
\Big|h\big(\ul{y}\big)(t)\Big|&\leq\max\limits_{0\leq k\leq n}\big| y_k\big|
\end{align*}
Fall 2 analog. Somit folgt (2).\nl
\underline{Zeige (3):}\\
Folgt aus \eqref{eqProof7.17Stern}.
\end{proof}

Es folgt aus dem Lemma \ref{lemma7.17} (1) (mit $y_k=S_k$):
\begin{align*}
T_n:&=\max\limits_{0\leq k\leq n} S_k
\overset{\ref{lemma7.17}}=
\max\limits_{0\leq t\leq 1}\underbrace{h\big(S_0,\ldots,S_n\big)}_{\stackrel{\text{Def}}{=}X_n=\text{ Partialsummenproz.}}(t)\\
T_n
&=\sup\limits_{0\leq t\leq 1} X_n(t)=M(X_n),\text{ wobei}\\
M&:C\big([0,1]\big)\to\R,\qquad M(f):=\sup\limits_{0\leq t\leq 1} f(t),\qquad\forall C\big([0,1]\big)
\end{align*}
Da 
\begin{align*}
\Big|M(f)-M(g)\Big|\overset{(!)}{\leq}\sup\limits_{0\leq t\leq 1}\Big|f(t)-g(t)\Big|=d(f,g)
\end{align*}
ist $M$ stetig auf $C\big([0,1]\big)$ folgt mit Donsker \ref{bemerkung7.16Einhalb} und dem CMT \ref{satz4.10ContinuousMappingTheorem}:
\begin{align*}
T_n=M(X_n)\stackrel{\L}{\longrightarrow} M(B)=\sup\limits_{0\leq t\leq 1} B(t),~\text{\ul{falls} } H_0\text{ gilt}
\end{align*}
Es gilt (siehe z.B. Borodin + Salminen):
\begin{align*}
\sup\limits_{0\leq t\leq 1}B(t)
\overset{\L}&=
\big|\mathcal{N}(0,1)\big|\quad\Big(=|Z|,~Z\sim\mathcal{N}(0,1)\Big)\\
&\implies
\underbrace{\P_{H_0}\big(T_n>x\big)}_{=1-\P\big(T_n\leq x\big)}\stackrel{n\to\infty}{\longrightarrow}\P\big(|Z|>x\big)
=1-\P\big(|Z|\leq x\big)\qquad\forall x\in\R,
\end{align*}
da Verteilungsfunktion von $|Z|$ stetig auf ganz $\R$, denn:
\begin{align*}
\P\big(|Z|>x\big)
\stackrelnew{Z\stackeq{\L}-Z}{\text{Sym}}{=}
2\cdot\big(1-\Phi(x)\big)
\end{align*}
Also folgt
\begin{align*}
\P\big(T_n>k_\alpha\big)\stackrel{n\to\infty}{\longrightarrow}2\cdot\big(1-\Phi(k_\alpha)\big)\stackeq{!}\alpha\\
2\cdot\big(1-\Phi(k_\alpha)\big)=\alpha
\Longleftrightarrow 1-\frac{\alpha}{2}=\Phi(k_\alpha)\\
\Longleftrightarrow k_\alpha=\Phi^{-1}\left(1-\frac{\alpha}{2}\right)=: u_{1-\frac{\alpha}{2}}
\end{align*}
Es folgt: Der Test $H_0$ verwerfen $:\Longleftrightarrow T_n> u_{1-\frac{\alpha}{2}}$\\ %\alpha/2-Quantil der Standardnormalverteilung
ist ein \textbf{asymptotischer Niveau-$\alpha$-Test für $H_0$}, d.h.
\begin{align*}
\limn\P_{H_0}\big(H_0\text{ wird verworfen}\big)=\alpha\qquad\forall \alpha\in(0,1)
\end{align*}

Falls nur bekannt, dass $\nu\neq\mu$, so modifiziere obigen Test zu\\
$H_0$ verwerfen $:\Longleftrightarrow\hat{T}_n:=\max\limits_{0\leq k\leq n}\big|S_k\big|>c_\alpha$\\
Aus \ref{lemma7.17} (2) folgt:
\begin{align*}
\hat{T}_n=\sup\limits_{0\leq t\leq 1}\big|X_n(t)\big|=\Vert X_n\Vert_\infty=:\hat{M}(X_n)
\end{align*}
Da $\hat{M}$ stetig auf $C\big([0,1]\big)$, folgt analog wie oben
\begin{align*}
\hat{T}_n=\hat{M}(X_n)\stackrel{\L}{\longrightarrow}\hat{M}(B)=\sup\limits_{0\leq t\leq 1}\big|B(t)\big|
\end{align*}
Es gilt (vergleiche z.B. Shorak und Wellner (1986) \textit{Empirical Processes with applications to statistics}):
\begin{align*}
H(x):&=\P\left(\sup\limits_{0\leq t\leq 1}\big|B(t)\big|\leq x\right)
=\left\lbrace\begin{array}{cl}
\frac{4}{\pi}\cdot\sum\limits_{k\in\N_0}\frac{(-1)^k}{2\cdot k+1}\cdot\exp\left(-\frac{(2\cdot k+1)^2\cdot\pi^2}{8\cdot x^2}\right), &\falls x>0\\
0, &\falls x\leq 0
\end{array}\right.
\end{align*}
Ferner ist $H$ stetig auf $\R$ und streng monoton wachsend auf $(0,\infty)$. Somit erhalten wir (analog wie oben)
\begin{align*}
\limn\P_{H_0}\Big(H_0\text{ wird verworfen}\Big)=\alpha\qquad\forall\alpha\in(0,1)
\end{align*}
\ul{falls} man $c_\alpha:=H^{-1}(1-\alpha)$ wählt.\\
Natürlich ist die Annahme $\mu$ und $\sigma^2$ beide bekannt sehr restriktiv. Falls beide Parameter unbekannt, so ersetze sie durch Schätzer (sogenannte \textbf{Plug-in-Methode}).
\begin{align*}
\overline{X}_n:=\frac{1}{n}\cdot\sum\limits_{i=1}^n X_i,\qquad 
\hat{\sigma}_n^2:=\frac{1}{n}\cdot\sum\limits_{i=1}^n\big(X_i-\overline{X}_n\big)^2
\end{align*}
Wir wissen
\begin{align*}
\big(\overline{X}_n,\sigma_n^2\big)\stackrel{n\to\infty}{\longrightarrow}\big(\mu,\hat{\sigma}^2\big)\qquad\P_{H_0}\text{-f.s.}
\end{align*}
Beachte: Es gibt auch ein starkes Gesetz der großen Zahlen (SGGZ bzw. auf englisch SLLN) für Dreiecksschemata (arrays), siehe z.B. Chow und Teicher (1997) \textit{Probability Theory - Independence, Interchangeability, Martingales}\nl
Das Ersetzen von $\mu$ und $\sigma^2$ in der Statistik
\begin{align*}
T_n=\max\limits_{0\leq k\leq n}\frac{1}{\sqrt{n}}\left|\sum\limits_{i=1}^k\frac{X_i-\mu}{\sigma}\right|
\end{align*}
liefert
\begin{align*}
T_n^\ast
:&=\frac{1}{\hat{\sigma}_n}\cdot\max\limits_{0\leq k\leq n}\frac{1}{\sqrt{n}}\cdot\left|\sum\limits_{i=1}^k \big(X_{i,n}-\overline{X}_n\big)\right|
\mit \hat{\sigma}=\sqrt{\hat{\sigma}^2}
\end{align*}

\begin{satz}\label{satz7.18}
Unter $H_0$ gilt:
\begin{align*}
T_n^\ast\stackrel{\L}{\longrightarrow}\sup\limits_{0\leq t\leq 1}\big|B(t)-t\cdot B(1)\big|
\end{align*}
\end{satz}

\begin{proof}
\begin{align*}
X_i-\overline{X}_n
&=X_i-\frac{1}{n}\cdot\sum\limits_{j=1}^n X_j
=\sigma\bigg(\underbrace{\frac{X_i-\mu}{\sigma}}_{=:Z_i}-\underbrace{\frac{1}{n}\cdot\sum\limits_{j=1}^n\frac{X_j-\mu}{\sigma}}_{=:\overline{Z}_n}\bigg)\\
\implies 
T_n^\ast&=\frac{\sigma}{\hat{\sigma}_n}\cdot\max\limits_{0\leq k\leq n}\left|\frac{1}{n\sqrt{n}}\cdot\sum\limits_{i=1}^k\big(Z_i-\overline{Z}_n\big)\right|\\
&=\frac{\sigma}{\hat{\sigma}_n}\cdot\max\limits_{0\leq k\leq n}\left|S_k-\frac{k}{n}\cdot S_n\right|\\
\overset{\ref{lemma7.17}(2)+(3)}&=
\frac{\sigma}{\hat{\sigma}_n}\cdot\max\limits_{0\leq t\leq 1}\Big|\underbrace{X_n(t)-t\cdot X_n(1)}_{=:Y_n(t)}\Big|
\end{align*}
wobei $X_n$ der Partialsummenprozess zu $Z_1,\ldots,Z_n$ ist (= Polygonzug durch $\left(\frac{k}{n},S_k\right)$) und $S_k:=\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^k Z_i$.\nl
Betrachte die Abbildung
\begin{align*}
&h:C\big([0,1]\big)\to C\big([0,1]\big),\qquad f \mapsto h(f):[0,1]\to\R,\qquad t\mapsto h(f)(t):=f(t)-t\cdot f(1)\\
&\implies Y_n=h(X_n)
\end{align*}
Wir zeigen nun, dass $h$ stetig ist.
\begin{align*}
\big|h(f)(t)-h(g)(t)\big|
&=\Big|f(t)-t\cdot f(1)-\big(g(t)-t\cdot g(1)\big)\Big|\\
&=\Big|f(t)-g(t)-t\cdot\big(f(1)-g(1)\big)\Big|\\
\overset{\text{DU}}&\leq
\big|f(t)-g(t)\big|+\underbrace{|t|}_{\leq 1}\cdot\big|f(1)-g(1)\big|\\
&\leq
2\cdot d(f,g)\qquad\forall t\in[0,1]\\
\implies d\big(h(f),h(g)\big)&\leq 2\cdot d(f,g)\qquad\forall f,g\in C\big([0,1]\big)
\end{align*}
Also ist $h$ stetig auf $C\big([0,1]\big)$. Dann folgt aus Bemerkung  \ref{bemerkung7.16Einhalb} + CMT \ref{satz4.10ContinuousMappingTheorem}:
\begin{align*}
Y_n=h(X_n)\stackrel{\L}{\longrightarrow} h(B)=:B_0\text{ in }C\big([0,1]\big)
\end{align*}
wobei $B_0(t)\overset{\text{Def}}{=}B(t)-t\cdot B(1)$.
\begin{align*}
\max\limits_{0\leq t\leq 1}\big|Y_n(t)\big|=\hat{M}(Y_n)=\Vert Y_n\Vert_\infty\stackrelnew{\ref{satz4.10ContinuousMappingTheorem}}{\L}{\longrightarrow}\Vert B_0\Vert=\sup\limits_{0\leq t\leq 1}\big|B_0(t)\big|
\end{align*}
Da $\frac{\sigma}{\hat{\sigma}_n}\stackrel{\P}{\longrightarrow}1$, liefert \ref{beisp4.18}(3):
\begin{align*}
T_n^\ast\stackrel{\L}{\longrightarrow}\sup\limits_{0\leq t\leq 1}\big|B_0(t)\big|
\end{align*}
\end{proof}

\begin{definition} %7.19
Sei $B$ ein Brownsche Bewegung aud $[0,1]$. Der stochastische Prozess
\begin{align*}
B_0(t):=B(t)-t\cdot B(1)\qquad\forall t\in[0,1]
\end{align*}
heißt \textbf{Brownsche Brücke}.
\end{definition}

%TODO Hier könnte man eine Skizze einfügen
%B_0(0)=B_0(1)=0
%Zwischen 0 und 1 gezitterter Sinus
%Ferger: "Also über die Brücke würde ich jetzt nicht gehen. Kein Ahnung, warum man Brücke dazu sagt."

%T_n^\ast ist Supremumsnorm von Partialsummenprozess?+

Es gilt (vergleiche Shorak und Wellner 1986 \textit{empirical processes}, Seite 34)
\begin{align*}
H_0(x)&:=\P\left(\sup\limits_{0\leq t\leq 1}\big|B_0(t)\big|\leq x\right)
\overset{\text{Doob}}{=}
1-\sum\limits_{k\geq 1}(-1)^{k+1}\cdot\exp\big(-2\cdot k^2\cdot x^2\big)&\forall x>0\\
H_0(x)&~=0 &\forall x\leq 0
\end{align*}
$H_0$ ist stetig auf $\R$ uns streng monoton auf $[0,\infty)$. Folglich ist der Test
\begin{align*}
H_0\text{ verwerfen }:\Longleftrightarrow T_n^\ast>H_0^{-1}(1-\alpha)
\end{align*}
ist ein asymptotischer Niveau-$\alpha$-Test.\\
$H_0$ ist die Verteilungsfunktion der \textbf{Kolmogorov-Smirnov-Verteilung}.\\
Falls $\mu,\sigma^2$ unbekannt, aber $\nu>\mu$, so betrachte 
\begin{align*}
\tilde{T}_n&:=\frac{1}{\sqrt{n}}\cdot\frac{1}{\hat{\sigma}_n}\cdot\max\limits_{0\leq k\leq n}\left(\sum\limits_{i=1}^kX_i-\overline{X}_n\right)
\end{align*}
Vollkommen analog folgt: $\tilde{T}_n\stackrel{\L}{\longrightarrow}\sup\limits_{0\leq t\leq 1} B_0(t)$ unter $H_0$. Es gilt (vergleiche z.B. Gänssler + Stute (1977) \textit{Wahrscheinlichkeitstheorie}, Seite 322)
\begin{align*}
H_0^+(x)&:=\P\left(\sup\limits_{0\leq t\leq 1}B_0(t)\leq x\right)=1-\exp\left(-2\cdot x^2\right) &\forall x\geq0\\
H_0^+(x)&:=0 &\forall x<0
\end{align*}
Da 
\begin{align*}
(H_0^+)^{-1}(1-\alpha)&=\left(-\frac{1}{2}\cdot\log(\alpha)\right)^{\frac{1}{2}}
\end{align*}
folgt, dass der (einseitige) Test
\begin{align*}
H_0\text{ verwerfen}:\Longleftrightarrow\tilde{T}_n>\left(-\frac{1}{2}\cdot\log(\alpha)\right)^{\frac{1}{2}}
\end{align*}
ein asymptotischer Niveau-$\alpha$-Test für $H_0$ ist.\nl
Anstelle von
\begin{align*}
T_n^\ast=\frac{1}{\sqrt{n}}\cdot\frac{1}{\hat{\sigma}_n}\cdot\max\limits_{0\leq k \leq n}\bigg|\underbrace{\sum\limits_{i=1}^k\big(X_i-\overline{X}_n\big)}_{=:C_k}\bigg|
\end{align*}
(die $C_k$ heißen \textbf{kumulierte Summe}) betrachten Csörgő und Horváth (1977) in \textit{Limit Theorems in Change-point Analysis} \ul{gewichtete} kumulierte Summen:
\begin{align*}
U_n^\ast&:=\sqrt{n}\cdot\frac{1}{\hat{\sigma}_n}\cdot\max\limits_{1\leq k\leq n-1}\frac{|C_k|}{\sqrt{k\cdot(n-k)}}
\end{align*}

\begin{thm}[Csörgő und Horváth]\label{theoremCH} %noNumber
	Seien
	\begin{align*}
		A_n&:=\sqrt{2\cdot\log\big(\log(n)\big)}\\
		D_n&:=2\cdot\log\big(\log(n)\big)+\frac{1}{2}\cdot\log\Big(\log\big(\log(n)\big)\Big)-\frac{1}{2}\cdot\log(\pi)\qquad\forall n\geq n_0
	\end{align*}
	Dann gilt unter $H_0$:
	\begin{align}\label{eqTheoremCsorgoHorvath}\tag{1}
		\limn\P\Big(A_n\cdot U_n^\ast-D_n\leq t\Big)=\exp\big(-2\cdot\exp(-t)\big)=:G(t)
		\qquad\forall t\in\R
	\end{align}
	$G$ ist die sogenannte \textbf{Gumbel-Verteilung}.
\end{thm}

Damit Konstruktion eines asymptotischen Niveau-$\alpha$-Tests für $H_0$ wie folgt möglich: Sei
\begin{align*}
t_\alpha&:=G^{-1}(1-\alpha)=-\log\left(-\frac{1}{2}\cdot\log(1-\alpha)\right)
\end{align*}
Klarerweise gilt:
\begin{align*}
A_n\cdot U_n^\ast-D_n>t_\alpha\implies U_n^\ast>\frac{t_\alpha+D_n}{A_n}:=d_n(\alpha)
\end{align*}
Somit ist 
\begin{align*}
H_0\text{ verwerfen}:\Longleftrightarrow U_n^\ast>d_n(\alpha)
\end{align*}
ein asymptotischer Niveau-$\alpha$-Test, d.h.
\begin{align}\label{eqUnderTheoremCsorgoHarcath}\tag{2}
\limn\P_{H_0}\big(U_n^\ast>d_n(\alpha)\big)=\alpha
\end{align}
\textbf{Problem:} Die Konvergenz in \eqref{eqTheoremCsorgoHorvath} bzw. in \eqref{eqUnderTheoremCsorgoHarcath} ist sehr sehr sehr laaaaaangsaaaam. %genauso stand das an der Tafel
%Ferger: "[...] Grottenschlecht."
Andererseits führen die Gewichte in $\big(k\cdot(n-k)\big)^{-\frac{1}{2}}$ in $U_n^\ast$ zu einer verbesserten Sensitivität des \textbf{$U_n^\ast$-Test} unter der Alternativen, \underline{insbesondere}, falls $\tau_n$ nahe bei $1$ bzw. oder $n-1$ (am Rand) liegt.\nl
\textbf{Idee:} Weiterhin gewichten, aber durch die \ul{zufällige} Größe $\sqrt{\hat{k}_n\cdot(n-\hat{k}_n)}$ wobei 
\begin{align*}
	\hat{k}_n&:=\min\left\lbrace 1\leq k\leq n:\big|C_k\big|=\max\limits_{1\leq j\leq n-1}\big|C_j\big|\right\rbrace\\
	C_j&:=\sum\limits_{i=1}^j \big(X_i-\overline{X}_n\big)
\end{align*}
d.h. $\hat{k}_n$ ist die kleinste Maximalstelle von $|C_k|,k\in\lbrace1,\ldots,n-1\rbrace$. Erhalte:
\begin{align*}
	R_n=\sqrt{n}\cdot\frac{1}{\hat{\sigma}_n}\cdot\frac{\max\limits_{1\leq k\leq n-1}\left|\sum\limits_{i=1}^k\big)(X_i-\overline{X}_n\big)\right|}{\sqrt{\hat{k}_n\cdot(n-\hat{k}_n)}}
\end{align*}

\begin{thm}[Dietmar Ferger, 2018]\label{theoremFerger2018} %nonumber
	Sei $(X_i)_{i\in\N}$ eine Folge von i.i.d. Zufallsvariablen mit positiver endlicher Varianz. Dann gilt:
	\begin{align}\label{eqTheoremFerger2018}\tag{3}
		R_n&\stackrel{\L}{\longrightarrow} R
		\qquad\text{wobei}\qquad
		R=\frac{M}{\sqrt{T\cdot(1-T)}}\mit\\
		M&:=\sup\limits_{0\leq t\leq 1}\big|B_0(t)\big|\nonumber,\qquad
		T:=\arg\max\limits_{0<t<1}\big|B_0(t)\big|,\qquad
		B_0=\text{ Brownsche Brücke}\nonumber
	\end{align}
\end{thm}

In diesem Theorem \ref{theoremFerger2018} wird die Verteilungsfunktion $H$ von $R$ explizit bestimmt ($H$ hat Reihendarstellung).
%Ferger: "Sie müssen immer eine Anwendung haben. Am besten Sie retten die Welt."
Falls $r_\alpha:=H^{-1}(1-\alpha)$, so liefert \eqref{eqTheoremFerger2018} für den Test
\begin{align*}
	H_0\text{ verwerfen}:\Longleftrightarrow R_n>r_\alpha
\end{align*}
dass gilt:
\begin{align*}
	\limn\P_{H_0}\big(R_n>r_\alpha\big)=\alpha
\end{align*}

\begin{bemerkung}
	\eqref{eqTheoremFerger2018} gilt für $X_{1,n},\ldots,X_{n,n}$ i.i.d. $\forall n\in\N$.\\
	In Simulationsstudien zeigt der $R_n$-Test eine deutlich bessere Güte als der $U_n^\ast$-Test von Csörgő und Horvath.\\
	Falls bekannt ist, dass $\nu>\mu$, so betrachte
	\begin{align*}
		R_n^+&:=\sqrt{n}\cdot\frac{1}{\hat{\sigma}_n}\cdot\frac{\max\limits_{1\leq k\leq n-1}\sum\limits_{i=1}^k\big(X_i-\overline{X}_n\big)}{\sqrt{\hat{k}_n^+\cdot\big(n-\hat{k}_n^+\big)}}\\
		\hat{k}_n^+&:=\min\left\lbrace 1\leq k\leq n-1:C_k=\max\limits_{1\leq j\leq n-1}C_j\right\rbrace
	\end{align*}
\end{bemerkung}

Es gilt:
\begin{align*}
	R_n^+\stackrel{\L}{\longrightarrow} R^+=\frac{M^+}{\sqrt{T^+\cdot(1-T^+)}},\qquad
	M^+=\sup\limits B_0(t),\qquad
	T^*=\arg\max\limits B_0(t)\\
	H^+(x):=\P\big(R^+\leq x\big)=2\cdot\Phi(x)-\sqrt{\frac{2}{\pi}}\cdot x\cdot\exp\left(-\frac{1}{2}\cdot x^2\right)-1\qquad x\geq0~(=0, x<0)
\end{align*}
Hierbei ist $\Phi$ die Verteilungsfunktion der Standardnormalverteilung. Ferner gilt:\\
Die Zufallsgrößen $R^+$ und $T^+$ sind stochastisch unabhängig!
\begin{align*}
	\hat{\tau}_n-\tau_n\stackrel{\L}{\longrightarrow}V=\\
	\hat{\tau}_n:=\arg\max\limits_{1\leq k\leq n-1}\left|\sum\limits_{i=1}^k\big(X_i-\overline{X_n}\big)\right|
\end{align*}

\subsection{Verteilungskonvergenz in \texorpdfstring{$C(\R)$}{C(R)}}
Häufig hat man mit stochastischen Prozessen zu tun, deren Pfade in
\begin{align*}
	C(\R):=\Big\lbrace f:\R\to\R:f\text{ stetig}\Big\rbrace
\end{align*}
liegen (siehe Beispiel Median ganz am Anfang). Die bisherige Theorie für $C(I)$, $I$\\ \underline{kompaktes} Intervall deckt das nicht ab. Wir versehen $C(\R)$ mit der Metrik
\begin{align*}
	d(f,g)&:=\sum\limits_{j\geq 1} 2^{-j}\cdot\frac{d_j(f,g)}{1+d_j(f,g)} &\forall f,g\in C(\R)\\
	d_j(f,g)&:=\sup\limits_{-j\leq t\leq j}\Big|f(t)-g(t)\Big| &\forall f,g\in C(\R)
\end{align*}
Es zeigt sich, das $\big(C(\R),d\big)$ ein vollständiger separabler metrischer Raum ist. Ferner gilt:
\begin{align*}
	d(f_n,f)\stackrel{n\to\infty}{\longrightarrow}0
	\Longleftrightarrow \forall j\in\N: d_j(f_n,f)\stackrel{n\to\infty}{\longrightarrow}0
	\Longleftrightarrow\text{ glm. Konvergenz auf Kompakta}
\end{align*}
Seien $\pi_t:C(\R)\to R$ und $\pi_T:C(\R)\to\R^{|T|}$ die Projektionsabbildungen, d.h. z.B.
\begin{align*}
	\pi_T(f)=\big(f(t)\big)_{t\in T}=\Big(f(t_1),\ldots,f(t_k)\Big)\qquad
	T=\lbrace t_1,\ldots, t_k\rbrace
\end{align*}

\begin{theorem}\label{theorem7.20}
	\begin{align*}
		\B_d\big(C(\R)\big)&=\sigma\Big(\pi_t:t\in\R\Big)=\sigma\Big(\pi_T:T\subseteq\R\text{ endlich}\Big)
	\end{align*}
\end{theorem}

\begin{proof}
	Die Argumente im Beweis von Satz \ref{satz7.2} lassen sich problemlos übertragen.
\end{proof}

Ebenfalls analog beweist man:

\begin{satz}\label{satz7.21}
	 Sei $(\Omega,\A)$ messbarer Raum und $C:=C(\R)$ versehen mit $\B_d(D)$.\\
	 Dann gilt für eine Abbildung $X:\Omega\to C(\R):$
	 \begin{align*}
	 	X~\A\text{-}\B_d(C)\text{-messbar}
	 	\Longleftrightarrow\forall t\in\R:
	 	\pi_t\circ X~\A\text{-}\B(\R)\text{-messbar }
	 \end{align*}
\end{satz}

Konsequenz aus Satz \ref{satz7.21}: Jeder stetige stochastische Prozess indiziert nach $\R$ kann aufgefasst werden als Zufallsvariable in $(C,d)$.

\begin{satz}\label{satz7.22}
	Seien $X,Y$ Zufallsvariablen in $\big(C(\R),d\big)$. Dann gilt:
	\begin{align*}
		X\stackeq{\L}Y\Longleftrightarrow
		\Big(X(t_1),\ldots,X(t_k)\Big)\stackeq{\L}\Big(Y(t_1),\ldots,Y(t_k)\Big)
	\end{align*}
	für jede Wahl von Punkten $t_1<\ldots<t_k$ aus $\R$.
\end{satz}

\begin{proof}
	Siehe Theorem \ref{theorem7.20} + Maßeindeutigkeitssatz.
\end{proof}

Für $f\in C(\R)$ sei
\begin{align*}
	f^{(j)}:=f|_{[-j,j]},\qquad I_j:=[-j,j]\text{ d.h.}\\
	f^{(j)}:I_j\to\R,\qquad f^{(j)}(t):=f(t)\qquad\forall t\in I_j
\end{align*}

\begin{satz}\label{satz7.23}
	Seien $X,X_n,n\in\N$ Zufallsvariablen in $\big(C(\R),d\big)$. Dann gilt:
	\begin{align*}
		X_n\stackrel{\L}{\longrightarrow}X\text{ in }\big(C(\R),d\big)
		\Longleftrightarrow\forall j\in\N:
		X_n^{(j)}\stackrel{\L}{\longrightarrow} X^{(j)}\text{ in }\big(C(I_j),d_j\big)
	\end{align*}
\end{satz}

Satz \ref{satz7.23} geht auf Whitt (1970) zurück. Ein (relativ) einfacher Beweis findet sich in Kallenberg (1997), \textit{Foundations of modern probability}, Seite 260.

\begin{bemerkungnr}\label{bemerkung7.24}\
	\begin{enumerate}[label=(\arabic*)]
		\item Die Resultate in \ref{theorem7.20},\ref{satz7.21},\ref{satz7.22} und \ref{satz7.23} gelten analog für $C\big([0,\infty)\big)$ mit $I_j$ ersetzt durch $[0,j]$.
		\item Satz \ref{satz7.23} ermöglicht es insbesondere, auf die Konvergenz-Kriterien in \ref{satz7.9} und \ref{satz7.11MomentenkriteriumVonKolmogoroff} zurückzugreifen.
	\end{enumerate}
\end{bemerkungnr}

\begin{beispiel}\label{beispiel7.25}
	 Seien $(\xi_i)_{i\in\N}$ i.i.d. mit $\E[\xi_1]=0$, $\Var(\xi_1)=1$ und 
	 \begin{align*}
	 	S_k&:=\sum\limits_{j=1}^k\xi_j\qquad\forall k\in\N_0\\
	 	X_n(t)&:=\frac{1}{\sqrt{n}}\cdot S_{\lceil n\cdot t\rceil}+\frac{1}{\sqrt{n}}\cdot\big(n\cdot t-\lceil n\cdot t\rceil\big)\cdot\xi_{\lceil n\cdot t\rceil+1}\qquad\forall t\in[0,\infty)
	 \end{align*}
	 D.h. $X_n$ ist Polygonzug durch $\left(\frac{k}{n},\frac{1}{\sqrt{n}\cdot S_n}\right),k\in\N_0$.
	 %TODO Hier Skizze einfügen
	 Aus Satz \ref{satz7.16Donsker} folgt:
	 \begin{align*}
	 	&X_n^{(j)}\stackrel{\L}{\longrightarrow}B^{(j)}\qquad\forall j\in\N\\
	 	&\overset{\ref{bemerkung7.24}(1)+\ref{satz7.23}}&\Longleftrightarrow
	 	X_n\stackrel{\L}{\longleftarrow} B\text{ in }\Big(C\big([0,\infty)\big),d\Big)
	 \end{align*}
\end{beispiel}

\section{Argmin-Theoreme in \texorpdfstring{$C(\R)$}{C(R)}}
Erinnere an folgende Probleme, vergleiche  1.2 und 1.5 (Median)\\ %TODO
Wann überträgt sich die Konvergenz (f.s. oder in Verteilung) von stetigen stochastischen Prozessen auf deren Minimalstellen?

\begin{definition}\label{definition8.1}
	Sei $f\in C(\R)$.
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			A(f)=\arg\min(f):=\left\lbrace t\in\R:f(t)=\inf\limits_{s\in\R} f(s)\right\rbrace
		\end{aligned}$ = Menge aller Minimalstellen von $f$
		\item $\tau\in A(f)$ heißt \textbf{wohl-separiert} 
		\begin{align*}
			:\Longleftrightarrow\int\Big\lbrace f(t):|t-\tau|\geq\varepsilon\Big\rbrace>f(\tau)\qquad\forall 0<\varepsilon\in\Q
		\end{align*}
	\end{enumerate}
\end{definition}

\begin{bemerkungnr}\label{bemerkung8.2}\
	\begin{enumerate}[label=(\arabic*)]
		\item $A(f)\neq\emptyset$ ist natürlich nicht ausgeschlossen, aber in jedem Fall ist $A(f)$\\ \underline{abgeschlossen} in $\R$, denn:\\
		Sei $(t_n)_{n\in\N}\subseteq A(f)$ mit $t_n\stackrel{n\to\infty}{\longrightarrow}t.$ Dann gilt:
		\begin{align*}
			f(t)&=\limn\underbrace{f(t_n)}_{=\inf\limits_{s\in\R}f(s)}=\inf\limits_{s\in\R}f(s)
			\implies t\in A(f)
		\end{align*}
		\item $\tau$ wohl-separariert $\implies\tau$ eindeutig, denn:\\
		Sei $\tilde{\tau}\in A(f)$ und $\tilde{\tau}\neq\tau$. Also:
		\begin{align}\label{eqBemerkung8.2Stern}\tag{$\ast$}
			\exists 0&<\varepsilon\in\Q:\big|\tau-\tau|>\varepsilon\\\nonumber
			\inf\limits_{s\in\R}f(s)
			\overset{\tilde{\tau}\in A(f)}&=
			f(\tau)
			\overset{\eqref{eqBemerkung8.2Stern}}{\geq}
			\inf\Big\lbrace f(t):|t-\tau|\geq\varepsilon\Big\rbrace
			\overset{\tau\text{ wohl-sep}}{\geq}
			f(\tau)
			\overset{\tau\in A(f)}{=}
			\inf\limits_{s\in\R} f(s)
		\end{align}
		Dies ist ein Widerspruch!
	\end{enumerate}
\end{bemerkungnr}
